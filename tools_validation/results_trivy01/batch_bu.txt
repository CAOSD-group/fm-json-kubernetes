
Report Summary

┌────────────────────────────────────────────────┬────────────┬───────────────────┐
│                     Target                     │    Type    │ Misconfigurations │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role4.yaml               │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role4_1.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role4_2.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role4_3.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role5.yaml               │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role5_1.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role5_2.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role5_3.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role7.yaml               │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role7_1.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role7_2.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role7_3.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role8.yaml               │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role8_1.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role8_2.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role8_3.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role9.yaml               │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role9_1.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role9_2.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role9_3.yaml             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role_1.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role_2.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-node-controller-role_3.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding10.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding10_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding11.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding11_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding13.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding13_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding14.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding14_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding15.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding15_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding16.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding16_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding17.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding17_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding18.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding18_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding1_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding2.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding2_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding3_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding4_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding5.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding5_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding6.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding6_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding7.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding7_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding8.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding8_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding9.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding9_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-binding_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role1.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role10.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role10_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role10_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role10_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role11.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role11_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role11_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role11_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role13.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role13_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role13_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role13_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role14.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role14_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role14_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role14_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role15.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role15_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role15_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role15_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role16.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role16_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role16_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role16_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role17.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role17_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role17_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role17_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role18.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role18_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role18_2.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role18_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role1_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role1_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role1_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role2.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role2_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role2_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role2_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role3.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role3_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role3_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role3_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role4.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role4_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role4_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role4_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role5.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role5_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role5_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role5_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role6.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role6_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role6_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role6_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role7.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role7_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role7_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role7_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role8.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role8_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role8_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role8_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role9.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role9_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role9_2.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role9_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role_1.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role_2.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud-provider-role_3.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudDriveVol.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_deployment.yaml                │ kubernetes │        12         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_deployment1.yaml               │ kubernetes │        12         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role1.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role1_1.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role1_2.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role1_3.yaml                   │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role1_4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role_2.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role_3.yaml                    │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_role_4.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_service.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_service1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_tls_config.yaml                │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_connector_tls_config1.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress1.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_grpcs.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_grpcs1.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_https.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_https1.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_ip.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_ip1.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_ip2.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_ip3.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_ip4.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_ingress_ip5.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_model_editor_role.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_model_viewer_role.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_pipeline_editor_role.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_pipeline_viewer_role.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_pipelineexecution_editor_role.yaml       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_pipelineexecution_viewer_role.yaml       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_repository_editor_role.yaml              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_repository_viewer_role.yaml              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_storage_editor_role.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloud_storage_viewer_role.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudbuild113.yaml                             │ kubernetes │         9         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudbuild113_1.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudbuild113_2.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudcontroller.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudcontroller_1.yaml                         │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudcontroller_2.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudcontroller_3.yaml                         │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudcontroller_4.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-api-key.sops.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-api-key.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-api-token-secret.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-clusterissuer.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-ddns-luke.yaml                      │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-ddns-raj.yaml                       │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-ddns2.yaml                          │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-ddns4.yaml                          │ kubernetes │        34         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-ddns4_1.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-deployment.yaml                     │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare-minecraft.yaml                      │ kubernetes │        15         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare.sops.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare.sops1.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare.yaml                                │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare1.yaml                               │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare1_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare2.yaml                               │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare2_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare3.yaml                               │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare3_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflare_1.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-api.yaml                           │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-deployment.yaml                    │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-deployment1.yaml                   │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-deployment2.yaml                   │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-deployment3.yaml                   │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-deployment4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-deployment4_1.yaml                 │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-luke.yaml                          │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-raj.yaml                           │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared-typeo.yaml                         │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared3.yaml                              │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared3_1.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared4.yaml                              │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared4_1.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared6.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared7.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared8.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflared9.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflaredtunnel_editor_role.yaml             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudflaredtunnel_viewer_role.yaml             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudquery_v1_cronjob.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudquery_v1_cronjob1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator.yaml              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator1.yaml             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator1_1.yaml           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator1_2.yaml           │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator1_3.yaml           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator_1.yaml            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator_2.yaml            │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-coordinator_3.yaml            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers1_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers1_2.yaml               │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers1_3.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers1_4.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers_2.yaml                │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers_3.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cloudretro-setup-workers_4.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluserole-binding.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ clusteip-definition.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-binding.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding2.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding3.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding4.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding5.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding6.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding7.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role-binding8.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role2.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role3.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role4.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role5.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role6.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role7.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role8.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin-role9.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin.yaml                             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin1.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin2.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin3.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-admin3_1.yaml                          │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_10.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_11.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_12.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_13.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_14.yaml               │ kubernetes │        10         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_17.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_18.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_8.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-api-provider-gcp_9.yaml                │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-assert.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover.yaml           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover1.yaml          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover1_1.yaml        │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover1_2.yaml        │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover1_3.yaml        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover1_4.yaml        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover1_5.yaml        │ kubernetes │        10         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover_1.yaml         │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover_2.yaml         │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover_3.yaml         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover_4.yaml         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscover_5.yaml         │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery.yaml          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery1.yaml         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery1_1.yaml       │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery1_2.yaml       │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery1_3.yaml       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery1_4.yaml       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery1_5.yaml       │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery_1.yaml        │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery_2.yaml        │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery_3.yaml        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery_4.yaml        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-autodiscovery_5.yaml        │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding.yaml                │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding1.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding10.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding11.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding13.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding14.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding15.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding16.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding17.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding18.yaml              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding2.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding3.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding4.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding5.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding6.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding7.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding8.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-binding9.yaml               │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac.yaml                   │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac1.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac10.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac10_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac11.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac11_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac13_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac14_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac15_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac16_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac17_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac18.yaml                 │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac18_1.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac1_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac2.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac2_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac3.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac3_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac4.yaml                  │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac4_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac5.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac5_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac6.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac6_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac7.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac7_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac8.yaml                  │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac8_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac9.yaml                  │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac9_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler-rbac_1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler10.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler10_1.yaml                    │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler10_2.yaml                    │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler10_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler10_4.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler10_5.yaml                    │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler12.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler12_1.yaml                    │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler12_2.yaml                    │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler12_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler12_4.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler12_5.yaml                    │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler13.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler13_1.yaml                    │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler13_2.yaml                    │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler13_3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler13_4.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler13_5.yaml                    │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler2.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler2_1.yaml                     │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler2_2.yaml                     │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler2_3.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler2_4.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler2_5.yaml                     │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler3.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler3_1.yaml                     │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler3_2.yaml                     │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler3_3.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler3_4.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler3_5.yaml                     │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler5.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler5_1.yaml                     │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler5_2.yaml                     │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler5_3.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler5_4.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler5_5.yaml                     │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler6.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler6_1.yaml                     │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler6_2.yaml                     │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler6_3.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler6_4.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler6_5.yaml                     │ kubernetes │        14         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler7.yaml                       │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler8.yaml                       │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler9.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler9_1.yaml                     │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler9_2.yaml                     │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler9_3.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler9_4.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler9_5.yaml                     │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler_1.yaml                      │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler_2.yaml                      │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler_3.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler_4.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-autoscaler_5.yaml                      │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-binding.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-binding1.yaml                          │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-binding2.yaml                          │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-connector.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-connector_1.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-connector_2.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-connector_3.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-connector_4.yaml                       │ kubernetes │        10         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-deployment.yaml                        │ kubernetes │        19         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-deployment_1.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-example-app-user.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-example-superuser.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip-definition.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip-service.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip-svc.yaml                            │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip-svc_1.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip-svc_2.yaml                          │ kubernetes │        17         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip-svc_3.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip.yaml                                │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip1.yaml                               │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip10_1.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip10_3.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip1_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip2.yaml                               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip3.yaml                               │ kubernetes │        18         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip4.yaml                               │ kubernetes │        19         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip4_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip5.yaml                               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip6.yaml                               │ kubernetes │        19         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip6_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip7.yaml                               │ kubernetes │        19         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip7_1.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip8.yaml                               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip9.yaml                               │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ip_1.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ipam.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-ipservice.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-local-ingress.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-local-ingress1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-local-ingress2.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-local-ingress3.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-manager.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-manager_1.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-manager_2.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-network-addons-operator-v0.95.0.yaml   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-network-addons-operator-v0.95.0_1.yaml │ kubernetes │         8         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-network-addons-operator-v0.95.0_2.yaml │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-network-addons-operator-v0.95.0_3.yaml │ kubernetes │         5         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-network-addons-operator-v0.95.0_4.yaml │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-network-addons-operator-v0.95.0_5.yaml │ kubernetes │        12         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-nginx-deployment.yaml                  │ kubernetes │        19         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-proxy-admin-rbac.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-proxy-admin-rbac_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-rbac.yaml                              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-rbac_1.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-resolver-config-cm.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin1.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin2.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin2_1.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin3.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin3_1.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin4.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-admin4_1.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-aggregated-metrics-reader.yaml    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-aggregated-metrics-reader1.yaml   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-bind.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-delegator.yaml            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-delegator1.yaml           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-group.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-hpa-custom-metrics.yaml   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-hpa-custom-metrics1.yaml  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-readers.yaml              │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding-sa.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding.yaml.verified.yaml        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding1.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding10.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding101.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding102.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding103.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding104.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding105.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding106.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding107.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding11.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding116.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding118.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding119.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding12.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding120.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding121.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding122.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding123.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding124.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding125.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding126.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding127.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding128.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding129.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding13.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding130.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding131.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding132.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding133.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding134.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding135.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding136.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding137.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding138.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding139.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding14.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding140.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding141.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding142.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding143.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding144.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding145.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding146.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding147.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding148.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding149.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding15.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding150.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding151.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding152.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding153.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding154.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding156.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding157.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding159.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding16.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding160.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding161.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding162.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding163.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding164.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding165.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding166.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding167.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding168.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding169.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding17.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding170.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding171.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding172.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding173.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding174.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding175.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding176.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding177.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding178.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding179.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding18.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding180.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding181.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding182.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding183.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding184.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding185.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding186.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding187.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding188.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding189.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding19.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding190.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding191.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding192.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding193.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding194.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding195.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding196.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding197.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding198.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding199.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding2.yaml                     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding20.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding200.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding201.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding202.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding203.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding204.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding205.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding206.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding21.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding212.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding213.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding214.yaml                   │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding215.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding216.yaml                   │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding23.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding24.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding25.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding26.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding27.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding28.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding30.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding31.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding32.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding35.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding36.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding37.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding38.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding39.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding4.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding40.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding41.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding42.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding43.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding44.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding45.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding46.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding47.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding49.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding5.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding50.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding51.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding52.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding53.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding54.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding55.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding56.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding57.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding58.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding59.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding6.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding60.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding61.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding63.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding64.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding65.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding66.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding67.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding68.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding69.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding70.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding71.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding72.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding73.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding74.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding75.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding76.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding77.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding78.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding79.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding80.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding81.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding82.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding83.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding84.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding85.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding86.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding87.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding88.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding89.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding9.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding90.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding91.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding92.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding94.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding96.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding97.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binding99.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-binfing.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-delete-pods.yaml                  │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-dev-bind.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-dev.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-metrics-server-resources.yaml     │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-metrics-server-resources1.yaml    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-n-binding.yaml                    │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-n-binding_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-readers.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view1.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view2.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view2_1.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view3.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view3_1.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view4.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role-view4_1.yaml                      │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role.yaml.verified.yaml                │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role1.yaml                             │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role10.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role100.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role101.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role102.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role103.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role104.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role104_1.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role107.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role108.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role109.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role11.yaml                            │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role110.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role111.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role112.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role113.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role114.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role115.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role117.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role119.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role12.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role120.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role121.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role122.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role123.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role124.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role125.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role126.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role127.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role128.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role129.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role13.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role130.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role131.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role132.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role133.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role134.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role135.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role138.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role14.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role142.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role143.yaml                           │ kubernetes │         9         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role144.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role145.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role146.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role147.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role148.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role149.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role15.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role150.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role151.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role152.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role153.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role154.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role155.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role156.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role157.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role158.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role159.yaml                           │ kubernetes │        11         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role16.yaml                            │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role160.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role161.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role161_1.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role162.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role164.yaml                           │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role165.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role166.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role167.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role168.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role169.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role17.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role170.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role171.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role172.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role173.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role174.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role175.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role176.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role177.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role178.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role179.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role18.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role180.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role181.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role182.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role183.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role185.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role186.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role188.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role188_1.yaml                         │ kubernetes │         2         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role188_2.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role189.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role19.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role190.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role191.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role192.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role193.yaml                           │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role194.yaml                           │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role195.yaml                           │ kubernetes │         4         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role196.yaml                           │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role197.yaml                           │ kubernetes │         3         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role198.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role199.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role2.yaml                             │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role20.yaml                            │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role200.yaml                           │ kubernetes │         7         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role201.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cluster-role202.yaml                           │ kubernetes │         1         │
└────────────────────────────────────────────────┴────────────┴───────────────────┘
Legend:
- '-': Not scanned
- '0': Clean (no security findings detected)


cloud-node-controller-role4.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cloud-node-controller-role4.yaml:62-70
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - create
  68 │   - delete
  69 │   - get
  70 └   - update
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cloud-node-controller-role4.yaml:77-83
────────────────────────────────────────
  77 ┌ - apiGroups:
  78 │   - '*'
  79 │   resources:
  80 │   - '*'
  81 │   verbs:
  82 │   - list
  83 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:cloud-controller-manager' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloud-node-controller-role4.yaml:32-40
────────────────────────────────────────
  32 ┌ - apiGroups:
  33 │   - ''
  34 │   resources:
  35 │   - endpoints
  36 │   - serviceaccounts
  37 │   verbs:
  38 │   - create
  39 │   - get
  40 └   - update
────────────────────────────────────────



cloud-node-controller-role4_1.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system:cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role4_1.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role4_2.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system::leader-locking-cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role4_2.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role4_3.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'system:controller:cloud-node-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud-node-controller-role4_3.yaml:36-42
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - pods
  40 │   verbs:
  41 │   - list
  42 └   - delete
────────────────────────────────────────



cloud-node-controller-role5.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cloud-node-controller-role5.yaml:62-70
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - create
  68 │   - delete
  69 │   - get
  70 └   - update
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cloud-node-controller-role5.yaml:77-83
────────────────────────────────────────
  77 ┌ - apiGroups:
  78 │   - '*'
  79 │   resources:
  80 │   - '*'
  81 │   verbs:
  82 │   - list
  83 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:cloud-controller-manager' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloud-node-controller-role5.yaml:32-40
────────────────────────────────────────
  32 ┌ - apiGroups:
  33 │   - ''
  34 │   resources:
  35 │   - endpoints
  36 │   - serviceaccounts
  37 │   verbs:
  38 │   - create
  39 │   - get
  40 └   - update
────────────────────────────────────────



cloud-node-controller-role5_1.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system:cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role5_1.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role5_2.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system::leader-locking-cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role5_2.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role5_3.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'system:controller:cloud-node-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud-node-controller-role5_3.yaml:36-42
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - pods
  40 │   verbs:
  41 │   - list
  42 └   - delete
────────────────────────────────────────



cloud-node-controller-role7.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cloud-node-controller-role7.yaml:62-70
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - create
  68 │   - delete
  69 │   - get
  70 └   - update
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cloud-node-controller-role7.yaml:77-83
────────────────────────────────────────
  77 ┌ - apiGroups:
  78 │   - '*'
  79 │   resources:
  80 │   - '*'
  81 │   verbs:
  82 │   - list
  83 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:cloud-controller-manager' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloud-node-controller-role7.yaml:32-40
────────────────────────────────────────
  32 ┌ - apiGroups:
  33 │   - ''
  34 │   resources:
  35 │   - endpoints
  36 │   - serviceaccounts
  37 │   verbs:
  38 │   - create
  39 │   - get
  40 └   - update
────────────────────────────────────────



cloud-node-controller-role7_1.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system:cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role7_1.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role7_2.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system::leader-locking-cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role7_2.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role7_3.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'system:controller:cloud-node-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud-node-controller-role7_3.yaml:36-42
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - pods
  40 │   verbs:
  41 │   - list
  42 └   - delete
────────────────────────────────────────



cloud-node-controller-role8.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cloud-node-controller-role8.yaml:62-70
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - create
  68 │   - delete
  69 │   - get
  70 └   - update
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cloud-node-controller-role8.yaml:77-83
────────────────────────────────────────
  77 ┌ - apiGroups:
  78 │   - '*'
  79 │   resources:
  80 │   - '*'
  81 │   verbs:
  82 │   - list
  83 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:cloud-controller-manager' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloud-node-controller-role8.yaml:32-40
────────────────────────────────────────
  32 ┌ - apiGroups:
  33 │   - ''
  34 │   resources:
  35 │   - endpoints
  36 │   - serviceaccounts
  37 │   verbs:
  38 │   - create
  39 │   - get
  40 └   - update
────────────────────────────────────────



cloud-node-controller-role8_1.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system:cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role8_1.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role8_2.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system::leader-locking-cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role8_2.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role8_3.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'system:controller:cloud-node-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud-node-controller-role8_3.yaml:36-42
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - pods
  40 │   verbs:
  41 │   - list
  42 └   - delete
────────────────────────────────────────



cloud-node-controller-role9.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cloud-node-controller-role9.yaml:62-70
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - create
  68 │   - delete
  69 │   - get
  70 └   - update
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:cloud-controller-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cloud-node-controller-role9.yaml:77-83
────────────────────────────────────────
  77 ┌ - apiGroups:
  78 │   - '*'
  79 │   resources:
  80 │   - '*'
  81 │   verbs:
  82 │   - list
  83 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:cloud-controller-manager' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloud-node-controller-role9.yaml:32-40
────────────────────────────────────────
  32 ┌ - apiGroups:
  33 │   - ''
  34 │   resources:
  35 │   - endpoints
  36 │   - serviceaccounts
  37 │   verbs:
  38 │   - create
  39 │   - get
  40 └   - update
────────────────────────────────────────



cloud-node-controller-role9_1.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system:cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role9_1.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role9_2.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system::leader-locking-cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role9_2.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role9_3.yaml (kubernetes)
===============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'system:controller:cloud-node-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud-node-controller-role9_3.yaml:36-42
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - pods
  40 │   verbs:
  41 │   - list
  42 └   - delete
────────────────────────────────────────



cloud-node-controller-role_1.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system:cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role_1.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role_2.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'system::leader-locking-cloud-controller-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-node-controller-role_2.yaml:15-23
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cloud-controller-manager
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



cloud-node-controller-role_3.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'system:controller:cloud-node-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud-node-controller-role_3.yaml:36-42
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - pods
  40 │   verbs:
  41 │   - list
  42 └   - delete
────────────────────────────────────────



cloud-provider-role.yaml (kubernetes)
=====================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role1.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role1.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role10.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role10.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role10_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role10_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role11.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role11.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role11_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role11_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role13.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role13.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role13_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role13_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role14.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role14.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role14_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role14_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role15.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role15.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role15_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role15_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role16.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role16.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role16_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role16_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role17.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role17.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role17_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role17_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role18.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role18.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role18_2.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role18_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role1_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role1_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role2.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role2.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role2_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role2_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role3.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role3.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role3_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role3_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role4.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role4.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role4_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role4_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role5.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role5.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role5_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role5_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role6.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role6.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role6_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role6_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role7.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role7.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role7_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role7_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role8.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role8.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role8_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role8_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role9.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'gce:cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role9.yaml:9-19
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - create
  15 │   - get
  16 │   - patch
  17 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role9_2.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role9_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud-provider-role_2.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloud-provider' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloud-provider-role_2.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   verbs:
  17 │   - create
  18 │   - get
  19 │   - patch
  20 └   - update
  ..   
────────────────────────────────────────



cloud_connector_deployment.yaml (kubernetes)
============================================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 9, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloud_connector_deployment.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloud_connector_deployment.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'app' of Deployment 'vizier-cloud-connector' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloud_connector_deployment.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'app' of Deployment 'vizier-cloud-connector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloud_connector_deployment.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloud_connector_deployment.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloud_connector_deployment.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloud_connector_deployment.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloud_connector_deployment.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloud_connector_deployment.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloud_connector_deployment.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloud_connector_deployment.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment vizier-cloud-connector in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloud_connector_deployment.yaml:5
────────────────────────────────────────
   5 [   name: vizier-cloud-connector
────────────────────────────────────────



cloud_connector_deployment1.yaml (kubernetes)
=============================================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 9, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloud_connector_deployment1.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloud_connector_deployment1.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'app' of Deployment 'vizier-cloud-connector' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloud_connector_deployment1.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'app' of Deployment 'vizier-cloud-connector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloud_connector_deployment1.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloud_connector_deployment1.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloud_connector_deployment1.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloud_connector_deployment1.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloud_connector_deployment1.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloud_connector_deployment1.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'app' of Deployment 'vizier-cloud-connector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloud_connector_deployment1.yaml:65-123
────────────────────────────────────────
  65 ┌       - name: app
  66 │         image: vizier-cloud_connector_server_image:latest
  67 │         env:
  68 │         - name: PL_JWT_SIGNING_KEY
  69 │           valueFrom:
  70 │             secretKeyRef:
  71 │               key: jwt-signing-key
  72 │               name: pl-cluster-secrets
  73 └         - name: PL_CLUSTER_ID
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nats-wait' of Deployment 'vizier-cloud-connector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloud_connector_deployment1.yaml:37-63
────────────────────────────────────────
  37 ┌       - name: nats-wait
  38 │         # yamllint disable-line rule:line-length
  39 │         image: gcr.io/pixie-oss/pixie-dev-public/curl:multiarch-7.87.0@sha256:f7f265d5c64eb4463a43a99b6bf773f9e61a50aaa7cefaf564f43e42549a01dd
  40 │         # yamllint disable rule:indentation
  41 │         command: ['sh', '-c', 'set -xe;
  42 │           URL="${PROTOCOL}://${SERVICE_NAME}:${SERVICE_PORT}${HEALTH_PATH}";
  43 │           until [ $(curl -m 0.5 -s -o /dev/null -w "%{http_code}" -k ${URL}) -eq 200 ]; do
  44 │             echo "waiting for ${URL}";
  45 └             sleep 2;
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment vizier-cloud-connector in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloud_connector_deployment1.yaml:5
────────────────────────────────────────
   5 [   name: vizier-cloud-connector
────────────────────────────────────────



cloud_connector_role1_3.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): Role 'pl-cloud-connector-ns-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud_connector_role1_3.yaml:16-27
────────────────────────────────────────
  16 ┌ - apiGroups:
  17 │   - batch
  18 │   resources:
  19 │   - jobs
  20 │   verbs:
  21 │   - create
  22 │   - delete
  23 │   - get
  24 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): Role 'pl-cloud-connector-ns-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud_connector_role1_3.yaml:28-40
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   - pods
  33 │   verbs:
  34 │   - create
  35 │   - delete
  36 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'pl-cloud-connector-ns-role' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cloud_connector_role1_3.yaml:28-40
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   - pods
  33 │   verbs:
  34 │   - create
  35 │   - delete
  36 └   - get
  ..   
────────────────────────────────────────



cloud_connector_role_3.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): Role 'pl-cloud-connector-ns-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud_connector_role_3.yaml:16-27
────────────────────────────────────────
  16 ┌ - apiGroups:
  17 │   - batch
  18 │   resources:
  19 │   - jobs
  20 │   verbs:
  21 │   - create
  22 │   - delete
  23 │   - get
  24 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): Role 'pl-cloud-connector-ns-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cloud_connector_role_3.yaml:28-40
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   - pods
  33 │   verbs:
  34 │   - create
  35 │   - delete
  36 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'pl-cloud-connector-ns-role' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cloud_connector_role_3.yaml:28-40
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   - pods
  33 │   verbs:
  34 │   - create
  35 │   - delete
  36 └   - get
  ..   
────────────────────────────────────────



cloud_connector_tls_config.yaml (kubernetes)
============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'pl-cloud-connector-tls-config' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"PL_CLIENT_TLS_KEY", "PL_SERVER_TLS_KEY"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────
 cloud_connector_tls_config.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────



cloud_connector_tls_config1.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'pl-cloud-connector-tls-config' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"PL_CLIENT_TLS_KEY", "PL_SERVER_TLS_KEY"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────
 cloud_connector_tls_config1.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────



cloudbuild113.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 5, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'laravel-kube-1' of Deployment 'laravel-kube' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'laravel-kube-1' of Deployment 'laravel-kube' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'laravel-kube-1' of Deployment 'laravel-kube' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'laravel-kube-1' of Deployment 'laravel-kube' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'laravel-kube-1' of Deployment 'laravel-kube' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'laravel-kube-1' of Deployment 'laravel-kube' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'laravel-kube-1' of Deployment 'laravel-kube' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudbuild113.yaml:110-125
────────────────────────────────────────
 110 ┌       - image: gcr.io/silicon-monitor-417609/github.com/ggabri4/laravel-kube
 111 │         imagePullPolicy: IfNotPresent
 112 │         name: laravel-kube-1
 113 │         resources:
 114 │           limits:
 115 │             cpu: 500m
 116 │             ephemeral-storage: 1Gi
 117 │             memory: 2Gi
 118 └           requests:
 ...   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment laravel-kube in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudbuild113.yaml:4-93
────────────────────────────────────────
   4 ┌   annotations:
   5 │     autopilot.gke.io/resource-adjustment: '{"input":{"containers":[{"name":"laravel-kube-1"}]},"output":{"containers":[{"limits":{"cpu":"500m","ephemeral-storage":"1Gi","memory":"2Gi"},"requests":{"cpu":"500m","ephemeral-storage":"1Gi","memory":"2Gi"},"name":"laravel-kube-1"}]},"modified":true}'
   6 │     autopilot.gke.io/warden-version: 2.7.52
   7 │   labels:
   8 │     app: laravel-kube
   9 │   managedFields:
  10 │   - apiVersion: apps/v1
  11 │     fieldsType: FieldsV1
  12 └     fieldsV1:
  ..   
────────────────────────────────────────



cloudcontroller_1.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'system:kube-vip-cloud-controller-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cloudcontroller_1.yaml:18-27
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   - endpoints
  23 │   - events
  24 │   - services/status
  25 │   - leases
  26 │   verbs:
  27 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:kube-vip-cloud-controller-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloudcontroller_1.yaml:18-27
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   - endpoints
  23 │   - events
  24 │   - services/status
  25 │   - leases
  26 │   verbs:
  27 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:kube-vip-cloud-controller-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cloudcontroller_1.yaml:28-37
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - nodes
  32 │   - services
  33 │   verbs:
  34 │   - list
  35 │   - get
  36 │   - watch
  37 └   - update
────────────────────────────────────────



cloudcontroller_3.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 10, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kube-vip-cloud-provider' of 'deployment' 'kube-vip-cloud-provider' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-vip-cloud-provider' of Deployment 'kube-vip-cloud-provider' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'kube-vip-cloud-provider' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cloudcontroller_3.yaml:7-52
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   revisionHistoryLimit: 10
   9 │   selector:
  10 │     matchLabels:
  11 │       app: kube-vip
  12 │       component: kube-vip-cloud-provider
  13 │   strategy:
  14 │     rollingUpdate:
  15 └       maxSurge: 25%
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-vip-cloud-provider" of deployment "kube-vip-cloud-provider" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container kube-vip-cloud-provider in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment kube-vip-cloud-provider in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudcontroller_3.yaml:24-52
────────────────────────────────────────
  24 ┌       containers:
  25 │       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 │         imagePullPolicy: Always
  31 │       dnsPolicy: ClusterFirst
  32 └       restartPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kube-vip-cloud-provider in deployment kube-vip-cloud-provider (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudcontroller_3.yaml:25-30
────────────────────────────────────────
  25 ┌       - command:
  26 │         - /kube-vip-cloud-provider
  27 │         - --leader-elect-resource-name=kube-vip-cloud-controller
  28 │         image: ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.9
  29 │         name: kube-vip-cloud-provider
  30 └         imagePullPolicy: Always
────────────────────────────────────────



cloudflare-ddns-luke.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 97, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflare-ddns' of 'deployment' 'cloudflare-ddns-luke' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-luke' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflare-ddns" of deployment "cloudflare-ddns-luke" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflare-ddns-luke in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:4-6
────────────────────────────────────────
   4 ┌   name: cloudflare-ddns-luke
   5 │   labels:
   6 └     app: cloudflare-ddns-luke
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflare-ddns in deployment cloudflare-ddns-luke (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-ddns-luke.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-lukehouge-com
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────



cloudflare-ddns-raj.yaml (kubernetes)
=====================================
Tests: 114 (SUCCESSES: 97, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflare-ddns' of 'deployment' 'cloudflare-ddns-raj' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns-raj' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflare-ddns" of deployment "cloudflare-ddns-raj" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflare-ddns-raj in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:4-6
────────────────────────────────────────
   4 ┌   name: cloudflare-ddns-raj
   5 │   labels:
   6 └     app: cloudflare-ddns-raj
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflare-ddns in deployment cloudflare-ddns-raj (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-ddns-raj.yaml:18-31
────────────────────────────────────────
  18 ┌         - name: cloudflare-ddns
  19 │           image: favonia/cloudflare-ddns:latest
  20 │           env:
  21 │             - name: CLOUDFLARE_API_TOKEN
  22 │               valueFrom:
  23 │                 secretKeyRef:
  24 │                   name: cloudflare
  25 │                   key: cloudflare-k8s-token-rajsingh-info
  26 └             - name: DOMAINS
  ..   
────────────────────────────────────────



cloudflare-ddns2.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 10, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflare-ddns' of 'deployment' 'cloudflare-ddns' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflare-ddns" of deployment "cloudflare-ddns" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflare-ddns in cert-manager namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflare-ddns in cert-manager namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-ddns2.yaml:19-30
────────────────────────────────────────
  19 ┌       containers:
  20 │       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 └                 name: cloudflare-api-token-secret
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflare-ddns in deployment cloudflare-ddns (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-ddns2.yaml:20-30
────────────────────────────────────────
  20 ┌       - image: oznu/cloudflare-ddns
  21 │         name: cloudflare-ddns
  22 │         resources: {}
  23 │         env:
  24 │           - name: API_KEY
  25 │             valueFrom:
  26 │               secretKeyRef:
  27 │                 name: cloudflare-api-token-secret
  28 └                 key: cloudflare_api_token
  ..   
────────────────────────────────────────



cloudflare-ddns4.yaml (kubernetes)
==================================
Tests: 130 (SUCCESSES: 96, FAILURES: 34)
Failures: 34 (UNKNOWN: 0, LOW: 19, MEDIUM: 10, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflare-ddns' of 'deployment' 'cloudflare-ddns' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'init-config' of 'deployment' 'cloudflare-ddns' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'init-config' of Deployment 'cloudflare-ddns' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflare-ddns' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'init-config' of Deployment 'cloudflare-ddns' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflare-ddns" of deployment "cloudflare-ddns" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "init-config" of deployment "cloudflare-ddns" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflare-ddns in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare-ddns4.yaml:4-6
────────────────────────────────────────
   4 ┌   labels:
   5 │     app: cloudflare-ddns
   6 └   name: cloudflare-ddns
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflare-ddns in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflare-ddns in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflare-ddns in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-ddns4.yaml:17-69
────────────────────────────────────────
  17 ┌       initContainers:
  18 │       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 └         - name: API_TOKEN
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflare-ddns in deployment cloudflare-ddns (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-ddns4.yaml:51-63
────────────────────────────────────────
  51 ┌       - name: cloudflare-ddns
  52 │         image: timothyjmiller/cloudflare-ddns:latest
  53 │         env:
  54 │         - name: CONFIG_PATH
  55 │           value: /etc/cloudflare-ddns/
  56 │         volumeMounts:
  57 │         - mountPath: /etc/cloudflare-ddns
  58 │           name: config
  59 └           readOnly: true
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container init-config in deployment cloudflare-ddns (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-ddns4.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: init-config
  19 │         image: bhgedigital/envsubst:latest
  20 │         command:
  21 │         - sh
  22 │         - -c
  23 │         - envsubst < /cloudflare-ddns-config-template/config.json > /config/config.json
  24 │         env:
  25 │         - name: API_TOKEN
  26 └           valueFrom:
  ..   
────────────────────────────────────────



cloudflare-deployment.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare-deployment.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-deployment.yaml:15-45
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 └         - tunnel
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:1517-bb29a0e19437
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on. 
  ..   
────────────────────────────────────────



cloudflare-minecraft.yaml (kubernetes)
======================================
Tests: 115 (SUCCESSES: 100, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 7, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'minecraft-cloudflare-tunnel' of 'deployment' 'minecraft-cloudflare-tunnel' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'minecraft-cloudflare-tunnel' of Deployment 'minecraft-cloudflare-tunnel' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "minecraft-cloudflare-tunnel" of deployment "minecraft-cloudflare-tunnel" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment minecraft-cloudflare-tunnel in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare-minecraft.yaml:4
────────────────────────────────────────
   4 [   name: minecraft-cloudflare-tunnel
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container minecraft-cloudflare-tunnel in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment minecraft-cloudflare-tunnel in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare-minecraft.yaml:15-32
────────────────────────────────────────
  15 ┌       containers:
  16 │         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 └                   name: "minecraft-cloudflare-token"
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container minecraft-cloudflare-tunnel in deployment minecraft-cloudflare-tunnel (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare-minecraft.yaml:16-32
────────────────────────────────────────
  16 ┌         - name: minecraft-cloudflare-tunnel
  17 │           image: cloudflare/cloudflared:latest
  18 │           env:
  19 │             - name: token
  20 │               valueFrom:
  21 │                 secretKeyRef:
  22 │                   key: "tunnel_token"
  23 │                   name: "minecraft-cloudflare-token"
  24 └           command: ["cloudflared"]
  ..   
────────────────────────────────────────



cloudflare.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare.yaml:16-51
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────



cloudflare1.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare1.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare1.yaml:16-51
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare1.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────



cloudflare2.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare2.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare2.yaml:16-62
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare2.yaml:17-43
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────



cloudflare3.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflare3.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflare3.yaml:16-51
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflare3.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:2025.2.1
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────



cloudflared-api.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-api' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared-api' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared-api' in 'argocd' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-api' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-api' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared-api' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared-api' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared-api' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared-api" in "argocd" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared-api in argocd namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared-api in argocd namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-api.yaml:17-49
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 └         - --metrics
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared-api (namespace: argocd) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-api.yaml:18-49
────────────────────────────────────────
  18 ┌       - name: cloudflared
  19 │         image: docker.io/cloudflare/cloudflared:latest
  20 │         ports:
  21 │           - name: metrics
  22 │             containerPort: 2000
  23 │         args:
  24 │         - tunnel
  25 │         - --metrics
  26 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────



cloudflared-deployment.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflared-deployment.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment.yaml:15-45
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 └         - tunnel
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-deployment.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────



cloudflared-deployment1.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflared-deployment1.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment1.yaml:15-45
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 └         - tunnel
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-deployment1.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────



cloudflared-deployment2.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'openhands' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "openhands" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in openhands namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in openhands namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment2.yaml:16-57
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 └         - --metrics
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: openhands) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-deployment2.yaml:17-48
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: docker.io/cloudflare/cloudflared:latest
  19 │         ports:
  20 │           - name: metrics
  21 │             containerPort: 2000
  22 │         args:
  23 │         - tunnel
  24 │         - --metrics
  25 └         - 0.0.0.0:2000
  ..   
────────────────────────────────────────



cloudflared-deployment3.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflared-deployment3.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment3.yaml:15-45
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 └         - tunnel
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-deployment3.yaml:16-45
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: docker.io/cloudflare/cloudflared:2025.2.1
  18 │         ports:
  19 │           - name: metrics
  20 │             containerPort: 2000
  21 │         imagePullPolicy: IfNotPresent
  22 │         args:
  23 │         - tunnel
  24 └         # In a k8s environment, the metrics server needs to listen outside the pod it runs on.
  ..   
────────────────────────────────────────



cloudflared-deployment4_1.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 10, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'cloudflare' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "cloudflare" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in cloudflare namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in cloudflare namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:16-47
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: cloudflare) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-deployment4_1.yaml:17-37
────────────────────────────────────────
  17 ┌       - name: cloudflared
  18 │         image: cloudflare/cloudflared:latest
  19 │         args:
  20 │         - tunnel
  21 │         - --config
  22 │         - /etc/cloudflared/config/config.yaml
  23 │         - run
  24 │         livenessProbe:
  25 └           httpGet:
  ..   
────────────────────────────────────────



cloudflared-luke.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-luke' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared-luke' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared-luke' in 'cloudflared' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-luke' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-luke' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared-luke' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared-luke' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared-luke' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared-luke" in "cloudflared" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared-luke in cloudflared namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared-luke in cloudflared namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-luke.yaml:18-49
────────────────────────────────────────
  18 ┌       containers:
  19 │         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 └             - --token
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared-luke (namespace: cloudflared) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-luke.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────



cloudflared-raj.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-raj' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared-raj' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared-raj' in 'cloudflared' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-raj' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-raj' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared-raj' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared-raj' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared-raj' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared-raj" in "cloudflared" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared-raj in cloudflared namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared-raj in cloudflared namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-raj.yaml:18-49
────────────────────────────────────────
  18 ┌       containers:
  19 │         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 └             - --token
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared-raj (namespace: cloudflared) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-raj.yaml:19-49
────────────────────────────────────────
  19 ┌         - command:
  20 │             - cloudflared
  21 │             - tunnel
  22 │             - --metrics
  23 │             - 0.0.0.0:2000
  24 │             - run
  25 │           args:
  26 │             - --token
  27 └             - $(token)
  ..   
────────────────────────────────────────



cloudflared-typeo.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-typeo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared-typeo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared-typeo' in 'cloudflared' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-typeo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared-typeo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared-typeo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared-typeo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared-typeo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared-typeo" in "cloudflared" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared-typeo in cloudflared namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared-typeo in cloudflared namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared-typeo.yaml:18-49
────────────────────────────────────────
  18 ┌       containers:
  19 │       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 └         - --token 
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared-typeo (namespace: cloudflared) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared-typeo.yaml:19-49
────────────────────────────────────────
  19 ┌       - command:
  20 │         - cloudflared
  21 │         - tunnel
  22 │         - --metrics
  23 │         - 0.0.0.0:2000
  24 │         - run
  25 │         args:
  26 │         - --token 
  27 └         - $(token)
  ..   
────────────────────────────────────────



cloudflared3.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflared3.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared3.yaml:15-46
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared3.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2022.3.0
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────



cloudflared3_1.yaml (kubernetes)
================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'cloudflared' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"config.yaml"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



cloudflared4.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cloudflared' of 'deployment' 'cloudflared' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cloudflared' of Deployment 'cloudflared' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cloudflared" of deployment "cloudflared" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cloudflared in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cloudflared4.yaml:4
────────────────────────────────────────
   4 [   name: cloudflared
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cloudflared in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cloudflared in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudflared4.yaml:15-46
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cloudflared in deployment cloudflared (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudflared4.yaml:16-36
────────────────────────────────────────
  16 ┌       - name: cloudflared
  17 │         image: cloudflare/cloudflared:2024.2.0-arm64
  18 │         args:
  19 │         - tunnel
  20 │         - --config
  21 │         - /etc/cloudflared/config/config.yaml
  22 │         - run
  23 │         livenessProbe:
  24 └           httpGet:
  ..   
────────────────────────────────────────



cloudretro-setup-coordinator1_2.yaml (kubernetes)
=================================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 10, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'coordinator' of 'deployment' 'coordinator-deployment' in 'cloudretro' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'coordinator-deployment' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:7-33
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: coordinator
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: coordinator
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "coordinator" of deployment "coordinator-deployment" in "cloudretro" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container coordinator-deployment in cloudretro namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment coordinator-deployment in cloudretro namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:16-33
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container coordinator in deployment coordinator-deployment (namespace: cloudretro) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudretro-setup-coordinator1_2.yaml:17-28
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - ./coordinator
  24 │         ports:
  25 └         - containerPort: 8000
  ..   
────────────────────────────────────────



cloudretro-setup-coordinator_2.yaml (kubernetes)
================================================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'coordinator' of 'deployment' 'coordinator-deployment' in 'cloudretro' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'coordinator' of Deployment 'coordinator-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "coordinator" of deployment "coordinator-deployment" in "cloudretro" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container coordinator-deployment in cloudretro namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment coordinator-deployment in cloudretro namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:16-27
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container coordinator in deployment coordinator-deployment (namespace: cloudretro) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudretro-setup-coordinator_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: coordinator
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-c
  22 │         command:
  23 │         - coordinator
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────



cloudretro-setup-workers1_2.yaml (kubernetes)
=============================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 10, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'worker' of Deployment 'worker-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'worker' of 'deployment' 'worker-deployment' in 'cloudretro' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'worker-deployment' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:7-34
────────────────────────────────────────
   7 ┌   replicas: 2
   8 │   selector:
   9 │     matchLabels:
  10 │       app: worker
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: worker
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "worker" of deployment "worker-deployment" in "cloudretro" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container worker-deployment in cloudretro namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment worker-deployment in cloudretro namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:16-34
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container worker in deployment worker-deployment (namespace: cloudretro) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudretro-setup-workers1_2.yaml:17-29
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: ghcr.io/giongto35/cloud-game/cloud-game:v3.0.5
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - ./worker
  24 │         ports:
  25 └         - containerPort: 9000
  ..   
────────────────────────────────────────



cloudretro-setup-workers_2.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'worker' of Deployment 'worker-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'worker' of 'deployment' 'worker-deployment' in 'cloudretro' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'worker' of Deployment 'worker-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "worker" of deployment "worker-deployment" in "cloudretro" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container worker-deployment in cloudretro namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment worker-deployment in cloudretro namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:16-27
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container worker in deployment worker-deployment (namespace: cloudretro) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cloudretro-setup-workers_2.yaml:17-27
────────────────────────────────────────
  17 ┌       - name: worker
  18 │         image: valniae/snekyrepo:crdi
  19 │         envFrom:
  20 │         - configMapRef:
  21 │             name: cloudretro-config-w
  22 │         command:
  23 │         - worker
  24 │         args:
  25 └         - --v=5
  ..   
────────────────────────────────────────



cluster-admin-binding.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-admin-binding.yaml:4
────────────────────────────────────────
   4 [   name: cluster-admin-user
────────────────────────────────────────



cluster-admin-role-binding3.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-admin-role-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-admin-role-binding3.yaml:4
────────────────────────────────────────
   4 [   name: cluster-admin-role-binding
────────────────────────────────────────



cluster-admin.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-admin-custom-ricardo' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-admin.yaml:4
────────────────────────────────────────
   4 [   name: cluster-admin-custom-ricardo
────────────────────────────────────────



cluster-admin1.yaml (kubernetes)
================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'redacted-rbac' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-admin1.yaml:4
────────────────────────────────────────
   4 [   name: redacted-rbac
────────────────────────────────────────



cluster-admin2.yaml (kubernetes)
================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-admin-custom-ricardo' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-admin2.yaml:4
────────────────────────────────────────
   4 [   name: cluster-admin-custom-ricardo
────────────────────────────────────────



cluster-admin3_1.yaml (kubernetes)
==================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-clusterrolebinding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-admin3_1.yaml:4
────────────────────────────────────────
   4 [   name: admin-clusterrolebinding
────────────────────────────────────────



cluster-api-provider-gcp_10.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'capg-manager-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-api-provider-gcp_10.yaml:20-31
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - create
  26 │   - delete
  27 │   - get
  28 └   - list
  ..   
────────────────────────────────────────



cluster-api-provider-gcp_14.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 104, FAILURES: 10)
Failures: 10 (UNKNOWN: 0, LOW: 5, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'manager' of Deployment 'capg-controller-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'manager' of Deployment 'capg-controller-manager' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'capg-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'manager' of Deployment 'capg-controller-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'manager' of Deployment 'capg-controller-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'manager' of Deployment 'capg-controller-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "capg-controller-manager" in "capg-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment capg-controller-manager in capg-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:23-97
────────────────────────────────────────
  23 ┌       affinity:
  24 │         nodeAffinity:
  25 │           requiredDuringSchedulingIgnoredDuringExecution:
  26 │             nodeSelectorTerms:
  27 │             - matchExpressions:
  28 │               - key: node-role.kubernetes.io/control-plane
  29 │                 operator: Exists
  30 │             - matchExpressions:
  31 └               - key: node-role.kubernetes.io/master
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment capg-controller-manager (namespace: capg-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-api-provider-gcp_14.yaml:34-85
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --leader-elect
  36 │         - --feature-gates=GKE=false
  37 │         - --metrics-bind-addr=localhost:8080
  38 │         - --v=0
  39 │         env:
  40 │         - name: NODE_NAME
  41 │           valueFrom:
  42 └             fieldRef:
  ..   
────────────────────────────────────────



cluster-api-provider-gcp_9.yaml (kubernetes)
============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'capg-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-api-provider-gcp_9.yaml:9-20
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - configmaps
  13 │   verbs:
  14 │   - get
  15 │   - list
  16 │   - watch
  17 └   - create
  ..   
────────────────────────────────────────



cluster-assert.yaml (kubernetes)
================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0110 (LOW): statefulset airflowcluster-webservers-default in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-assert.yaml:4
────────────────────────────────────────
   4 [   name: airflowcluster-webservers-default
────────────────────────────────────────



cluster-autoscaler-autodiscover1_1.yaml (kubernetes)
====================================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler-autodiscover1_2.yaml (kubernetes)
====================================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscover1_5.yaml (kubernetes)
====================================================
Tests: 114 (SUCCESSES: 104, FAILURES: 10)
Failures: 10 (UNKNOWN: 0, LOW: 5, MEDIUM: 4, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-autoscaler' of 'deployment' 'cluster-autoscaler' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:9-53
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:9-53
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-autoscaler" of deployment "cluster-autoscaler" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-autoscaler-autodiscover1_5.yaml:28-49
────────────────────────────────────────
  28 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.2
  29 │         name: cluster-autoscaler
  30 │         resources:
  31 │           limits:
  32 │             cpu: 100m
  33 │             memory: 600Mi
  34 │           requests:
  35 │             cpu: 100m
  36 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscover_1.yaml (kubernetes)
===================================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler-autodiscover_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscover_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscover_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler-autodiscover_2.yaml (kubernetes)
===================================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscover_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscover_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscover_5.yaml (kubernetes)
===================================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler-autodiscover_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler-autodiscover_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler-autodiscover_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler-autodiscover_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscovery1_1.yaml (kubernetes)
=====================================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler-autodiscovery1_2.yaml (kubernetes)
=====================================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscovery1_5.yaml (kubernetes)
=====================================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_5.yaml:30-58
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_5.yaml:9-62
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_5.yaml:9-62
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler-autodiscovery1_5.yaml:30-58
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscovery_1.yaml (kubernetes)
====================================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler-autodiscovery_2.yaml (kubernetes)
====================================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler-autodiscovery_5.yaml (kubernetes)
====================================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_5.yaml:30-58
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_5.yaml:9-62
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_5.yaml:9-62
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler-autodiscovery_5.yaml:30-58
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler-binding.yaml (kubernetes)
============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding1.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding1.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding10.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding10.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding11.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding11.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding13.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding13.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding14.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding14.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding15.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding15.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding16.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding16.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding17.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding17.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding18.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding18.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding2.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding2.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding3.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding3.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding4.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding4.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding5.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding5.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding6.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding6.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding7.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding7.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding8.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding8.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-binding9.yaml (kubernetes)
=============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-autoscaler-view-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-autoscaler-binding9.yaml:6-8
────────────────────────────────────────
   6 ┌   name: cluster-autoscaler-view-binding
   7 │   labels:
   8 └     kubernetes.io/cluster-service: "true"
────────────────────────────────────────



cluster-autoscaler-rbac.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac1.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac1.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac1.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac10.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac10.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac10.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac11.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac11.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac11.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac13.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac13.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac13.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac14.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac14.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac14.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac15.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac15.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac15.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac16.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac16.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac16.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac17.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac17.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac17.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac18.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac18.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac18.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac2.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac2.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac2.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac3.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac3.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac3.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac4.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 112, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 2, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac4.yaml:111-116
────────────────────────────────────────
 111 ┌ - apiGroups:
 112 │   - ''
 113 │   resources:
 114 │   - configmaps
 115 │   verbs:
 116 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac4.yaml:117-127
────────────────────────────────────────
 117 ┌ - apiGroups:
 118 │   - ''
 119 │   resources:
 120 │   - configmaps
 121 │   resourceNames:
 122 │   - cluster-autoscaler-status
 123 │   verbs:
 124 │   - get
 125 └   - update
 ...   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-rbac4.yaml:8-13
────────────────────────────────────────
   8 ┌ - apiGroups:
   9 │   - ''
  10 │   resources:
  11 │   - endpoints
  12 │   verbs:
  13 └   - create
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-rbac4.yaml:14-24
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - endpoints
  18 │   resourceNames:
  19 │   - cluster-autoscaler
  20 │   verbs:
  21 │   - get
  22 └   - update
  ..   
────────────────────────────────────────



cluster-autoscaler-rbac5.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac5.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac5.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac6.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac6.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac6.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac7.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac7.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac7.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler-rbac8.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 112, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 2, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac8.yaml:128-133
────────────────────────────────────────
 128 ┌ - apiGroups:
 129 │   - ''
 130 │   resources:
 131 │   - configmaps
 132 │   verbs:
 133 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac8.yaml:134-144
────────────────────────────────────────
 134 ┌ - apiGroups:
 135 │   - ''
 136 │   resources:
 137 │   - configmaps
 138 │   resourceNames:
 139 │   - cluster-autoscaler-status
 140 │   verbs:
 141 │   - get
 142 └   - update
 ...   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-rbac8.yaml:25-30
────────────────────────────────────────
  25 ┌ - apiGroups:
  26 │   - ''
  27 │   resources:
  28 │   - endpoints
  29 │   verbs:
  30 └   - create
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler-rbac8.yaml:31-41
────────────────────────────────────────
  31 ┌ - apiGroups:
  32 │   - ''
  33 │   resources:
  34 │   - endpoints
  35 │   resourceNames:
  36 │   - cluster-autoscaler
  37 │   verbs:
  38 │   - get
  39 └   - update
  ..   
────────────────────────────────────────



cluster-autoscaler-rbac9.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac9.yaml:110-115
────────────────────────────────────────
 110 ┌ - apiGroups:
 111 │   - ''
 112 │   resources:
 113 │   - configmaps
 114 │   verbs:
 115 └   - create
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler-rbac9.yaml:116-126
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - configmaps
 120 │   resourceNames:
 121 │   - cluster-autoscaler-status
 122 │   verbs:
 123 │   - get
 124 └   - update
 ...   
────────────────────────────────────────



cluster-autoscaler10_1.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler10_1.yaml:94-103
────────────────────────────────────────
  94 ┌ - apiGroups:
  95 │   - batch
  96 │   - extensions
  97 │   resources:
  98 │   - jobs
  99 │   verbs:
 100 │   - get
 101 │   - list
 102 │   - watch
 103 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler10_1.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - events
  10 │   - endpoints
  11 │   verbs:
  12 │   - create
  13 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler10_1.yaml:26-34
────────────────────────────────────────
  26 ┌ - apiGroups:
  27 │   - ''
  28 │   resources:
  29 │   - endpoints
  30 │   resourceNames:
  31 │   - cluster-autoscaler
  32 │   verbs:
  33 │   - get
  34 └   - update
────────────────────────────────────────



cluster-autoscaler10_2.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler10_2.yaml:7-14
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - create
  13 │   - list
  14 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler10_2.yaml:15-26
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cluster-autoscaler-status
  21 │   - cluster-autoscaler-priority-expander
  22 │   verbs:
  23 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler10_5.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-autoscaler' of 'deployment' 'cluster-autoscaler' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:9-47
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:9-47
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-autoscaler" of deployment "cluster-autoscaler" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-autoscaler in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-autoscaler in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler10_5.yaml:18-47
────────────────────────────────────────
  18 ┌       serviceAccountName: cluster-autoscaler
  19 │       containers:
  20 │       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 └           requests:
  ..   
────────────────────────────────────────



cluster-autoscaler12_1.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler12_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler12_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler12_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler12_2.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler12_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler12_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler12_5.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler12_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler12_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler12_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler12_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler13_1.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler13_1.yaml:94-103
────────────────────────────────────────
  94 ┌ - apiGroups:
  95 │   - batch
  96 │   - extensions
  97 │   resources:
  98 │   - jobs
  99 │   verbs:
 100 │   - get
 101 │   - list
 102 │   - watch
 103 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler13_1.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - events
  10 │   - endpoints
  11 │   verbs:
  12 │   - create
  13 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler13_1.yaml:26-34
────────────────────────────────────────
  26 ┌ - apiGroups:
  27 │   - ''
  28 │   resources:
  29 │   - endpoints
  30 │   resourceNames:
  31 │   - cluster-autoscaler
  32 │   verbs:
  33 │   - get
  34 └   - update
────────────────────────────────────────



cluster-autoscaler13_2.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler13_2.yaml:7-14
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - create
  13 │   - list
  14 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler13_2.yaml:15-26
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cluster-autoscaler-status
  21 │   - cluster-autoscaler-priority-expander
  22 │   verbs:
  23 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler13_5.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-autoscaler' of 'deployment' 'cluster-autoscaler' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:9-47
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:9-47
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-autoscaler" of deployment "cluster-autoscaler" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-autoscaler in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-autoscaler in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler13_5.yaml:18-47
────────────────────────────────────────
  18 ┌       serviceAccountName: cluster-autoscaler
  19 │       containers:
  20 │       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 └           requests:
  ..   
────────────────────────────────────────



cluster-autoscaler2_1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler2_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler2_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler2_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler2_2.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler2_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler2_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler2_5.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler2_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler2_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler2_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler2_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler3_1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler3_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler3_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler3_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler3_2.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler3_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler3_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler3_5.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler3_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler3_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler3_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler3_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler5_1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler5_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler5_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler5_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler5_2.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler5_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler5_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler5_5.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler5_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler5_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler5_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler5_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler6_1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler6_1.yaml:94-103
────────────────────────────────────────
  94 ┌ - apiGroups:
  95 │   - batch
  96 │   - extensions
  97 │   resources:
  98 │   - jobs
  99 │   verbs:
 100 │   - get
 101 │   - list
 102 │   - watch
 103 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler6_1.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - events
  10 │   - endpoints
  11 │   verbs:
  12 │   - create
  13 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler6_1.yaml:26-34
────────────────────────────────────────
  26 ┌ - apiGroups:
  27 │   - ''
  28 │   resources:
  29 │   - endpoints
  30 │   resourceNames:
  31 │   - cluster-autoscaler
  32 │   verbs:
  33 │   - get
  34 └   - update
────────────────────────────────────────



cluster-autoscaler6_2.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler6_2.yaml:7-14
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - create
  13 │   - list
  14 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler6_2.yaml:15-26
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - configmaps
  19 │   resourceNames:
  20 │   - cluster-autoscaler-status
  21 │   - cluster-autoscaler-priority-expander
  22 │   verbs:
  23 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler6_5.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-autoscaler' of 'deployment' 'cluster-autoscaler' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:9-47
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:9-47
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-autoscaler" of deployment "cluster-autoscaler" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-autoscaler in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:20-43
────────────────────────────────────────
  20 ┌       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 │           requests:
  27 │             cpu: 100m
  28 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-autoscaler in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler6_5.yaml:18-47
────────────────────────────────────────
  18 ┌       serviceAccountName: cluster-autoscaler
  19 │       containers:
  20 │       - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
  21 │         name: cluster-autoscaler
  22 │         resources:
  23 │           limits:
  24 │             cpu: 100m
  25 │             memory: 600Mi
  26 └           requests:
  ..   
────────────────────────────────────────



cluster-autoscaler7.yaml (kubernetes)
=====================================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-autoscaler' of 'deployment' 'cluster-autoscaler' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler7.yaml:7-26
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: cluster-autoscaler
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: cluster-autoscaler
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-autoscaler" of deployment "cluster-autoscaler" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-autoscaler in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler7.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-autoscaler in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler7.yaml:16-26
────────────────────────────────────────
  16 ┌       containers:
  17 │       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 └         - --skip-nodes-with-local-storage=false
  ..   
────────────────────────────────────────



cluster-autoscaler8.yaml (kubernetes)
=====================================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-autoscaler' of 'deployment' 'cluster-autoscaler' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler8.yaml:7-26
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: cluster-autoscaler
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: cluster-autoscaler
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-autoscaler" of deployment "cluster-autoscaler" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-autoscaler in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler8.yaml:17-26
────────────────────────────────────────
  17 ┌       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 │         - --skip-nodes-with-local-storage=false
  25 │         - --expander=least-waste
  26 └         - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-autoscaler in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-autoscaler8.yaml:16-26
────────────────────────────────────────
  16 ┌       containers:
  17 │       - image: k8s.gcr.io/cluster-autoscaler:v1.22.0
  18 │         name: cluster-autoscaler
  19 │         command:
  20 │         - ./cluster-autoscaler
  21 │         - --v=4
  22 │         - --stderrthreshold=info
  23 │         - --cloud-provider=aws
  24 └         - --skip-nodes-with-local-storage=false
  ..   
────────────────────────────────────────



cluster-autoscaler9_1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler9_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler9_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler9_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler9_2.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler9_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler9_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler9_5.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler9_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler9_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler9_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler9_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.25.1
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-autoscaler_1.yaml (kubernetes)
======================================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-autoscaler' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-autoscaler_1.yaml:97-106
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - batch
  99 │   - extensions
 100 │   resources:
 101 │   - jobs
 102 │   verbs:
 103 │   - get
 104 │   - list
 105 │   - watch
 106 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler_1.yaml:9-16
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ''
  11 │   resources:
  12 │   - events
  13 │   - endpoints
  14 │   verbs:
  15 │   - create
  16 └   - patch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-autoscaler' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-autoscaler_1.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - endpoints
  33 │   resourceNames:
  34 │   - cluster-autoscaler
  35 │   verbs:
  36 │   - get
  37 └   - update
────────────────────────────────────────



cluster-autoscaler_2.yaml (kubernetes)
======================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler_2.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - create
  16 │   - list
  17 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-autoscaler' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-autoscaler_2.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   resourceNames:
  23 │   - cluster-autoscaler-status
  24 │   - cluster-autoscaler-priority-expander
  25 │   verbs:
  26 └   - delete
  ..   
────────────────────────────────────────



cluster-autoscaler_5.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 1, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0021 (LOW): Container 'cluster-autoscaler' of Deployment 'cluster-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-autoscaler_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'cluster-autoscaler' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 cluster-autoscaler_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'cluster-autoscaler' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 cluster-autoscaler_5.yaml:9-61
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: cluster-autoscaler
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: cluster-autoscaler
  17 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-autoscaler in deployment cluster-autoscaler (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-autoscaler_5.yaml:30-57
────────────────────────────────────────
  30 ┌       - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
  31 │         name: cluster-autoscaler
  32 │         resources:
  33 │           limits:
  34 │             cpu: 100m
  35 │             memory: 600Mi
  36 │           requests:
  37 │             cpu: 100m
  38 └             memory: 600Mi
  ..   
────────────────────────────────────────



cluster-binding.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-binding.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-binding1.yaml (kubernetes)
==================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-binding1.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-binding2.yaml (kubernetes)
==================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-binding2.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-connector_2.yaml (kubernetes)
=====================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'nginx-cluster-connector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-connector_2.yaml:14-21
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - secrets
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 └   - watch
────────────────────────────────────────



cluster-connector_4.yaml (kubernetes)
=====================================
Tests: 114 (SUCCESSES: 104, FAILURES: 10)
Failures: 10 (UNKNOWN: 0, LOW: 5, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx-cluster-connector' of Deployment 'nginx-cluster-connector' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx-cluster-connector' of Deployment 'nginx-cluster-connector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx-cluster-connector' of Deployment 'nginx-cluster-connector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx-cluster-connector' of Deployment 'nginx-cluster-connector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx-cluster-connector' of Deployment 'nginx-cluster-connector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx-cluster-connector' of Deployment 'nginx-cluster-connector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx-cluster-connector" of deployment "nginx-cluster-connector" in "nginx-cluster-connector" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-cluster-connector in nginx-cluster-connector namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-connector_4.yaml:16-43
────────────────────────────────────────
  16 ┌       serviceAccountName: nginx-cluster-connector
  17 │       automountServiceAccountToken: true
  18 │       containers:
  19 │       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 └             cpu: 100m
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nginx-cluster-connector in deployment nginx-cluster-connector (namespace: nginx-cluster-connector) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-connector_4.yaml:19-43
────────────────────────────────────────
  19 ┌       - image: docker-registry.nginx.com/cluster-connector/cluster-connector:0.1.0
  20 │         imagePullPolicy: IfNotPresent
  21 │         name: nginx-cluster-connector
  22 │         resources:
  23 │           requests:
  24 │             cpu: 100m
  25 │             memory: 128Mi
  26 │         securityContext:
  27 └           runAsUser: 101
  ..   
────────────────────────────────────────



cluster-deployment.yaml (kubernetes)
====================================
Tests: 115 (SUCCESSES: 96, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-nginx' of 'deployment' 'cluster-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cluster-nginx' of Deployment 'cluster-deployment' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-nginx' of Deployment 'cluster-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-nginx" of deployment "cluster-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment cluster-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-deployment.yaml:4-6
────────────────────────────────────────
   4 ┌   name: cluster-deployment
   5 │   labels:
   6 └     app: cluster-nginx
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment cluster-deployment in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-deployment.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-deployment.yaml:17-21
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: cluster-nginx
  19 │         image: nginx
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────



cluster-ip-svc.yaml (kubernetes)
================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-sample' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-sample' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-sample' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'nginx-sample' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-sample' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-sample' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx-sample' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx-sample' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx-sample' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-sample' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-sample' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-sample" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx-sample in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip-svc.yaml:4-5
────────────────────────────────────────
   4 ┌   name: nginx-sample
   5 └   namespace: default
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-sample in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-sample in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip-svc.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-sample in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip-svc.yaml:16-21
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: nginx
  18 │         image: nginx:1.25.4
  19 │         ports:
  20 │         - containerPort: 80
  21 └           name: http
────────────────────────────────────────



cluster-ip-svc_2.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'hello' of Deployment 'hello-sample' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'hello' of Deployment 'hello-sample' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'hello' of 'deployment' 'hello-sample' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'hello' of Deployment 'hello-sample' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'hello' of Deployment 'hello-sample' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'hello' of Deployment 'hello-sample' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'hello' of Deployment 'hello-sample' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'hello' of Deployment 'hello-sample' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'hello' of Deployment 'hello-sample' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'hello' of Deployment 'hello-sample' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'hello' of Deployment 'hello-sample' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "hello" of deployment "hello-sample" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment hello-sample in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip-svc_2.yaml:4-5
────────────────────────────────────────
   4 ┌   name: hello-sample
   5 └   namespace: default
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container hello-sample in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip-svc_2.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment hello-sample in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip-svc_2.yaml:16-21
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: hello
  18 │         image: gcr.io/google-samples/node-hello:1.0
  19 │         ports:
  20 │         - containerPort: 8080
  21 └           name: http
────────────────────────────────────────



cluster-ip.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-clusterip' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-clusterip" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx-clusterip in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip.yaml:4
────────────────────────────────────────
   4 [   name: nginx-clusterip
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-clusterip in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-clusterip in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-clusterip in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────



cluster-ip1.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-clusterip' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-clusterip' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-clusterip" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx-clusterip in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip1.yaml:4
────────────────────────────────────────
   4 [   name: nginx-clusterip
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-clusterip in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-clusterip in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-clusterip in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip1.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: nginx
  17 │         image: nginx:1.17
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────



cluster-ip3.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-despliegue' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-despliegue' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-despliegue' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-despliegue' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-despliegue' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-despliegue" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx-despliegue in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip3.yaml:4-6
────────────────────────────────────────
   4 ┌   name: nginx-despliegue
   5 │   labels:
   6 └     app: mi-app
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-despliegue in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-despliegue in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip3.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-despliegue in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip3.yaml:18-22
────────────────────────────────────────
  18 ┌       containers:
  19 │       - name: nginx
  20 │         image: nginx:1.7.9
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────



cluster-ip4.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 96, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'my-container' of Deployment 'my-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'my-container' of 'deployment' 'my-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "my-container" of deployment "my-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment my-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip4.yaml:4
────────────────────────────────────────
   4 [   name: my-deployment
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment my-deployment in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container my-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip4.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment my-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip4.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────



cluster-ip6.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 96, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'my-container' of Deployment 'my-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'my-container' of 'deployment' 'my-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "my-container" of deployment "my-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment my-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip6.yaml:4
────────────────────────────────────────
   4 [   name: my-deployment
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment my-deployment in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container my-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip6.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment my-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip6.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────



cluster-ip7.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 96, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'my-container' of Deployment 'my-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'my-container' of 'deployment' 'my-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'my-container' of Deployment 'my-deployment' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'my-container' of Deployment 'my-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "my-container" of deployment "my-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment my-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-ip7.yaml:4
────────────────────────────────────────
   4 [   name: my-deployment
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment my-deployment in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container my-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip7.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment my-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-ip7.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: my-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────



cluster-manager.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'cluster-role-test-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-manager.yaml:15-26
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - chaos-mesh.org
  17 │   resources:
  18 │   - '*'
  19 │   verbs:
  20 │   - create
  21 │   - delete
  22 │   - get
  23 └   - list
  ..   
────────────────────────────────────────



cluster-network-addons-operator-v0.95.0_1.yaml (kubernetes)
===========================================================
Tests: 116 (SUCCESSES: 108, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 2, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cluster-network-addons-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:183-191
────────────────────────────────────────
 183 ┌ - apiGroups:
 184 │   - ''
 185 │   resources:
 186 │   - secrets
 187 │   verbs:
 188 │   - list
 189 │   - watch
 190 │   - create
 191 └   - update
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'cluster-network-addons-operator' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:255-260
────────────────────────────────────────
 255 ┌ - apiGroups:
 256 │   - k8s.cni.cncf.io
 257 │   resources:
 258 │   - '*'
 259 │   verbs:
 260 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-network-addons-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:149-158
────────────────────────────────────────
 149 ┌ - apiGroups:
 150 │   - ''
 151 │   resources:
 152 │   - pods
 153 │   - pods/status
 154 │   verbs:
 155 │   - get
 156 │   - update
 157 │   - list
 158 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cluster-network-addons-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:231-238
────────────────────────────────────────
 231 ┌ - apiGroups:
 232 │   - apps
 233 │   resources:
 234 │   - deployments
 235 │   verbs:
 236 │   - get
 237 │   - create
 238 └   - update
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cluster-network-addons-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:176-182
────────────────────────────────────────
 176 ┌ - apiGroups:
 177 │   - ''
 178 │   resources:
 179 │   - configmaps
 180 │   verbs:
 181 │   - get
 182 └   - delete
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-network-addons-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:116-121
────────────────────────────────────────
 116 ┌ - apiGroups:
 117 │   - ''
 118 │   resources:
 119 │   - services
 120 │   verbs:
 121 └   - delete
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cluster-network-addons-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:200-209
────────────────────────────────────────
 200 ┌ - apiGroups:
 201 │   - ''
 202 │   resources:
 203 │   - services
 204 │   verbs:
 205 │   - get
 206 │   - create
 207 │   - update
 208 │   - list
 209 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cluster-network-addons-operator' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_1.yaml:100-108
────────────────────────────────────────
 100 ┌ - apiGroups:
 101 │   - admissionregistration.k8s.io
 102 │   resources:
 103 │   - mutatingwebhookconfigurations
 104 │   verbs:
 105 │   - get
 106 │   - create
 107 │   - update
 108 └   - delete
────────────────────────────────────────



cluster-network-addons-operator-v0.95.0_3.yaml (kubernetes)
===========================================================
Tests: 116 (SUCCESSES: 111, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 4, HIGH: 0, CRITICAL: 1)

AVD-KSV-0048 (MEDIUM): Role 'cluster-network-addons-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_3.yaml:9-17
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - apps
  11 │   resources:
  12 │   - daemonsets
  13 │   verbs:
  14 │   - get
  15 │   - create
  16 │   - update
  17 └   - delete
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): Role 'cluster-network-addons-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_3.yaml:26-31
────────────────────────────────────────
  26 ┌ - apiGroups:
  27 │   - apps
  28 │   resources:
  29 │   - deployments
  30 │   verbs:
  31 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-network-addons-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_3.yaml:18-25
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - configmaps
  22 │   verbs:
  23 │   - get
  24 │   - create
  25 └   - update
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cluster-network-addons-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_3.yaml:76-81
────────────────────────────────────────
  76 ┌ - apiGroups:
  77 │   - ''
  78 │   resources:
  79 │   - configmaps
  80 │   verbs:
  81 └   - patch
────────────────────────────────────────


AVD-KSV-0050 (CRITICAL): Role 'cluster-network-addons-operator' should not have access to resources ["roles", "rolebindings"] for verbs ["create", "update", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
An effective level of access equivalent to cluster-admin should not be provided.

See https://avd.aquasec.com/misconfig/ksv050
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_3.yaml:59-68
────────────────────────────────────────
  59 ┌ - apiGroups:
  60 │   - rbac.authorization.k8s.io
  61 │   resources:
  62 │   - roles
  63 │   - rolebindings
  64 │   verbs:
  65 │   - get
  66 │   - create
  67 │   - update
  68 └   - delete
────────────────────────────────────────



cluster-network-addons-operator-v0.95.0_5.yaml (kubernetes)
===========================================================
Tests: 120 (SUCCESSES: 108, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 8, MEDIUM: 2, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cluster-network-addons-operator' of Deployment 'cluster-network-addons-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:27-88
────────────────────────────────────────
  27 ┌       - env:
  28 │         - name: MULTUS_IMAGE
  29 │           value: ghcr.io/k8snetworkplumbingwg/multus-cni@sha256:3fbcc32bd4e4d15bd93c96def784a229cd84cca27942bf4858b581f31c97ee02
  30 │         - name: MULTUS_DYNAMIC_NETWORKS_CONTROLLER_IMAGE
  31 │           value: ghcr.io/k8snetworkplumbingwg/multus-dynamic-networks-controller@sha256:83b460502671fb4f34116363a1a39b2ddfc9d14a920ee0a6413bfc3bd0580404
  32 │         - name: LINUX_BRIDGE_IMAGE
  33 │           value: quay.io/kubevirt/cni-default-plugins@sha256:0c354fa9d695b8cab97b459e8afea2f7662407a987e83f6f6f1a8af4b45726be
  34 │         - name: LINUX_BRIDGE_MARKER_IMAGE
  35 └           value: quay.io/kubevirt/bridge-marker@sha256:18d954d58b9830738df9bf5c9a575d22b33096d1af26fb6bc2da09fb31c9f73a
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kube-rbac-proxy' of Deployment 'cluster-network-addons-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:89-109
────────────────────────────────────────
  89 ┌       - args:
  90 │         - --logtostderr
  91 │         - --secure-listen-address=:8443
  92 │         - --upstream=http://127.0.0.1:8080
  93 │         image: quay.io/openshift/origin-kube-rbac-proxy@sha256:e2def4213ec0657e72eb790ae8a115511d5b8f164a62d3568d2f1bff189917e8
  94 │         imagePullPolicy: Always
  95 │         name: kube-rbac-proxy
  96 │         ports:
  97 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-network-addons-operator' of Deployment 'cluster-network-addons-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:27-88
────────────────────────────────────────
  27 ┌       - env:
  28 │         - name: MULTUS_IMAGE
  29 │           value: ghcr.io/k8snetworkplumbingwg/multus-cni@sha256:3fbcc32bd4e4d15bd93c96def784a229cd84cca27942bf4858b581f31c97ee02
  30 │         - name: MULTUS_DYNAMIC_NETWORKS_CONTROLLER_IMAGE
  31 │           value: ghcr.io/k8snetworkplumbingwg/multus-dynamic-networks-controller@sha256:83b460502671fb4f34116363a1a39b2ddfc9d14a920ee0a6413bfc3bd0580404
  32 │         - name: LINUX_BRIDGE_IMAGE
  33 │           value: quay.io/kubevirt/cni-default-plugins@sha256:0c354fa9d695b8cab97b459e8afea2f7662407a987e83f6f6f1a8af4b45726be
  34 │         - name: LINUX_BRIDGE_MARKER_IMAGE
  35 └           value: quay.io/kubevirt/bridge-marker@sha256:18d954d58b9830738df9bf5c9a575d22b33096d1af26fb6bc2da09fb31c9f73a
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'cluster-network-addons-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:89-109
────────────────────────────────────────
  89 ┌       - args:
  90 │         - --logtostderr
  91 │         - --secure-listen-address=:8443
  92 │         - --upstream=http://127.0.0.1:8080
  93 │         image: quay.io/openshift/origin-kube-rbac-proxy@sha256:e2def4213ec0657e72eb790ae8a115511d5b8f164a62d3568d2f1bff189917e8
  94 │         imagePullPolicy: Always
  95 │         name: kube-rbac-proxy
  96 │         ports:
  97 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cluster-network-addons-operator' of Deployment 'cluster-network-addons-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:27-88
────────────────────────────────────────
  27 ┌       - env:
  28 │         - name: MULTUS_IMAGE
  29 │           value: ghcr.io/k8snetworkplumbingwg/multus-cni@sha256:3fbcc32bd4e4d15bd93c96def784a229cd84cca27942bf4858b581f31c97ee02
  30 │         - name: MULTUS_DYNAMIC_NETWORKS_CONTROLLER_IMAGE
  31 │           value: ghcr.io/k8snetworkplumbingwg/multus-dynamic-networks-controller@sha256:83b460502671fb4f34116363a1a39b2ddfc9d14a920ee0a6413bfc3bd0580404
  32 │         - name: LINUX_BRIDGE_IMAGE
  33 │           value: quay.io/kubevirt/cni-default-plugins@sha256:0c354fa9d695b8cab97b459e8afea2f7662407a987e83f6f6f1a8af4b45726be
  34 │         - name: LINUX_BRIDGE_MARKER_IMAGE
  35 └           value: quay.io/kubevirt/bridge-marker@sha256:18d954d58b9830738df9bf5c9a575d22b33096d1af26fb6bc2da09fb31c9f73a
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kube-rbac-proxy' of Deployment 'cluster-network-addons-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:89-109
────────────────────────────────────────
  89 ┌       - args:
  90 │         - --logtostderr
  91 │         - --secure-listen-address=:8443
  92 │         - --upstream=http://127.0.0.1:8080
  93 │         image: quay.io/openshift/origin-kube-rbac-proxy@sha256:e2def4213ec0657e72eb790ae8a115511d5b8f164a62d3568d2f1bff189917e8
  94 │         imagePullPolicy: Always
  95 │         name: kube-rbac-proxy
  96 │         ports:
  97 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-network-addons-operator' of Deployment 'cluster-network-addons-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:27-88
────────────────────────────────────────
  27 ┌       - env:
  28 │         - name: MULTUS_IMAGE
  29 │           value: ghcr.io/k8snetworkplumbingwg/multus-cni@sha256:3fbcc32bd4e4d15bd93c96def784a229cd84cca27942bf4858b581f31c97ee02
  30 │         - name: MULTUS_DYNAMIC_NETWORKS_CONTROLLER_IMAGE
  31 │           value: ghcr.io/k8snetworkplumbingwg/multus-dynamic-networks-controller@sha256:83b460502671fb4f34116363a1a39b2ddfc9d14a920ee0a6413bfc3bd0580404
  32 │         - name: LINUX_BRIDGE_IMAGE
  33 │           value: quay.io/kubevirt/cni-default-plugins@sha256:0c354fa9d695b8cab97b459e8afea2f7662407a987e83f6f6f1a8af4b45726be
  34 │         - name: LINUX_BRIDGE_MARKER_IMAGE
  35 └           value: quay.io/kubevirt/bridge-marker@sha256:18d954d58b9830738df9bf5c9a575d22b33096d1af26fb6bc2da09fb31c9f73a
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'cluster-network-addons-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:89-109
────────────────────────────────────────
  89 ┌       - args:
  90 │         - --logtostderr
  91 │         - --secure-listen-address=:8443
  92 │         - --upstream=http://127.0.0.1:8080
  93 │         image: quay.io/openshift/origin-kube-rbac-proxy@sha256:e2def4213ec0657e72eb790ae8a115511d5b8f164a62d3568d2f1bff189917e8
  94 │         imagePullPolicy: Always
  95 │         name: kube-rbac-proxy
  96 │         ports:
  97 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-network-addons-operator' of Deployment 'cluster-network-addons-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:27-88
────────────────────────────────────────
  27 ┌       - env:
  28 │         - name: MULTUS_IMAGE
  29 │           value: ghcr.io/k8snetworkplumbingwg/multus-cni@sha256:3fbcc32bd4e4d15bd93c96def784a229cd84cca27942bf4858b581f31c97ee02
  30 │         - name: MULTUS_DYNAMIC_NETWORKS_CONTROLLER_IMAGE
  31 │           value: ghcr.io/k8snetworkplumbingwg/multus-dynamic-networks-controller@sha256:83b460502671fb4f34116363a1a39b2ddfc9d14a920ee0a6413bfc3bd0580404
  32 │         - name: LINUX_BRIDGE_IMAGE
  33 │           value: quay.io/kubevirt/cni-default-plugins@sha256:0c354fa9d695b8cab97b459e8afea2f7662407a987e83f6f6f1a8af4b45726be
  34 │         - name: LINUX_BRIDGE_MARKER_IMAGE
  35 └           value: quay.io/kubevirt/bridge-marker@sha256:18d954d58b9830738df9bf5c9a575d22b33096d1af26fb6bc2da09fb31c9f73a
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'cluster-network-addons-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:89-109
────────────────────────────────────────
  89 ┌       - args:
  90 │         - --logtostderr
  91 │         - --secure-listen-address=:8443
  92 │         - --upstream=http://127.0.0.1:8080
  93 │         image: quay.io/openshift/origin-kube-rbac-proxy@sha256:e2def4213ec0657e72eb790ae8a115511d5b8f164a62d3568d2f1bff189917e8
  94 │         imagePullPolicy: Always
  95 │         name: kube-rbac-proxy
  96 │         ports:
  97 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-network-addons-operator in deployment cluster-network-addons-operator (namespace: cluster-network-addons) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:27-88
────────────────────────────────────────
  27 ┌       - env:
  28 │         - name: MULTUS_IMAGE
  29 │           value: ghcr.io/k8snetworkplumbingwg/multus-cni@sha256:3fbcc32bd4e4d15bd93c96def784a229cd84cca27942bf4858b581f31c97ee02
  30 │         - name: MULTUS_DYNAMIC_NETWORKS_CONTROLLER_IMAGE
  31 │           value: ghcr.io/k8snetworkplumbingwg/multus-dynamic-networks-controller@sha256:83b460502671fb4f34116363a1a39b2ddfc9d14a920ee0a6413bfc3bd0580404
  32 │         - name: LINUX_BRIDGE_IMAGE
  33 │           value: quay.io/kubevirt/cni-default-plugins@sha256:0c354fa9d695b8cab97b459e8afea2f7662407a987e83f6f6f1a8af4b45726be
  34 │         - name: LINUX_BRIDGE_MARKER_IMAGE
  35 └           value: quay.io/kubevirt/bridge-marker@sha256:18d954d58b9830738df9bf5c9a575d22b33096d1af26fb6bc2da09fb31c9f73a
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kube-rbac-proxy in deployment cluster-network-addons-operator (namespace: cluster-network-addons) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cluster-network-addons-operator-v0.95.0_5.yaml:89-109
────────────────────────────────────────
  89 ┌       - args:
  90 │         - --logtostderr
  91 │         - --secure-listen-address=:8443
  92 │         - --upstream=http://127.0.0.1:8080
  93 │         image: quay.io/openshift/origin-kube-rbac-proxy@sha256:e2def4213ec0657e72eb790ae8a115511d5b8f164a62d3568d2f1bff189917e8
  94 │         imagePullPolicy: Always
  95 │         name: kube-rbac-proxy
  96 │         ports:
  97 └         - containerPort: 8443
  ..   
────────────────────────────────────────



cluster-nginx-deployment.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 96, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'nginx' of Deployment 'nginx-deployment' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cluster-nginx-deployment.yaml:4
────────────────────────────────────────
   4 [   name: nginx-deployment
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-deployment in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-nginx-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cluster-nginx-deployment.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │         - name: nginx
  17 │           image: nginx:latest
  18 │           ports:
  19 └             - containerPort: 80
────────────────────────────────────────



cluster-role-admin.yaml (kubernetes)
====================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'nsm-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role-admin.yaml:19-21
────────────────────────────────────────
  19 ┌   - apiGroups: [""]
  20 │     resources: ["configmaps"]
  21 └     verbs: ["get", "update"]
────────────────────────────────────────



cluster-role-admin1.yaml (kubernetes)
=====================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'nsm-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role-admin1.yaml:19-21
────────────────────────────────────────
  19 ┌   - apiGroups: [""]
  20 │     resources: ["configmaps"]
  21 └     verbs: ["get", "update"]
────────────────────────────────────────



cluster-role-admin2_1.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'kube-dump' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-admin2_1.yaml:4
────────────────────────────────────────
   4 [   name: kube-dump
────────────────────────────────────────



cluster-role-admin3_1.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'kube-dump' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-admin3_1.yaml:4
────────────────────────────────────────
   4 [   name: kube-dump
────────────────────────────────────────



cluster-role-admin4_1.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'kube-dump' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-admin4_1.yaml:4
────────────────────────────────────────
   4 [   name: kube-dump
────────────────────────────────────────



cluster-role-binding101.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'heimdal-sa-cluster-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding101.yaml:4
────────────────────────────────────────
   4 [   name: heimdal-sa-cluster-admin
────────────────────────────────────────



cluster-role-binding104.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'argocd-clusterrolebinding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding104.yaml:4
────────────────────────────────────────
   4 [   name: argocd-clusterrolebinding
────────────────────────────────────────



cluster-role-binding134.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding134.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-role-binding150.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding150.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-role-binding152.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'dashboard-admin-crb' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding152.yaml:4
────────────────────────────────────────
   4 [   name: dashboard-admin-crb
────────────────────────────────────────



cluster-role-binding184.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'default-cluster-role-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding184.yaml:4
────────────────────────────────────────
   4 [   name: default-cluster-role-binding
────────────────────────────────────────



cluster-role-binding2.yaml (kubernetes)
=======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding2.yaml:5
────────────────────────────────────────
   5 [   name: cluster-admin
────────────────────────────────────────



cluster-role-binding203.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'dashboard-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding203.yaml:4
────────────────────────────────────────
   4 [   name: dashboard-admin
────────────────────────────────────────



cluster-role-binding214.yaml (kubernetes)
=========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding214.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-role-binding28.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cluster-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding28.yaml:5
────────────────────────────────────────
   5 [   name: cluster-admin
────────────────────────────────────────



cluster-role-binding64.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'admin-user' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role-binding64.yaml:4
────────────────────────────────────────
   4 [   name: admin-user
────────────────────────────────────────



cluster-role-delete-pods.yaml (kubernetes)
==========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:cleanup-pods' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role-delete-pods.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - pods
  14 │   verbs:
  15 │   - get
  16 │   - watch
  17 │   - list
  18 └   - delete
────────────────────────────────────────



cluster-role-metrics-server-resources.yaml (kubernetes)
=======================================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'resource-metrics-server-resources' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role-metrics-server-resources.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - metrics.k8s.io
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role-metrics-server-resources1.yaml (kubernetes)
========================================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'resource-metrics-server-resources' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role-metrics-server-resources1.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - metrics.k8s.io
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role-n-binding.yaml (kubernetes)
========================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'reader' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role-n-binding.yaml:6-17
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - deployments
  10 │   - configmaps
  11 │   - pods
  12 │   - secrets
  13 │   - services
  14 └   verbs:
  ..   
────────────────────────────────────────



cluster-role-readers.yaml (kubernetes)
======================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'readers-unlimited' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role-readers.yaml:10-12
────────────────────────────────────────
  10 ┌   - apiGroups: ["*"]
  11 │     resources: ["*"]
  12 └     verbs: ["get","list","watch"] 
────────────────────────────────────────



cluster-role.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role.yaml.verified.yaml (kubernetes)
============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'my-cluster-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role.yaml.verified.yaml:7-14
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   resources:
  10 │   - secrets
  11 │   verbs:
  12 │   - get
  13 │   - watch
  14 └   - list
────────────────────────────────────────



cluster-role1.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'application-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role1.yaml:20-22
────────────────────────────────────────
  20 ┌   - apiGroups: ["apps"]
  21 │     resources: ["deployments"]
  22 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'application-controller' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role1.yaml:24-26
────────────────────────────────────────
  24 ┌   - apiGroups: [""]
  25 │     resources: ["services"]
  26 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────



cluster-role101.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role101.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role101.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - apps
  26 │   resources:
  27 │   - statefulsets
  28 │   verbs:
  29 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role101.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role101.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - apps
  26 │   resources:
  27 │   - statefulsets
  28 │   verbs:
  29 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role101.yaml:37-43
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ""
  39 │   resources:
  40 │   - pods
  41 │   verbs:
  42 │   - list
  43 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role101.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role101.yaml:44-54
────────────────────────────────────────
  44 ┌ - apiGroups:
  45 │   - ""
  46 │   resources:
  47 │   - services
  48 │   - services/finalizers
  49 │   - endpoints
  50 │   verbs:
  51 │   - get
  52 └   - create
  ..   
────────────────────────────────────────



cluster-role103.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role103.yaml:10-28
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role104.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role104.yaml:6-12
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - apps
   8 │   resources:
   9 │   - statefulsets
  10 │   - deployments
  11 │   verbs:
  12 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'crossplane-oam' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role104.yaml:6-12
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - apps
   8 │   resources:
   9 │   - statefulsets
  10 │   - deployments
  11 │   verbs:
  12 └   - '*'
────────────────────────────────────────



cluster-role107.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role107.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role108.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cluster-role-secretadmin' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role108.yaml:6-8
────────────────────────────────────────
   6 ┌ - apiGroups: [""]
   7 │   resources: ["secrets"]
   8 └   verbs: ["watch", "list"]
────────────────────────────────────────



cluster-role109.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'my-cluster-role-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role109.yaml:4
────────────────────────────────────────
   4 [   name: my-cluster-role-binding
────────────────────────────────────────



cluster-role11.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role11.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role11.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role11.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role11.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role11.yaml:41-47
────────────────────────────────────────
  41 ┌ - apiGroups:
  42 │   - ""
  43 │   resources:
  44 │   - pods
  45 │   verbs:
  46 │   - list
  47 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role11.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role11.yaml:48-58
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ""
  50 │   resources:
  51 │   - services
  52 │   - services/finalizers
  53 │   - endpoints
  54 │   verbs:
  55 │   - get
  56 └   - create
  ..   
────────────────────────────────────────



cluster-role110.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'my-cluster-role-binding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role110.yaml:4-7
────────────────────────────────────────
   4 ┌   namespace: cert-manager
   5 │   name: my-cluster-role-binding
   6 │   annotations:
   7 └     argocd.argoproj.io/hook: PreSync
────────────────────────────────────────



cluster-role111.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role111.yaml:10-28
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role112.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 2, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'ekspose-cr' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role112.yaml:7-13
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - services
  11 │   verbs:
  12 │   - create
  13 └   - delete
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'ekspose-cr' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role112.yaml:22-28
────────────────────────────────────────
  22 ┌ - apiGroups:
  23 │   - networking.k8s.io
  24 │   resources:
  25 │   - ingresses
  26 │   verbs:
  27 │   - create
  28 └   - delete
────────────────────────────────────────



cluster-role113.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 cluster-role113.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:additional' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role113.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role114.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:restart-deploy-on-sc-change' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role114.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role115.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:update-image-tag' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role115.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role117.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): Role 'kondense' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role117.yaml:6-8
────────────────────────────────────────
   6 ┌   - apiGroups: [""]
   7 │     resources: ["pods"]
   8 └     verbs: ["get", "list", "watch", "patch"]
────────────────────────────────────────


AVD-KSV-0053 (HIGH): Role 'kondense' should not have access to resource '["pods/exec"]' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to exec into a container with privileged access to the host or with an attached SA with higher RBAC permissions is a common escalation path to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv053
────────────────────────────────────────
 cluster-role117.yaml:9-11
────────────────────────────────────────
   9 ┌   - apiGroups: [""]
  10 │     resources: ["pods/exec"]
  11 └     verbs: ["create"]
────────────────────────────────────────



cluster-role119.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role119.yaml:10-28
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role120.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 cluster-role120.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:additional' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role120.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role121.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:restart-deploy-on-sc-change' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role121.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role122.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:update-image-tag' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role122.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role123.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role123.yaml:10-27
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role124.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 cluster-role124.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:additional' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role124.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role125.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:restart-deploy-on-sc-change' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role125.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role126.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:update-image-tag' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role126.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role127.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 cluster-role127.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:additional' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role127.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role128.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:restart-deploy-on-sc-change' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role128.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role129.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:update-image-tag' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role129.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role13.yaml (kubernetes)
================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role13.yaml:6-23
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ""
   8 │   resources:
   9 │   - configmaps
  10 │   - secrets
  11 │   - nodes
  12 │   - pods
  13 │   - services
  14 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role130.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 cluster-role130.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:additional' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role130.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



cluster-role131.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:restart-deploy-on-sc-change' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role131.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role132.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:update-image-tag' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role132.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



cluster-role134.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'secretproviderclasses-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role134.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ""
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - create
  56 │   - delete
  57 │   - get
  58 └   - list
  ..   
────────────────────────────────────────



cluster-role135.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0114 (CRITICAL): ClusterRole 'k8-env' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role135.yaml:8-10
────────────────────────────────────────
   8 ┌ - apiGroups: ["admissionregistration.k8s.io"]
   9 │   resources: ["mutatingwebhookconfigurations"]
  10 └   verbs: ["create", "get", "delete", "list", "patch", "update", "watch"]
────────────────────────────────────────



cluster-role138.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'skooner-sa' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cluster-role138.yaml:5
────────────────────────────────────────
   5 [   name: skooner-sa
────────────────────────────────────────



cluster-role142.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role142.yaml:10-28
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role143.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 108, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 0, MEDIUM: 4, HIGH: 1, CRITICAL: 4)

AVD-KSV-0041 (CRITICAL): ClusterRole 'minio-operator-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role143.yaml:47-58
────────────────────────────────────────
  47 ┌   - apiGroups:
  48 │       - ""
  49 │     resources:
  50 │       - secrets
  51 │     verbs:
  52 │       - get
  53 │       - watch
  54 │       - create
  55 └       - update
  ..   
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'minio-operator-role' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role143.yaml:140-145
────────────────────────────────────────
 140 ┌   - apiGroups:
 141 │       - minio.min.io
 142 │     resources:
 143 │       - "*"
 144 │     verbs:
 145 └       - "*"
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'minio-operator-role' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role143.yaml:146-152
────────────────────────────────────────
 146 ┌   - apiGroups:
 147 │       - min.io
 148 │       - sts.min.io
 149 │     resources:
 150 │       - "*"
 151 │     verbs:
 152 └       - "*"
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'minio-operator-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role143.yaml:31-46
────────────────────────────────────────
  31 ┌   - apiGroups:
  32 │       - ""
  33 │     resources:
  34 │       - pods
  35 │       - services
  36 │       - events
  37 │       - configmaps
  38 │     verbs:
  39 └       - get
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'minio-operator-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role143.yaml:84-97
────────────────────────────────────────
  84 ┌   - apiGroups:
  85 │       - apps
  86 │     resources:
  87 │       - statefulsets
  88 │       - deployments
  89 │       - deployments/finalizers
  90 │     verbs:
  91 │       - get
  92 └       - create
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'minio-operator-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role143.yaml:98-109
────────────────────────────────────────
  98 ┌   - apiGroups:
  99 │       - batch
 100 │     resources:
 101 │       - jobs
 102 │     verbs:
 103 │       - get
 104 │       - create
 105 │       - list
 106 └       - patch
 ...   
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'minio-operator-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role143.yaml:31-46
────────────────────────────────────────
  31 ┌   - apiGroups:
  32 │       - ""
  33 │     resources:
  34 │       - pods
  35 │       - services
  36 │       - events
  37 │       - configmaps
  38 │     verbs:
  39 └       - get
  ..   
────────────────────────────────────────


AVD-KSV-0050 (CRITICAL): ClusterRole 'minio-operator-role' should not have access to resources ["roles", "rolebindings"] for verbs ["create", "update", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
An effective level of access equivalent to cluster-admin should not be provided.

See https://avd.aquasec.com/misconfig/ksv050
────────────────────────────────────────
 cluster-role143.yaml:71-83
────────────────────────────────────────
  71 ┌   - apiGroups:
  72 │       - rbac.authorization.k8s.io
  73 │     resources:
  74 │       - roles
  75 │       - rolebindings
  76 │     verbs:
  77 │       - create
  78 │       - delete
  79 └       - get
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'minio-operator-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role143.yaml:31-46
────────────────────────────────────────
  31 ┌   - apiGroups:
  32 │       - ""
  33 │     resources:
  34 │       - pods
  35 │       - services
  36 │       - events
  37 │       - configmaps
  38 │     verbs:
  39 └       - get
  ..   
────────────────────────────────────────



cluster-role144.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role144.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role146.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role146.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role146.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role146.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role146.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role146.yaml:41-47
────────────────────────────────────────
  41 ┌ - apiGroups:
  42 │   - ""
  43 │   resources:
  44 │   - pods
  45 │   verbs:
  46 │   - list
  47 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role146.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role146.yaml:48-58
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ""
  50 │   resources:
  51 │   - services
  52 │   - services/finalizers
  53 │   - endpoints
  54 │   verbs:
  55 │   - get
  56 └   - create
  ..   
────────────────────────────────────────



cluster-role148.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role148.yaml:6-23
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ""
   8 │   resources:
   9 │   - configmaps
  10 │   - secrets
  11 │   - nodes
  12 │   - pods
  13 │   - services
  14 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role151.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role151.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role151.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role151.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role151.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role151.yaml:41-47
────────────────────────────────────────
  41 ┌ - apiGroups:
  42 │   - ""
  43 │   resources:
  44 │   - pods
  45 │   verbs:
  46 │   - list
  47 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role151.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role151.yaml:48-58
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ""
  50 │   resources:
  51 │   - services
  52 │   - services/finalizers
  53 │   - endpoints
  54 │   verbs:
  55 │   - get
  56 └   - create
  ..   
────────────────────────────────────────



cluster-role152.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role152.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role155.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role155.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role155.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - apps
  26 │   resources:
  27 │   - statefulsets
  28 │   verbs:
  29 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role155.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role155.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - apps
  26 │   resources:
  27 │   - statefulsets
  28 │   verbs:
  29 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role155.yaml:37-43
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ""
  39 │   resources:
  40 │   - pods
  41 │   verbs:
  42 │   - list
  43 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role155.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role155.yaml:44-54
────────────────────────────────────────
  44 ┌ - apiGroups:
  45 │   - ""
  46 │   resources:
  47 │   - services
  48 │   - services/finalizers
  49 │   - endpoints
  50 │   verbs:
  51 │   - get
  52 └   - create
  ..   
────────────────────────────────────────



cluster-role158.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role158.yaml:10-27
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role159.yaml (kubernetes)
=================================
Tests: 118 (SUCCESSES: 107, FAILURES: 11)
Failures: 11 (UNKNOWN: 0, LOW: 0, MEDIUM: 4, HIGH: 2, CRITICAL: 5)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cnpg-manager' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role159.yaml:80-91
────────────────────────────────────────
  80 ┌   - apiGroups:
  81 │       - ""
  82 │     resources:
  83 │       - secrets
  84 │     verbs:
  85 │       - create
  86 │       - delete
  87 │       - get
  88 └       - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cnpg-manager' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role159.yaml:52-62
────────────────────────────────────────
  52 ┌   - apiGroups:
  53 │       - ""
  54 │     resources:
  55 │       - pods
  56 │     verbs:
  57 │       - create
  58 │       - delete
  59 │       - get
  60 └       - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cnpg-manager' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role159.yaml:137-148
────────────────────────────────────────
 137 ┌   - apiGroups:
 138 │       - apps
 139 │     resources:
 140 │       - deployments
 141 │     verbs:
 142 │       - create
 143 │       - delete
 144 │       - get
 145 └       - list
 ...   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cnpg-manager' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role159.yaml:149-159
────────────────────────────────────────
 149 ┌   - apiGroups:
 150 │       - batch
 151 │     resources:
 152 │       - jobs
 153 │     verbs:
 154 │       - create
 155 │       - delete
 156 │       - get
 157 └       - list
 ...   
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'cnpg-manager' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role159.yaml:6-17
────────────────────────────────────────
   6 ┌   - apiGroups:
   7 │       - ""
   8 │     resources:
   9 │       - configmaps
  10 │     verbs:
  11 │       - create
  12 │       - delete
  13 │       - get
  14 └       - list
  ..   
────────────────────────────────────────


AVD-KSV-0050 (CRITICAL): ClusterRole 'cnpg-manager' should not have access to resources ["roles", "rolebindings"] for verbs ["create", "update", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
An effective level of access equivalent to cluster-admin should not be provided.

See https://avd.aquasec.com/misconfig/ksv050
────────────────────────────────────────
 cluster-role159.yaml:301-311
────────────────────────────────────────
 301 ┌   - apiGroups:
 302 │       - rbac.authorization.k8s.io
 303 │     resources:
 304 │       - rolebindings
 305 │     verbs:
 306 │       - create
 307 │       - get
 308 │       - list
 309 └       - patch
 ...   
────────────────────────────────────────


AVD-KSV-0050 (CRITICAL): ClusterRole 'cnpg-manager' should not have access to resources ["roles", "rolebindings"] for verbs ["create", "update", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
An effective level of access equivalent to cluster-admin should not be provided.

See https://avd.aquasec.com/misconfig/ksv050
────────────────────────────────────────
 cluster-role159.yaml:312-322
────────────────────────────────────────
 312 ┌   - apiGroups:
 313 │       - rbac.authorization.k8s.io
 314 │     resources:
 315 │       - roles
 316 │     verbs:
 317 │       - create
 318 │       - get
 319 │       - list
 320 └       - patch
 ...   
────────────────────────────────────────


AVD-KSV-0053 (HIGH): ClusterRole 'cnpg-manager' should not have access to resource '["pods/exec"]' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to exec into a container with privileged access to the host or with an attached SA with higher RBAC permissions is a common escalation path to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv053
────────────────────────────────────────
 cluster-role159.yaml:63-73
────────────────────────────────────────
  63 ┌   - apiGroups:
  64 │       - ""
  65 │     resources:
  66 │       - pods/exec
  67 │     verbs:
  68 │       - create
  69 │       - delete
  70 │       - get
  71 └       - list
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cnpg-manager' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role159.yaml:111-122
────────────────────────────────────────
 111 ┌   - apiGroups:
 112 │       - ""
 113 │     resources:
 114 │       - services
 115 │     verbs:
 116 │       - create
 117 │       - delete
 118 │       - get
 119 └       - list
 ...   
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cnpg-manager' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role159.yaml:123-129
────────────────────────────────────────
 123 ┌   - apiGroups:
 124 │       - admissionregistration.k8s.io
 125 │     resources:
 126 │       - mutatingwebhookconfigurations
 127 │     verbs:
 128 │       - get
 129 └       - patch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cnpg-manager' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role159.yaml:130-136
────────────────────────────────────────
 130 ┌   - apiGroups:
 131 │       - admissionregistration.k8s.io
 132 │     resources:
 133 │       - validatingwebhookconfigurations
 134 │     verbs:
 135 │       - get
 136 └       - patch
────────────────────────────────────────



cluster-role16.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role16.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role16.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role16.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role16.yaml:28-33
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - apps
  30 │   resources:
  31 │   - statefulsets
  32 │   verbs:
  33 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role16.yaml:41-47
────────────────────────────────────────
  41 ┌ - apiGroups:
  42 │   - ""
  43 │   resources:
  44 │   - pods
  45 │   verbs:
  46 │   - list
  47 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role16.yaml:34-40
────────────────────────────────────────
  34 ┌ - apiGroups:
  35 │   - ""
  36 │   resources:
  37 │   - configmaps
  38 │   - secrets
  39 │   verbs:
  40 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role16.yaml:48-58
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ""
  50 │   resources:
  51 │   - services
  52 │   - services/finalizers
  53 │   - endpoints
  54 │   verbs:
  55 │   - get
  56 └   - create
  ..   
────────────────────────────────────────



cluster-role161.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'role-full-access-to-secrets' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role161.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   resourceNames:
  11 │   - regcred
  12 │   verbs:
  13 └   - delete
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'role-full-access-to-secrets' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role161.yaml:14-19
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - secrets
  18 │   verbs:
  19 └   - create
────────────────────────────────────────



cluster-role164.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'platform-devportal-read-only' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role164.yaml:7-31
────────────────────────────────────────
   7 ┌   - apiGroups:
   8 │       - '*'
   9 │     resources:
  10 │       - configmaps
  11 │       - cronjobs
  12 │       - daemonsets
  13 │       - deployments
  14 │       - horizontalpodautoscalers
  15 └       - ingressclasses
  ..   
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'platform-devportal-read-only' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role164.yaml:7-31
────────────────────────────────────────
   7 ┌   - apiGroups:
   8 │       - '*'
   9 │     resources:
  10 │       - configmaps
  11 │       - cronjobs
  12 │       - daemonsets
  13 │       - deployments
  14 │       - horizontalpodautoscalers
  15 └       - ingressclasses
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'platform-devportal-read-only' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role164.yaml:7-31
────────────────────────────────────────
   7 ┌   - apiGroups:
   8 │       - '*'
   9 │     resources:
  10 │       - configmaps
  11 │       - cronjobs
  12 │       - daemonsets
  13 │       - deployments
  14 │       - horizontalpodautoscalers
  15 └       - ingressclasses
  ..   
────────────────────────────────────────



cluster-role166.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role166.yaml:33-39
────────────────────────────────────────
  33 ┌ - apiGroups:
  34 │   - ""
  35 │   resources:
  36 │   - configmaps
  37 │   - secrets
  38 │   verbs:
  39 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role166.yaml:27-32
────────────────────────────────────────
  27 ┌ - apiGroups:
  28 │   - apps
  29 │   resources:
  30 │   - statefulsets
  31 │   verbs:
  32 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role166.yaml:33-39
────────────────────────────────────────
  33 ┌ - apiGroups:
  34 │   - ""
  35 │   resources:
  36 │   - configmaps
  37 │   - secrets
  38 │   verbs:
  39 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role166.yaml:27-32
────────────────────────────────────────
  27 ┌ - apiGroups:
  28 │   - apps
  29 │   resources:
  30 │   - statefulsets
  31 │   verbs:
  32 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role166.yaml:40-46
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - ""
  42 │   resources:
  43 │   - pods
  44 │   verbs:
  45 │   - list
  46 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role166.yaml:33-39
────────────────────────────────────────
  33 ┌ - apiGroups:
  34 │   - ""
  35 │   resources:
  36 │   - configmaps
  37 │   - secrets
  38 │   verbs:
  39 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role166.yaml:47-57
────────────────────────────────────────
  47 ┌ - apiGroups:
  48 │   - ""
  49 │   resources:
  50 │   - services
  51 │   - services/finalizers
  52 │   - endpoints
  53 │   verbs:
  54 │   - get
  55 └   - create
  ..   
────────────────────────────────────────



cluster-role17.yaml (kubernetes)
================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role17.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role170.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role170.yaml:40-46
────────────────────────────────────────
  40 ┌   - apiGroups:
  41 │       - ""
  42 │     resources:
  43 │       - configmaps
  44 │       - secrets
  45 │     verbs:
  46 └       - "*"
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role170.yaml:34-39
────────────────────────────────────────
  34 ┌   - apiGroups:
  35 │       - apps
  36 │     resources:
  37 │       - statefulsets
  38 │     verbs:
  39 └       - "*"
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role170.yaml:40-46
────────────────────────────────────────
  40 ┌   - apiGroups:
  41 │       - ""
  42 │     resources:
  43 │       - configmaps
  44 │       - secrets
  45 │     verbs:
  46 └       - "*"
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role170.yaml:34-39
────────────────────────────────────────
  34 ┌   - apiGroups:
  35 │       - apps
  36 │     resources:
  37 │       - statefulsets
  38 │     verbs:
  39 └       - "*"
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role170.yaml:47-53
────────────────────────────────────────
  47 ┌   - apiGroups:
  48 │       - ""
  49 │     resources:
  50 │       - pods
  51 │     verbs:
  52 │       - list
  53 └       - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role170.yaml:40-46
────────────────────────────────────────
  40 ┌   - apiGroups:
  41 │       - ""
  42 │     resources:
  43 │       - configmaps
  44 │       - secrets
  45 │     verbs:
  46 └       - "*"
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role170.yaml:54-64
────────────────────────────────────────
  54 ┌   - apiGroups:
  55 │       - ""
  56 │     resources:
  57 │       - services
  58 │       - services/finalizers
  59 │       - endpoints
  60 │     verbs:
  61 │       - get
  62 └       - create
  ..   
────────────────────────────────────────



cluster-role171.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role171.yaml:6-24
────────────────────────────────────────
   6 ┌   - apiGroups:
   7 │       - ""
   8 │     resources:
   9 │       - configmaps
  10 │       - secrets
  11 │       - nodes
  12 │       - pods
  13 │       - services
  14 └       - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role173.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role173.yaml:40-46
────────────────────────────────────────
  40 ┌   - apiGroups:
  41 │       - ""
  42 │     resources:
  43 │       - configmaps
  44 │       - secrets
  45 │     verbs:
  46 └       - "*"
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role173.yaml:34-39
────────────────────────────────────────
  34 ┌   - apiGroups:
  35 │       - apps
  36 │     resources:
  37 │       - statefulsets
  38 │     verbs:
  39 └       - "*"
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role173.yaml:40-46
────────────────────────────────────────
  40 ┌   - apiGroups:
  41 │       - ""
  42 │     resources:
  43 │       - configmaps
  44 │       - secrets
  45 │     verbs:
  46 └       - "*"
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role173.yaml:34-39
────────────────────────────────────────
  34 ┌   - apiGroups:
  35 │       - apps
  36 │     resources:
  37 │       - statefulsets
  38 │     verbs:
  39 └       - "*"
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role173.yaml:47-53
────────────────────────────────────────
  47 ┌   - apiGroups:
  48 │       - ""
  49 │     resources:
  50 │       - pods
  51 │     verbs:
  52 │       - list
  53 └       - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role173.yaml:40-46
────────────────────────────────────────
  40 ┌   - apiGroups:
  41 │       - ""
  42 │     resources:
  43 │       - configmaps
  44 │       - secrets
  45 │     verbs:
  46 └       - "*"
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role173.yaml:54-64
────────────────────────────────────────
  54 ┌   - apiGroups:
  55 │       - ""
  56 │     resources:
  57 │       - services
  58 │       - services/finalizers
  59 │       - endpoints
  60 │     verbs:
  61 │       - get
  62 └       - create
  ..   
────────────────────────────────────────



cluster-role174.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role174.yaml:6-24
────────────────────────────────────────
   6 ┌   - apiGroups:
   7 │       - ""
   8 │     resources:
   9 │       - configmaps
  10 │       - secrets
  11 │       - nodes
  12 │       - pods
  13 │       - services
  14 └       - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role175.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role175.yaml:6-24
────────────────────────────────────────
   6 ┌   - apiGroups:
   7 │       - ""
   8 │     resources:
   9 │       - configmaps
  10 │       - secrets
  11 │       - nodes
  12 │       - pods
  13 │       - services
  14 └       - serviceaccounts
  ..   
────────────────────────────────────────



cluster-role177.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'service-reader' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role177.yaml:7-9
────────────────────────────────────────
   7 ┌   - apiGroups: [""]
   8 │     resources: ["*"]
   9 └     verbs: ["*"]
────────────────────────────────────────



cluster-role179.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role179.yaml:10-27
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role181.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'manifest-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role181.yaml:10-27
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role182.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'pod-creator-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role182.yaml:6-15
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ""
   8 │   resources:
   9 │   - pods
  10 │   - services
  11 │   - ingresses
  12 │   verbs:
  13 │   - create
  14 │   - delete
  15 └   - get
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'pod-creator-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role182.yaml:6-15
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ""
   8 │   resources:
   9 │   - pods
  10 │   - services
  11 │   - ingresses
  12 │   verbs:
  13 │   - create
  14 │   - delete
  15 └   - get
────────────────────────────────────────



cluster-role183.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'pod-creator-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role183.yaml:6-15
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ""
   8 │   resources:
   9 │   - pods
  10 │   - services
  11 │   - ingresses
  12 │   verbs:
  13 │   - create
  14 │   - delete
  15 └   - get
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'pod-creator-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role183.yaml:6-15
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ""
   8 │   resources:
   9 │   - pods
  10 │   - services
  11 │   - ingresses
  12 │   verbs:
  13 │   - create
  14 │   - delete
  15 └   - get
────────────────────────────────────────



cluster-role188_1.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 2, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'system:kube-vip-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role188_1.yaml:14-23
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - services
  18 │   - endpoints
  19 │   verbs:
  20 │   - list
  21 │   - get
  22 │   - watch
  23 └   - update
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'system:kube-vip-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role188_1.yaml:44-52
────────────────────────────────────────
  44 ┌ - apiGroups:
  45 │   - discovery.k8s.io
  46 │   resources:
  47 │   - endpointslices
  48 │   verbs:
  49 │   - list
  50 │   - get
  51 │   - watch
  52 └   - update
────────────────────────────────────────



cluster-role189.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0047 (HIGH): Role permits privilege escalation from node proxy
════════════════════════════════════════
Check whether role permits privilege escalation from node proxy

See https://avd.aquasec.com/misconfig/ksv047
────────────────────────────────────────
 cluster-role189.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups: [""]
   7 │   resources:
   8 │   - nodes
   9 │   - nodes/proxy
  10 │   - services
  11 │   - endpoints
  12 │   - pods
  13 └   verbs: ["get", "list", "watch"]
────────────────────────────────────────



cluster-role190.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role190.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role191.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role191.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role192.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role192.yaml:10-27
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   - secrets
  15 │   - nodes
  16 │   - pods
  17 │   - services
  18 └   - resourcequotas
  ..   
────────────────────────────────────────



cluster-role193.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 1)

AVD-KSV-0048 (MEDIUM): ClusterRole 'otaq-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role193.yaml:26-28
────────────────────────────────────────
  26 ┌   - apiGroups: ["apps"]
  27 │     resources: ["deployments"]
  28 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'otaq-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role193.yaml:18-20
────────────────────────────────────────
  18 ┌   - apiGroups: [""]
  19 │     resources: [events, services, namespaces]
  20 └     verbs: [create, list, watch]
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'otaq-operator' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role193.yaml:22-24
────────────────────────────────────────
  22 ┌   - apiGroups: [admissionregistration.k8s.io]
  23 │     resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
  24 └     verbs: ["get", "list", "create", "patch", "watch"]
────────────────────────────────────────



cluster-role194.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 1)

AVD-KSV-0048 (MEDIUM): ClusterRole 'axis-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role194.yaml:26-28
────────────────────────────────────────
  26 ┌   - apiGroups: ["apps"]
  27 │     resources: ["deployments"]
  28 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'axis-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role194.yaml:18-20
────────────────────────────────────────
  18 ┌   - apiGroups: [""]
  19 │     resources: [events, services, namespaces]
  20 └     verbs: [create, list, watch]
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'axis-operator' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role194.yaml:22-24
────────────────────────────────────────
  22 ┌   - apiGroups: [admissionregistration.k8s.io]
  23 │     resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
  24 └     verbs: ["get", "list", "create", "patch", "watch"]
────────────────────────────────────────



cluster-role195.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'video-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role195.yaml:30-32
────────────────────────────────────────
  30 ┌   - apiGroups: [""]
  31 │     resources: [secrets]
  32 └     verbs: [get, list, watch]
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'video-controller' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role195.yaml:26-28
────────────────────────────────────────
  26 ┌   - apiGroups: ["apps"]
  27 │     resources: ["deployments"]
  28 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'video-controller' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role195.yaml:18-20
────────────────────────────────────────
  18 ┌   - apiGroups: [""]
  19 │     resources: [events, services, namespaces]
  20 └     verbs: [create, list, watch]
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'video-controller' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role195.yaml:22-24
────────────────────────────────────────
  22 ┌   - apiGroups: [admissionregistration.k8s.io]
  23 │     resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
  24 └     verbs: ["get", "list", "create", "patch", "watch"]
────────────────────────────────────────



cluster-role196.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 1)

AVD-KSV-0048 (MEDIUM): ClusterRole 'video-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role196.yaml:26-28
────────────────────────────────────────
  26 ┌   - apiGroups: ["apps"]
  27 │     resources: ["deployments"]
  28 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'video-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role196.yaml:18-20
────────────────────────────────────────
  18 ┌   - apiGroups: [""]
  19 │     resources: [events, services, namespaces]
  20 └     verbs: [create, list, watch]
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'video-operator' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role196.yaml:22-24
────────────────────────────────────────
  22 ┌   - apiGroups: [admissionregistration.k8s.io]
  23 │     resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
  24 └     verbs: ["get", "list", "create", "patch", "watch"]
────────────────────────────────────────



cluster-role197.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 1)

AVD-KSV-0048 (MEDIUM): ClusterRole 'plc-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role197.yaml:26-28
────────────────────────────────────────
  26 ┌   - apiGroups: ["apps"]
  27 │     resources: ["deployments"]
  28 └     verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'plc-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role197.yaml:18-20
────────────────────────────────────────
  18 ┌   - apiGroups: [""]
  19 │     resources: [events, services, namespaces]
  20 └     verbs: [create, list, watch, patch]
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'plc-operator' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cluster-role197.yaml:22-24
────────────────────────────────────────
  22 ┌   - apiGroups: [admissionregistration.k8s.io]
  23 │     resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
  24 └     verbs: ["get", "list", "create", "patch", "watch"]
────────────────────────────────────────



cluster-role2.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'nginx-ingress-clusterrole' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role2.yaml:6-16
────────────────────────────────────────
   6 ┌   - apiGroups:
   7 │       - ""
   8 │     resources:
   9 │       - configmaps
  10 │       - endpoints
  11 │       - nodes
  12 │       - pods
  13 │       - secrets
  14 └     verbs:
  ..   
────────────────────────────────────────



cluster-role20.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role20.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role20.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - apps
  26 │   resources:
  27 │   - statefulsets
  28 │   verbs:
  29 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role20.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role20.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - apps
  26 │   resources:
  27 │   - statefulsets
  28 │   verbs:
  29 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role20.yaml:37-43
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ""
  39 │   resources:
  40 │   - pods
  41 │   verbs:
  42 │   - list
  43 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role20.yaml:30-36
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ""
  32 │   resources:
  33 │   - configmaps
  34 │   - secrets
  35 │   verbs:
  36 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role20.yaml:44-54
────────────────────────────────────────
  44 ┌ - apiGroups:
  45 │   - ""
  46 │   resources:
  47 │   - services
  48 │   - services/finalizers
  49 │   - endpoints
  50 │   verbs:
  51 │   - get
  52 └   - create
  ..   
────────────────────────────────────────



cluster-role200.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 109, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 3)

AVD-KSV-0041 (CRITICAL): ClusterRole 'prometheus-operator' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role200.yaml:33-39
────────────────────────────────────────
  33 ┌ - apiGroups:
  34 │   - ""
  35 │   resources:
  36 │   - configmaps
  37 │   - secrets
  38 │   verbs:
  39 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role200.yaml:27-32
────────────────────────────────────────
  27 ┌ - apiGroups:
  28 │   - apps
  29 │   resources:
  30 │   - statefulsets
  31 │   verbs:
  32 └   - '*'
────────────────────────────────────────


AVD-KSV-0045 (CRITICAL): Role permits wildcard verb on specific resources
════════════════════════════════════════
Check whether role permits wildcard verb on specific resources

See https://avd.aquasec.com/misconfig/ksv045
────────────────────────────────────────
 cluster-role200.yaml:33-39
────────────────────────────────────────
  33 ┌ - apiGroups:
  34 │   - ""
  35 │   resources:
  36 │   - configmaps
  37 │   - secrets
  38 │   verbs:
  39 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role200.yaml:27-32
────────────────────────────────────────
  27 ┌ - apiGroups:
  28 │   - apps
  29 │   resources:
  30 │   - statefulsets
  31 │   verbs:
  32 └   - '*'
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cluster-role200.yaml:40-46
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - ""
  42 │   resources:
  43 │   - pods
  44 │   verbs:
  45 │   - list
  46 └   - delete
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'prometheus-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cluster-role200.yaml:33-39
────────────────────────────────────────
  33 ┌ - apiGroups:
  34 │   - ""
  35 │   resources:
  36 │   - configmaps
  37 │   - secrets
  38 │   verbs:
  39 └   - '*'
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'prometheus-operator' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cluster-role200.yaml:47-57
────────────────────────────────────────
  47 ┌ - apiGroups:
  48 │   - ""
  49 │   resources:
  50 │   - services
  51 │   - services/finalizers
  52 │   - endpoints
  53 │   verbs:
  54 │   - get
  55 └   - create
  ..   
────────────────────────────────────────



cluster-role201.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'role-cluster-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 cluster-role201.yaml:9-11
────────────────────────────────────────
   9 ┌   - apiGroups: ["chaos-mesh.org"]
  10 │     resources: ["*"]
  11 └     verbs: ["get", "list", "watch", "create", "delete", "patch", "update"]
────────────────────────────────────────



cluster-role202.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'kube-state-metrics' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cluster-role202.yaml:9-26
────────────────────────────────────────
   9 ┌ - apiGroups:
  10 │   - ""
  11 │   resources:
  12 │   - configmaps
  13 │   - secrets
  14 │   - nodes
  15 │   - pods
  16 │   - services
  17 └   - resourcequotas
  ..   
────────────────────────────────────────


