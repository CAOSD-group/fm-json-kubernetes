
Report Summary

┌────────────────────────────────────┬────────────┬───────────────────┐
│               Target               │    Type    │ Misconfigurations │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm3.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm4.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm5.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm6.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm7.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm8.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-config-cm9.yaml            │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_1.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_10.yaml               │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_11.yaml               │ kubernetes │        21         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_2.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_3.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_4.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_5.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_6.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_7.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_8.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-etcd_9.yaml                │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-hook.yaml                  │ kubernetes │        18         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator.yaml    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_10.yaml │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_11.yaml │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_12.yaml │ kubernetes │        11         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_2.yaml  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_4.yaml  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_5.yaml  │ kubernetes │         3         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_6.yaml  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_7.yaml  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_8.yaml  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-initializ-operator_9.yaml  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-job.yaml                   │ kubernetes │        21         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-reference-cni-plugins.yaml │ kubernetes │        32         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install-stress-test.yaml           │ kubernetes │        15         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_13.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_16.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_17.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_4.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install109_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_4.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install110_8.yaml                  │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install111.yaml                    │ kubernetes │        14         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_13.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_14.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_6.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install113_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_12.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_4.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_5.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install114_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_12.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_4.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_5.yaml                  │ kubernetes │         4         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install115_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_13.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_14.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_15.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_16.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_17.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_18.yaml                 │ kubernetes │        11         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install116_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_6.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_7.yaml                  │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install117_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install118.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install118_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install118_2.yaml                  │ kubernetes │        13         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_11.yaml                 │ kubernetes │        11         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_3.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_4.yaml                  │ kubernetes │         3         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install119_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_13.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_14.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_15.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_16.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_17.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_21.yaml                 │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install120_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_13.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_14.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_17.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_3.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install121_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_41.yaml                 │ kubernetes │        17         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_42.yaml                 │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_43.yaml                 │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_47.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_48.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_49.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_50.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_51.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install122_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install126.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install126_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install126_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install126_3.yaml                  │ kubernetes │        19         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_17.yaml                 │ kubernetes │         3         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_44.yaml                 │ kubernetes │        16         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_45.yaml                 │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_46.yaml                 │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_50.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_51.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_54.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_55.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_56.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install127_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_44.yaml                 │ kubernetes │        17         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_45.yaml                 │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_46.yaml                 │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_50.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_51.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_54.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_55.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_56.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install128_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_10.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_2.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_3.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install129_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install130.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install130_1.yaml                  │ kubernetes │         3         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install130_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install130_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install130_4.yaml                  │ kubernetes │         6         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_18.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_43.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_44.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_46.yaml                 │ kubernetes │        16         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_47.yaml                 │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_48.yaml                 │ kubernetes │        14         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_54.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_55.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_56.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_57.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_58.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install131_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_12.yaml                 │ kubernetes │        12         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_4.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_5.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install132_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_44.yaml                 │ kubernetes │        16         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_45.yaml                 │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_46.yaml                 │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_50.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_51.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_54.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_55.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_56.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install133_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_13.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_14.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_15.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_16.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_17.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_19.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_20.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_27.yaml                 │ kubernetes │         4         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_30.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_34.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_35.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_43.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_44.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_45.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_46.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_47.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_48.yaml                 │ kubernetes │         6         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install135_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install137.yaml                    │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install137_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install137_2.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install137_3.yaml                  │ kubernetes │        34         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_18.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_44.yaml                 │ kubernetes │        17         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_45.yaml                 │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_46.yaml                 │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_50.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_51.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_54.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_55.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_56.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install138_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_10.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_11.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_12.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_13.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_14.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_15.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_16.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_17.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_18.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_19.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_20.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_21.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_22.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_23.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_24.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_25.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_26.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_27.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_28.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_29.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_30.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_31.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_32.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_33.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_34.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_35.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_36.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_37.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_38.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_39.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_40.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_42.yaml                  │ kubernetes │        17         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_43.yaml                  │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_44.yaml                  │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_48.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_49.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_5.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_50.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_51.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_52.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_53.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_54.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_6.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_7.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_8.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install13_9.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_10.yaml                 │ kubernetes │        14         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_13.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_5.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install141_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143.yaml                    │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143_2.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143_3.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install143_6.yaml                  │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_1.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_10.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_11.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_12.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_13.yaml                 │ kubernetes │        30         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_4.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_5.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install144_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_10.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_11.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_12.yaml                 │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_13.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_14.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_15.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_16.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_17.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_18.yaml                 │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_19.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_20.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_21.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_22.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_23.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_24.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_25.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_26.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_27.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_28.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_29.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_3.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_30.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_31.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_32.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_33.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_34.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_35.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_36.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_37.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_38.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_39.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_4.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_40.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_41.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_42.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_43.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_44.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_46.yaml                 │ kubernetes │        16         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_47.yaml                 │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_48.yaml                 │ kubernetes │        14         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_5.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_52.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_53.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_54.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_55.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_56.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_57.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_58.yaml                 │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_6.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_7.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_8.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install145_9.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_10.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_11.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_12.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_13.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_14.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_15.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_16.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_17.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_18.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_19.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_20.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_21.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_22.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_23.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_24.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_25.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_26.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_27.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_28.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_29.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_30.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_31.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_32.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_33.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_34.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_35.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_36.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_37.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_38.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_39.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_40.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_42.yaml                  │ kubernetes │        16         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_43.yaml                  │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_44.yaml                  │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_48.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_49.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_5.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_50.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_51.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_52.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_53.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_54.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_6.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_7.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_8.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install14_9.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_10.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_11.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_12.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_13.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_14.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_15.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_16.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_17.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_18.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_19.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_20.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_21.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_22.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_23.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_24.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_25.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_26.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_27.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_28.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_29.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_30.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_31.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_32.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_34.yaml                  │ kubernetes │         8         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_35.yaml                  │ kubernetes │         7         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_39.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_40.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_41.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_42.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_43.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_44.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_5.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_6.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_7.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_8.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install17_9.yaml                   │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_10.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_11.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_12.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_13.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_14.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_15.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_16.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_17.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_18.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_19.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_2.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_20.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_21.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_22.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_23.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_24.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_25.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_26.yaml                  │ kubernetes │        18         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_27.yaml                  │ kubernetes │        34         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_28.yaml                  │ kubernetes │        17         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_29.yaml                  │ kubernetes │        18         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_3.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_30.yaml                  │ kubernetes │        18         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_4.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_5.yaml                   │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_6.yaml                   │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_7.yaml                   │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_8.yaml                   │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install18_9.yaml                   │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19.yaml                     │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_10.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_11.yaml                  │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_12.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_13.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_14.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_15.yaml                  │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_16.yaml                  │ kubernetes │         4         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_17.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_18.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_19.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_20.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_21.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_22.yaml                  │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_23.yaml                  │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_24.yaml                  │ kubernetes │         9         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install19_9.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_10.yaml                   │ kubernetes │         1         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_11.yaml                   │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_12.yaml                   │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_13.yaml                   │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_14.yaml                   │ kubernetes │         2         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_15.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_16.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_17.yaml                   │ kubernetes │         0         │
├────────────────────────────────────┼────────────┼───────────────────┤
│ install1_18.yaml                   │ kubernetes │         0         │
└────────────────────────────────────┴────────────┴───────────────────┘
Legend:
- '-': Not scanned
- '0': Clean (no security findings detected)


install-etcd_10.yaml (kubernetes)
=================================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'reports-server' of Deployment 'reports-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'reports-server' of Deployment 'reports-server' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'reports-server' of Deployment 'reports-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'reports-server' of Deployment 'reports-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'reports-server' of Deployment 'reports-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'reports-server' of Deployment 'reports-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'reports-server' of Deployment 'reports-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container reports-server in deployment reports-server (namespace: reports-server) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install-etcd_10.yaml:35-101
────────────────────────────────────────
  35 ┌       - name: reports-server
  36 │         args:
  37 │         - --etcd
  38 │         - --etcdSkipTLS
  39 │         - --etcdEndpoints=https://etcd-0.etcd.reports-server:2379,https://etcd-1.etcd.reports-server:2379,https://etcd-2.etcd.reports-server:2379
  40 │         - --servicename=reports-server
  41 │         - --servicens=reports-server
  42 │         - --storereports=true
  43 └         - --storeephemeralreports=true
  ..   
────────────────────────────────────────



install-etcd_11.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 94, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 14, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KCV-0042 (LOW): Ensure that the --cert-file and --key-file arguments are set as appropriate
════════════════════════════════════════
Configure TLS encryption for the etcd service.

See https://avd.aquasec.com/misconfig/kcv0042
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KCV-0043 (LOW): Ensure that the --client-cert-auth argument is set to true
════════════════════════════════════════
Enable client authentication on etcd service.

See https://avd.aquasec.com/misconfig/kcv0043
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KCV-0045 (LOW): Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate
════════════════════════════════════════
etcd should be configured to make use of TLS encryption for peer connections.

See https://avd.aquasec.com/misconfig/kcv0045
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KCV-0046 (LOW): Ensure that the --peer-client-cert-auth argument is set to true
════════════════════════════════════════
etcd should be configured for peer authentication.

See https://avd.aquasec.com/misconfig/kcv0046
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'etcd' of StatefulSet 'etcd' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'etcd' of StatefulSet 'etcd' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'etcd' of 'statefulset' 'etcd' in 'reports-server' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'etcd' of StatefulSet 'etcd' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'etcd' of StatefulSet 'etcd' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'etcd' of StatefulSet 'etcd' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'etcd' of StatefulSet 'etcd' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'etcd' of StatefulSet 'etcd' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'etcd' of StatefulSet 'etcd' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'etcd' of StatefulSet 'etcd' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'etcd' of StatefulSet 'etcd' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "etcd" of statefulset "etcd" in "reports-server" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container etcd in reports-server namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): statefulset etcd in reports-server namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-etcd_11.yaml:29-102
────────────────────────────────────────
  29 ┌       affinity:
  30 │         podAntiAffinity:
  31 │           preferredDuringSchedulingIgnoredDuringExecution:
  32 │           - weight: 1
  33 │             podAffinityTerm:
  34 │               labelSelector:
  35 │                 matchExpressions:
  36 │                 - key: app
  37 └                   operator: In
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container etcd in statefulset etcd (namespace: reports-server) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install-etcd_11.yaml:42-102
────────────────────────────────────────
  42 ┌       - name: etcd
  43 │         image: quay.io/coreos/etcd:v3.5.15
  44 │         imagePullPolicy: IfNotPresent
  45 │         ports:
  46 │         - name: etcd-client
  47 │           containerPort: 2379
  48 │         - name: etcd-server
  49 │           containerPort: 2380
  50 └         - name: etcd-metrics
  ..   
────────────────────────────────────────



install-hook.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'install-hook' of Job 'install-hook' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'install-hook' of Job 'install-hook' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'install-hook' of 'job' 'install-hook' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'install-hook' of Job 'install-hook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'install-hook' of Job 'install-hook' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'install-hook' of Job 'install-hook' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'install-hook' of Job 'install-hook' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'install-hook' of Job 'install-hook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'install-hook' of Job 'install-hook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'install-hook' of Job 'install-hook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'install-hook' of Job 'install-hook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'install-hook' of Job 'install-hook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "install-hook" of job "install-hook" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): job install-hook in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install-hook.yaml:7-9
────────────────────────────────────────
   7 ┌   name: install-hook
   8 │   annotations:
   9 └      "helm.sh/hook": post-install
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container install-hook in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-hook.yaml:14-16
────────────────────────────────────────
  14 ┌       - name: install-hook
  15 │         image: alpine
  16 └         command: ["echo",  "Successful Installation"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): job install-hook in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-hook.yaml:13-17
────────────────────────────────────────
  13 ┌       containers:
  14 │       - name: install-hook
  15 │         image: alpine
  16 │         command: ["echo",  "Successful Installation"]
  17 └       restartPolicy: Never
────────────────────────────────────────



install-initializ-operator_12.yaml (kubernetes)
===============================================
Tests: 119 (SUCCESSES: 108, FAILURES: 11)
Failures: 11 (UNKNOWN: 0, LOW: 6, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'initializ-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-initializ-operator_12.yaml:43-65
────────────────────────────────────────
  43 ┌       - args:
  44 │         - --secure-listen-address=0.0.0.0:8443
  45 │         - --upstream=http://127.0.0.1:8080/
  46 │         - --logtostderr=true
  47 │         - --v=0
  48 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  49 │         name: kube-rbac-proxy
  50 │         ports:
  51 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'initializ-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-initializ-operator_12.yaml:66-97
────────────────────────────────────────
  66 ┌       - args:
  67 │         - --health-probe-bind-address=:8081
  68 │         - --metrics-bind-address=127.0.0.1:8080
  69 │         - --leader-elect
  70 │         command:
  71 │         - /manager
  72 │         image: initializ/secrets-operator:v1alpha1
  73 │         livenessProbe:
  74 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'initializ-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-initializ-operator_12.yaml:43-65
────────────────────────────────────────
  43 ┌       - args:
  44 │         - --secure-listen-address=0.0.0.0:8443
  45 │         - --upstream=http://127.0.0.1:8080/
  46 │         - --logtostderr=true
  47 │         - --v=0
  48 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  49 │         name: kube-rbac-proxy
  50 │         ports:
  51 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'initializ-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-initializ-operator_12.yaml:66-97
────────────────────────────────────────
  66 ┌       - args:
  67 │         - --health-probe-bind-address=:8081
  68 │         - --metrics-bind-address=127.0.0.1:8080
  69 │         - --leader-elect
  70 │         command:
  71 │         - /manager
  72 │         image: initializ/secrets-operator:v1alpha1
  73 │         livenessProbe:
  74 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'initializ-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-initializ-operator_12.yaml:43-65
────────────────────────────────────────
  43 ┌       - args:
  44 │         - --secure-listen-address=0.0.0.0:8443
  45 │         - --upstream=http://127.0.0.1:8080/
  46 │         - --logtostderr=true
  47 │         - --v=0
  48 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  49 │         name: kube-rbac-proxy
  50 │         ports:
  51 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'initializ-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-initializ-operator_12.yaml:66-97
────────────────────────────────────────
  66 ┌       - args:
  67 │         - --health-probe-bind-address=:8081
  68 │         - --metrics-bind-address=127.0.0.1:8080
  69 │         - --leader-elect
  70 │         command:
  71 │         - /manager
  72 │         image: initializ/secrets-operator:v1alpha1
  73 │         livenessProbe:
  74 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-initializ-operator_12.yaml:43-65
────────────────────────────────────────
  43 ┌       - args:
  44 │         - --secure-listen-address=0.0.0.0:8443
  45 │         - --upstream=http://127.0.0.1:8080/
  46 │         - --logtostderr=true
  47 │         - --v=0
  48 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  49 │         name: kube-rbac-proxy
  50 │         ports:
  51 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-initializ-operator_12.yaml:66-97
────────────────────────────────────────
  66 ┌       - args:
  67 │         - --health-probe-bind-address=:8081
  68 │         - --metrics-bind-address=127.0.0.1:8080
  69 │         - --leader-elect
  70 │         command:
  71 │         - /manager
  72 │         image: initializ/secrets-operator:v1alpha1
  73 │         livenessProbe:
  74 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "initializ-controller-manager" in "initializ-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-initializ-operator_12.yaml:43-65
────────────────────────────────────────
  43 ┌       - args:
  44 │         - --secure-listen-address=0.0.0.0:8443
  45 │         - --upstream=http://127.0.0.1:8080/
  46 │         - --logtostderr=true
  47 │         - --v=0
  48 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  49 │         name: kube-rbac-proxy
  50 │         ports:
  51 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "initializ-controller-manager" in "initializ-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-initializ-operator_12.yaml:66-97
────────────────────────────────────────
  66 ┌       - args:
  67 │         - --health-probe-bind-address=:8081
  68 │         - --metrics-bind-address=127.0.0.1:8080
  69 │         - --leader-elect
  70 │         command:
  71 │         - /manager
  72 │         image: initializ/secrets-operator:v1alpha1
  73 │         livenessProbe:
  74 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment initializ-controller-manager (namespace: initializ-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install-initializ-operator_12.yaml:66-97
────────────────────────────────────────
  66 ┌       - args:
  67 │         - --health-probe-bind-address=:8081
  68 │         - --metrics-bind-address=127.0.0.1:8080
  69 │         - --leader-elect
  70 │         command:
  71 │         - /manager
  72 │         image: initializ/secrets-operator:v1alpha1
  73 │         livenessProbe:
  74 └           httpGet:
  ..   
────────────────────────────────────────



install-initializ-operator_4.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'initializ-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install-initializ-operator_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install-initializ-operator_5.yaml (kubernetes)
==============================================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'initializ-manager-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install-initializ-operator_5.yaml:18-28
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - secrets
  22 │   verbs:
  23 │   - create
  24 │   - delete
  25 │   - get
  26 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'initializ-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install-initializ-operator_5.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - apps
  31 │   resources:
  32 │   - deployments
  33 │   verbs:
  34 │   - get
  35 │   - list
  36 │   - update
  37 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'initializ-manager-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install-initializ-operator_5.yaml:7-17
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - create
  13 │   - delete
  14 │   - get
  15 └   - list
  ..   
────────────────────────────────────────



install-job.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 93, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kwasm-initializer' of Job 'default-init' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kwasm-initializer' of Job 'default-init' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kwasm-initializer' of 'job' 'default-init' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0010 (HIGH): Job 'default-init' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 install-job.yaml:7-34
────────────────────────────────────────
   7 ┌   template:
   8 │     metadata:
   9 │       labels:
  10 │         name: kwasm-initializer
  11 │         app: default-init
  12 │     spec:
  13 │       restartPolicy: Never
  14 │       hostPID: true
  15 └       volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kwasm-initializer' of Job 'default-init' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kwasm-initializer' of Job 'default-init' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kwasm-initializer' of Job 'default-init' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'kwasm-initializer' of Job 'default-init' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'kwasm-initializer' of Job 'default-init' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'kwasm-initializer' of Job 'default-init' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kwasm-initializer' of Job 'default-init' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kwasm-initializer' of Job 'default-init' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kwasm-initializer' of Job 'default-init' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Job 'default-init' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 install-job.yaml:7-34
────────────────────────────────────────
   7 ┌   template:
   8 │     metadata:
   9 │       labels:
  10 │         name: kwasm-initializer
  11 │         app: default-init
  12 │     spec:
  13 │       restartPolicy: Never
  14 │       hostPID: true
  15 └       volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kwasm-initializer" of job "default-init" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────


AVD-KSV-0110 (LOW): job default-init in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install-job.yaml:4-5
────────────────────────────────────────
   4 ┌   creationTimestamp: null
   5 └   name: default-init
────────────────────────────────────────


AVD-KSV-0118 (HIGH): job default-init in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-job.yaml:13-33
────────────────────────────────────────
  13 ┌       restartPolicy: Never
  14 │       hostPID: true
  15 │       volumes:
  16 │         - name: node-root
  17 │           hostPath:
  18 │             path: /
  19 │         - name: entrypoint
  20 │           configMap:
  21 └             name: entrypoint
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): job default-init in default namespace shouldn't have volumes set to {"/"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 install-job.yaml:7-34
────────────────────────────────────────
   7 ┌   template:
   8 │     metadata:
   9 │       labels:
  10 │         name: kwasm-initializer
  11 │         app: default-init
  12 │     spec:
  13 │       restartPolicy: Never
  14 │       hostPID: true
  15 └       volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kwasm-initializer in job default-init (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install-job.yaml:24-33
────────────────────────────────────────
  24 ┌         - image: ghcr.io/kwasm/kwasm-node-installer:master
  25 │           name: kwasm-initializer
  26 │           env:
  27 │             - name: NODE_ROOT
  28 │               value: /mnt/node-root
  29 │           securityContext:
  30 │             privileged: true
  31 │           volumeMounts:
  32 │             - name: node-root
  33 └               mountPath: /mnt/node-root/
────────────────────────────────────────



install-reference-cni-plugins.yaml (kubernetes)
===============================================
Tests: 126 (SUCCESSES: 94, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 16, MEDIUM: 10, HIGH: 6, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'idle' of DaemonSet 'install-cni-plugins' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'idle' of DaemonSet 'install-cni-plugins' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'idle' of 'daemonset' 'install-cni-plugins' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'install-cni-plugins' of 'daemonset' 'install-cni-plugins' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'install-cni-plugins' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 install-reference-cni-plugins.yaml:12-63
────────────────────────────────────────
  12 ┌   selector:
  13 │     matchLabels:
  14 │       app: install-cni-plugins
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         tier: node
  19 │         app: install-cni-plugins
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'idle' of DaemonSet 'install-cni-plugins' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'idle' of DaemonSet 'install-cni-plugins' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'idle' of DaemonSet 'install-cni-plugins' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'idle' of DaemonSet 'install-cni-plugins' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'idle' of DaemonSet 'install-cni-plugins' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'install-cni-plugins' of DaemonSet 'install-cni-plugins' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'install-cni-plugins' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 install-reference-cni-plugins.yaml:12-63
────────────────────────────────────────
  12 ┌   selector:
  13 │     matchLabels:
  14 │       app: install-cni-plugins
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         tier: node
  19 │         app: install-cni-plugins
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'install-cni-plugins' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 install-reference-cni-plugins.yaml:12-63
────────────────────────────────────────
  12 ┌   selector:
  13 │     matchLabels:
  14 │       app: install-cni-plugins
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         tier: node
  19 │         app: install-cni-plugins
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "idle" of daemonset "install-cni-plugins" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "install-cni-plugins" of daemonset "install-cni-plugins" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container install-cni-plugins in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container install-cni-plugins in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-reference-cni-plugins.yaml:47-59
────────────────────────────────────────
  47 ┌       - name: idle
  48 │         image: busybox
  49 │         command:
  50 │         - sleep
  51 │         args:
  52 │         - 5d
  53 │         resources:
  54 │           requests:
  55 └             cpu: "100m"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset install-cni-plugins in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-reference-cni-plugins.yaml:21-63
────────────────────────────────────────
  21 ┌       affinity:
  22 │         nodeAffinity:
  23 │           requiredDuringSchedulingIgnoredDuringExecution:
  24 │             nodeSelectorTerms:
  25 │             - matchExpressions:
  26 │               - key: kubernetes.io/os
  27 │                 operator: In
  28 │                 values:
  29 └                 - linux
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container install-cni-plugins in daemonset install-cni-plugins (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install-reference-cni-plugins.yaml:36-45
────────────────────────────────────────
  36 ┌       - name: install-cni-plugins
  37 │         image: quay.io/casey_callendrello/cni-plugins:v1.1.0-38-gac86731
  38 │         command:
  39 │         - sh
  40 │         args:
  41 │         - -c
  42 │         - "cp /cni/* /opt/cni/bin/"
  43 │         volumeMounts:
  44 │         - name: cni-plugin
  45 └           mountPath: /opt/cni/bin
────────────────────────────────────────



install-stress-test.yaml (kubernetes)
=====================================
Tests: 114 (SUCCESSES: 99, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 8, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'bbox-stress-2' of 'deployment' 'bbox-stress-2' in 'learn' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'bbox-stress-2' of Deployment 'bbox-stress-2' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "bbox-stress-2" of deployment "bbox-stress-2" in "learn" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install-stress-test.yaml:20-35
────────────────────────────────────────
  20 ┌         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 │           resources:
  28 └             limits:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment bbox-stress-2 in learn namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install-stress-test.yaml:19-40
────────────────────────────────────────
  19 ┌       containers:
  20 │         - name: bbox-stress-2
  21 │           image: busybox
  22 │           command:
  23 │             - /bin/sh
  24 │           args:
  25 │             - '-c'
  26 │             - dd if=/dev/zero of=/dev/null bs=4096
  27 └           resources:
  ..   
────────────────────────────────────────



install109_13.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'cloudstack-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'cloudstack-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install109_13.yaml:68-91
────────────────────────────────────────
  68 ┌       - args:
  69 │         - --secure-listen-address=0.0.0.0:8443
  70 │         - --upstream=http://127.0.0.1:8080/
  71 │         - --logtostderr=true
  72 │         - --v=0
  73 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  74 │         imagePullPolicy: Always
  75 │         name: kube-rbac-proxy
  76 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'cloudstack-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'cloudstack-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install109_13.yaml:68-91
────────────────────────────────────────
  68 ┌       - args:
  69 │         - --secure-listen-address=0.0.0.0:8443
  70 │         - --upstream=http://127.0.0.1:8080/
  71 │         - --logtostderr=true
  72 │         - --v=0
  73 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  74 │         imagePullPolicy: Always
  75 │         name: kube-rbac-proxy
  76 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'cloudstack-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'cloudstack-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install109_13.yaml:68-91
────────────────────────────────────────
  68 ┌       - args:
  69 │         - --secure-listen-address=0.0.0.0:8443
  70 │         - --upstream=http://127.0.0.1:8080/
  71 │         - --logtostderr=true
  72 │         - --v=0
  73 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  74 │         imagePullPolicy: Always
  75 │         name: kube-rbac-proxy
  76 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'cloudstack-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install109_13.yaml:68-91
────────────────────────────────────────
  68 ┌       - args:
  69 │         - --secure-listen-address=0.0.0.0:8443
  70 │         - --upstream=http://127.0.0.1:8080/
  71 │         - --logtostderr=true
  72 │         - --v=0
  73 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  74 │         imagePullPolicy: Always
  75 │         name: kube-rbac-proxy
  76 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "cloudstack-controller-manager" in "cloudstack-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install109_13.yaml:68-91
────────────────────────────────────────
  68 ┌       - args:
  69 │         - --secure-listen-address=0.0.0.0:8443
  70 │         - --upstream=http://127.0.0.1:8080/
  71 │         - --logtostderr=true
  72 │         - --v=0
  73 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  74 │         imagePullPolicy: Always
  75 │         name: kube-rbac-proxy
  76 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "cloudstack-controller-manager" in "cloudstack-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment cloudstack-controller-manager (namespace: cloudstack-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install109_13.yaml:27-67
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         command:
  32 │         - /manager
  33 │         image: harbor.iblog.pro/test/asura:latest
  34 │         imagePullPolicy: Always
  35 └         livenessProbe:
  ..   
────────────────────────────────────────



install109_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cloudstack-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install109_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install110_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'o11y-rules-oper-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install110_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install110_8.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 3, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'o11y-rules-oper-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'o11y-rules-oper-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'o11y-rules-oper-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'o11y-rules-oper-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "o11y-rules-oper-controller-manager" in "o11y-rules-telemetry-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment o11y-rules-oper-controller-manager (namespace: o11y-rules-telemetry-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install110_8.yaml:27-64
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --leader-elect
  29 │         command:
  30 │         - /manager
  31 │         env:
  32 │         - name: CONTROLLER_MIMIR_API
  33 │           value: http://mimir-gateway.o11y-dev-metrics-mimir.svc.cluster.local/prometheus
  34 │         - name: CONTROLLER_CR_MIMIR_TENANT_ANNOTATION
  35 └           value: telemetry.springernature.com/o11y-tenant
  ..   
────────────────────────────────────────



install111.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'infro' of Deployment 'infro' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'infro' of Deployment 'infro' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'infro' of 'deployment' 'infro' in 'infro' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'infro' of Deployment 'infro' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'infro' of Deployment 'infro' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'infro' of Deployment 'infro' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'infro' of Deployment 'infro' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'infro' of Deployment 'infro' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "infro" of deployment "infro" in "infro" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container infro in infro namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment infro in infro namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install111.yaml:16-42
────────────────────────────────────────
  16 ┌       containers:
  17 │         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 └               memory: "1024Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container infro in deployment infro (namespace: infro) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install111.yaml:17-42
────────────────────────────────────────
  17 ┌         - name: infro
  18 │           image: infrolabs/infro-core:latest
  19 │           resources:
  20 │             requests:
  21 │               memory: "512Mi"
  22 │               cpu: "512m"
  23 │             limits:
  24 │               memory: "1024Mi"
  25 └               cpu: "1024m"
  ..   
────────────────────────────────────────



install113_14.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'torrent-operator-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'torrent-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install113_14.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'torrent-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'torrent-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install113_14.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'torrent-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'torrent-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install113_14.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'torrent-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install113_14.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "torrent-operator-controller-manager" in "torrent-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install113_14.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "torrent-operator-controller-manager" in "torrent-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment torrent-operator-controller-manager (namespace: torrent-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install113_14.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: alphayax/torrent-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install113_6.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'torrent-operator-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install113_6.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install114_12.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'scheduscaler-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'scheduscaler-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install114_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'scheduscaler-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'scheduscaler-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install114_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'scheduscaler-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'scheduscaler-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install114_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'scheduscaler-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install114_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "scheduscaler-controller-manager" in "scheduscaler-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install114_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "scheduscaler-controller-manager" in "scheduscaler-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment scheduscaler-controller-manager (namespace: scheduscaler-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install114_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: vincentdebo/scheduscaler:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install114_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'scheduscaler-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install114_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install114_5.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'scheduscaler-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install114_5.yaml:13-24
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - apps
  15 │   resources:
  16 │   - deployments
  17 │   verbs:
  18 │   - create
  19 │   - delete
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────



install115_12.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'broom-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'broom-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install115_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'broom-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'broom-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install115_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'broom-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'broom-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install115_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'broom-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install115_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "broom-controller-manager" in "broom-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install115_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "broom-controller-manager" in "broom-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment broom-controller-manager (namespace: broom-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install115_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/m3dev/broom:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install115_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'broom-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install115_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install115_5.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 112, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'broom-manager-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install115_5.yaml:26-33
────────────────────────────────────────
  26 ┌ - apiGroups:
  27 │   - ''
  28 │   resources:
  29 │   - secrets
  30 │   verbs:
  31 │   - get
  32 │   - list
  33 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'broom-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install115_5.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - pods
  18 │   verbs:
  19 │   - create
  20 │   - delete
  21 │   - get
  22 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'broom-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install115_5.yaml:60-71
────────────────────────────────────────
  60 ┌ - apiGroups:
  61 │   - batch
  62 │   resources:
  63 │   - cronjobs
  64 │   verbs:
  65 │   - create
  66 │   - delete
  67 │   - get
  68 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'broom-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install115_5.yaml:72-83
────────────────────────────────────────
  72 ┌ - apiGroups:
  73 │   - batch
  74 │   resources:
  75 │   - jobs
  76 │   verbs:
  77 │   - create
  78 │   - delete
  79 │   - get
  80 └   - list
  ..   
────────────────────────────────────────



install116_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'github-operator-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install116_10.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install116_18.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 108, FAILURES: 11)
Failures: 11 (UNKNOWN: 0, LOW: 6, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'github-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install116_18.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'github-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install116_18.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/eczy/github-operator:v0.0.1
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'github-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install116_18.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'github-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install116_18.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/eczy/github-operator:v0.0.1
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'github-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install116_18.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'github-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install116_18.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/eczy/github-operator:v0.0.1
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install116_18.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install116_18.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/eczy/github-operator:v0.0.1
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "github-operator-controller-manager" in "github-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install116_18.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "github-operator-controller-manager" in "github-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install116_18.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/eczy/github-operator:v0.0.1
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment github-operator-controller-manager (namespace: github-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install116_18.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/eczy/github-operator:v0.0.1
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install117_6.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0037 (MEDIUM): Service 'metrics-server' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 install117_6.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: https
  11 │     port: 443
  12 │     protocol: TCP
  13 │     targetPort: https
  14 │   selector:
  15 └     k8s-app: metrics-server
────────────────────────────────────────



install117_7.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 4, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'metrics-server' of Deployment 'metrics-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install117_7.yaml:21-66
────────────────────────────────────────
  21 ┌       - args:
  22 │         - --cert-dir=/tmp
  23 │         - --secure-port=10250
  24 │         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  25 │         - --kubelet-use-node-status-port
  26 │         - --metric-resolution=15s
  27 │         - --kubelet-insecure-tls
  28 │         image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
  29 └         imagePullPolicy: IfNotPresent
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'metrics-server' of Deployment 'metrics-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install117_7.yaml:21-66
────────────────────────────────────────
  21 ┌       - args:
  22 │         - --cert-dir=/tmp
  23 │         - --secure-port=10250
  24 │         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  25 │         - --kubelet-use-node-status-port
  26 │         - --metric-resolution=15s
  27 │         - --kubelet-insecure-tls
  28 │         image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
  29 └         imagePullPolicy: IfNotPresent
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'metrics-server' of Deployment 'metrics-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install117_7.yaml:21-66
────────────────────────────────────────
  21 ┌       - args:
  22 │         - --cert-dir=/tmp
  23 │         - --secure-port=10250
  24 │         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  25 │         - --kubelet-use-node-status-port
  26 │         - --metric-resolution=15s
  27 │         - --kubelet-insecure-tls
  28 │         image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
  29 └         imagePullPolicy: IfNotPresent
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'metrics-server' of Deployment 'metrics-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install117_7.yaml:21-66
────────────────────────────────────────
  21 ┌       - args:
  22 │         - --cert-dir=/tmp
  23 │         - --secure-port=10250
  24 │         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  25 │         - --kubelet-use-node-status-port
  26 │         - --metric-resolution=15s
  27 │         - --kubelet-insecure-tls
  28 │         image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
  29 └         imagePullPolicy: IfNotPresent
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'metrics-server' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 install117_7.yaml:9-73
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       k8s-app: metrics-server
  12 │   strategy:
  13 │     rollingUpdate:
  14 │       maxUnavailable: 0
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment metrics-server in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install117_7.yaml:20-73
────────────────────────────────────────
  20 ┌       containers:
  21 │       - args:
  22 │         - --cert-dir=/tmp
  23 │         - --secure-port=10250
  24 │         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  25 │         - --kubelet-use-node-status-port
  26 │         - --metric-resolution=15s
  27 │         - --kubelet-insecure-tls
  28 └         image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container metrics-server in deployment metrics-server (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install117_7.yaml:21-66
────────────────────────────────────────
  21 ┌       - args:
  22 │         - --cert-dir=/tmp
  23 │         - --secure-port=10250
  24 │         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  25 │         - --kubelet-use-node-status-port
  26 │         - --metric-resolution=15s
  27 │         - --kubelet-insecure-tls
  28 │         image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
  29 └         imagePullPolicy: IfNotPresent
  ..   
────────────────────────────────────────



install118_2.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 102, FAILURES: 13)
Failures: 13 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'hello' of Deployment 'hello' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'hello' of Deployment 'hello' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'hello' of 'deployment' 'hello' in 'hello' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'hello' of Deployment 'hello' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'hello' of Deployment 'hello' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'hello' of Deployment 'hello' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'hello' of Deployment 'hello' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "hello" of deployment "hello" in "hello" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container hello in hello namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment hello in hello namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install118_2.yaml:18-62
────────────────────────────────────────
  18 ┌       containers:
  19 │       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container hello in deployment hello (namespace: hello) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install118_2.yaml:19-61
────────────────────────────────────────
  19 ┌       - env:
  20 │         - name: SERVICE
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               fieldPath: metadata.labels['service']
  24 │         - name: VERSION
  25 │           valueFrom:
  26 │             fieldRef:
  27 └               fieldPath: metadata.labels['version']
  ..   
────────────────────────────────────────



install119_11.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 108, FAILURES: 11)
Failures: 11 (UNKNOWN: 0, LOW: 6, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'ollama-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install119_11.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'ollama-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install119_11.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/nekomeowww/ollama-operator:0.10.7
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'ollama-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install119_11.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'ollama-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install119_11.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/nekomeowww/ollama-operator:0.10.7
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'ollama-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install119_11.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'ollama-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install119_11.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/nekomeowww/ollama-operator:0.10.7
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install119_11.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install119_11.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/nekomeowww/ollama-operator:0.10.7
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "ollama-operator-controller-manager" in "ollama-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install119_11.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "ollama-operator-controller-manager" in "ollama-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install119_11.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/nekomeowww/ollama-operator:0.10.7
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment ollama-operator-controller-manager (namespace: ollama-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install119_11.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: ghcr.io/nekomeowww/ollama-operator:0.10.7
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install119_3.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'ollama-operator-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install119_3.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install119_4.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'ollama-operator-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install119_4.yaml:29-41
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - apps
  31 │   resources:
  32 │   - deployments
  33 │   - statefulsets
  34 │   verbs:
  35 │   - create
  36 │   - delete
  37 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'ollama-operator-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install119_4.yaml:42-54
────────────────────────────────────────
  42 ┌ - apiGroups:
  43 │   - batch
  44 │   resources:
  45 │   - jobs
  46 │   verbs:
  47 │   - create
  48 │   - delete
  49 │   - deletecollection
  50 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'ollama-operator-manager-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 install119_4.yaml:16-28
────────────────────────────────────────
  16 ┌ - apiGroups:
  17 │   - ''
  18 │   resources:
  19 │   - persistentvolumeclaims
  20 │   - services
  21 │   verbs:
  22 │   - create
  23 │   - delete
  24 └   - get
  ..   
────────────────────────────────────────



install120_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'promoter-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install120_10.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install120_13.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'promoter-manager-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install120_13.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   verbs:
  11 │   - get
  12 │   - list
  13 └   - watch
────────────────────────────────────────



install120_21.yaml (kubernetes)
===============================
Tests: 118 (SUCCESSES: 109, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 4, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'promoter-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install120_21.yaml:65-96
────────────────────────────────────────
  65 ┌       - args:
  66 │         - --health-probe-bind-address=:8081
  67 │         - --metrics-bind-address=127.0.0.1:8080
  68 │         - --leader-elect
  69 │         command:
  70 │         - /gitops-promoter
  71 │         image: quay.io/argoprojlabs/gitops-promoter:latest
  72 │         livenessProbe:
  73 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'promoter-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install120_21.yaml:42-64
────────────────────────────────────────
  42 ┌       - args:
  43 │         - --secure-listen-address=0.0.0.0:8443
  44 │         - --upstream=http://127.0.0.1:8080/
  45 │         - --logtostderr=true
  46 │         - --v=0
  47 │         image: quay.io/brancz/kube-rbac-proxy:v0.17.0
  48 │         name: kube-rbac-proxy
  49 │         ports:
  50 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'promoter-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install120_21.yaml:65-96
────────────────────────────────────────
  65 ┌       - args:
  66 │         - --health-probe-bind-address=:8081
  67 │         - --metrics-bind-address=127.0.0.1:8080
  68 │         - --leader-elect
  69 │         command:
  70 │         - /gitops-promoter
  71 │         image: quay.io/argoprojlabs/gitops-promoter:latest
  72 │         livenessProbe:
  73 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'promoter-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install120_21.yaml:42-64
────────────────────────────────────────
  42 ┌       - args:
  43 │         - --secure-listen-address=0.0.0.0:8443
  44 │         - --upstream=http://127.0.0.1:8080/
  45 │         - --logtostderr=true
  46 │         - --v=0
  47 │         image: quay.io/brancz/kube-rbac-proxy:v0.17.0
  48 │         name: kube-rbac-proxy
  49 │         ports:
  50 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'promoter-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install120_21.yaml:65-96
────────────────────────────────────────
  65 ┌       - args:
  66 │         - --health-probe-bind-address=:8081
  67 │         - --metrics-bind-address=127.0.0.1:8080
  68 │         - --leader-elect
  69 │         command:
  70 │         - /gitops-promoter
  71 │         image: quay.io/argoprojlabs/gitops-promoter:latest
  72 │         livenessProbe:
  73 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'promoter-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install120_21.yaml:42-64
────────────────────────────────────────
  42 ┌       - args:
  43 │         - --secure-listen-address=0.0.0.0:8443
  44 │         - --upstream=http://127.0.0.1:8080/
  45 │         - --logtostderr=true
  46 │         - --v=0
  47 │         image: quay.io/brancz/kube-rbac-proxy:v0.17.0
  48 │         name: kube-rbac-proxy
  49 │         ports:
  50 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'promoter-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install120_21.yaml:65-96
────────────────────────────────────────
  65 ┌       - args:
  66 │         - --health-probe-bind-address=:8081
  67 │         - --metrics-bind-address=127.0.0.1:8080
  68 │         - --leader-elect
  69 │         command:
  70 │         - /gitops-promoter
  71 │         image: quay.io/argoprojlabs/gitops-promoter:latest
  72 │         livenessProbe:
  73 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kube-rbac-proxy in deployment promoter-controller-manager (namespace: promoter-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install120_21.yaml:42-64
────────────────────────────────────────
  42 ┌       - args:
  43 │         - --secure-listen-address=0.0.0.0:8443
  44 │         - --upstream=http://127.0.0.1:8080/
  45 │         - --logtostderr=true
  46 │         - --v=0
  47 │         image: quay.io/brancz/kube-rbac-proxy:v0.17.0
  48 │         name: kube-rbac-proxy
  49 │         ports:
  50 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment promoter-controller-manager (namespace: promoter-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install120_21.yaml:65-96
────────────────────────────────────────
  65 ┌       - args:
  66 │         - --health-probe-bind-address=:8081
  67 │         - --metrics-bind-address=127.0.0.1:8080
  68 │         - --leader-elect
  69 │         command:
  70 │         - /gitops-promoter
  71 │         image: quay.io/argoprojlabs/gitops-promoter:latest
  72 │         livenessProbe:
  73 └           httpGet:
  ..   
────────────────────────────────────────



install121_14.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'pvc-autoscaler-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'pvc-autoscaler-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install121_14.yaml:69-91
────────────────────────────────────────
  69 ┌       - args:
  70 │         - --secure-listen-address=0.0.0.0:8443
  71 │         - --upstream=http://127.0.0.1:8080/
  72 │         - --logtostderr=true
  73 │         - --v=0
  74 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  75 │         name: kube-rbac-proxy
  76 │         ports:
  77 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'pvc-autoscaler-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'pvc-autoscaler-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install121_14.yaml:69-91
────────────────────────────────────────
  69 ┌       - args:
  70 │         - --secure-listen-address=0.0.0.0:8443
  71 │         - --upstream=http://127.0.0.1:8080/
  72 │         - --logtostderr=true
  73 │         - --v=0
  74 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  75 │         name: kube-rbac-proxy
  76 │         ports:
  77 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'pvc-autoscaler-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'pvc-autoscaler-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install121_14.yaml:69-91
────────────────────────────────────────
  69 ┌       - args:
  70 │         - --secure-listen-address=0.0.0.0:8443
  71 │         - --upstream=http://127.0.0.1:8080/
  72 │         - --logtostderr=true
  73 │         - --v=0
  74 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  75 │         name: kube-rbac-proxy
  76 │         ports:
  77 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'pvc-autoscaler-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install121_14.yaml:69-91
────────────────────────────────────────
  69 ┌       - args:
  70 │         - --secure-listen-address=0.0.0.0:8443
  71 │         - --upstream=http://127.0.0.1:8080/
  72 │         - --logtostderr=true
  73 │         - --v=0
  74 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  75 │         name: kube-rbac-proxy
  76 │         ports:
  77 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "pvc-autoscaler-controller-manager" in "pvc-autoscaler-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install121_14.yaml:69-91
────────────────────────────────────────
  69 ┌       - args:
  70 │         - --secure-listen-address=0.0.0.0:8443
  71 │         - --upstream=http://127.0.0.1:8080/
  72 │         - --logtostderr=true
  73 │         - --v=0
  74 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  75 │         name: kube-rbac-proxy
  76 │         ports:
  77 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "pvc-autoscaler-controller-manager" in "pvc-autoscaler-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment pvc-autoscaler-controller-manager (namespace: pvc-autoscaler-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install121_14.yaml:27-68
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --health-probe-bind-address=:8081
  29 │         - --metrics-bind-address=127.0.0.1:8080
  30 │         - --leader-elect
  31 │         - --interval=30s
  32 │         - --prometheus-address=http://prometheus-k8s.monitoring.svc.cluster.local:9090
  33 │         command:
  34 │         - /manager
  35 └         image: europe-docker.pkg.dev/gardener-project/releases/gardener/pvc-autoscaler:latest
  ..   
────────────────────────────────────────



install121_3.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'pvc-autoscaler-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install121_3.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install122_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install122_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install122_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install122_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install122_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install122_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install122_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install122_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install122_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install122_14.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install122_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install122_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install122_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install122_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install122_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install122_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install122_16.yaml:60-67
────────────────────────────────────────
  60 ┌ - apiGroups:
  61 │   - ''
  62 │   resources:
  63 │   - secrets
  64 │   verbs:
  65 │   - get
  66 │   - list
  67 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install122_16.yaml:48-59
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - configmaps
  52 │   verbs:
  53 │   - create
  54 │   - update
  55 │   - delete
  56 └   - get
  ..   
────────────────────────────────────────



install122_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install122_17.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install122_17.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install122_41.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 104, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 13, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'copyutil' of Deployment 'argocd-dex-server' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install122_41.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install122_41.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install122_41.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install122_41.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install122_42.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install122_42.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install122_42.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install122_43.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install122_43.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install122_43.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install122_43.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install122_43.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install122_43.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install122_43.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install122_43.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install126_3.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 95, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 8, MEDIUM: 7, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kube-network-policies' of 'daemonset' 'kube-network-policies' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'kube-network-policies' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 install126_3.yaml:11-58
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: kube-network-policies
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: kube-network-policies
  19 └         k8s-app: kube-network-policies
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'kube-network-policies' of DaemonSet 'kube-network-policies' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'kube-network-policies' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 install126_3.yaml:11-58
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: kube-network-policies
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: kube-network-policies
  19 └         k8s-app: kube-network-policies
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'kube-network-policies' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 install126_3.yaml:11-58
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: kube-network-policies
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: kube-network-policies
  19 └         k8s-app: kube-network-policies
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-network-policies" of daemonset "kube-network-policies" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset kube-network-policies in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install126_3.yaml:21-58
────────────────────────────────────────
  21 ┌       hostNetwork: true
  22 │       dnsPolicy: ClusterFirst
  23 │       nodeSelector:
  24 │         kubernetes.io/os: linux
  25 │       tolerations:
  26 │       - operator: Exists
  27 │         effect: NoSchedule
  28 │       serviceAccountName: kube-network-policies
  29 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kube-network-policies in daemonset kube-network-policies (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install126_3.yaml:30-54
────────────────────────────────────────
  30 ┌       - name: kube-network-policies
  31 │         image: registry.k8s.io/networking/kube-network-policies:v0.7.0
  32 │         args:
  33 │         - /bin/netpol
  34 │         - --hostname-override=$(MY_NODE_NAME)
  35 │         - --v=2
  36 │         - --nfqueue-id=98
  37 │         volumeMounts:
  38 └         - name: lib-modules
  ..   
────────────────────────────────────────



install127_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install127_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install127_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install127_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install127_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install127_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install127_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install127_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install127_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install127_14.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install127_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install127_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install127_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install127_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install127_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install127_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install127_16.yaml:60-67
────────────────────────────────────────
  60 ┌ - apiGroups:
  61 │   - ''
  62 │   resources:
  63 │   - secrets
  64 │   verbs:
  65 │   - get
  66 │   - list
  67 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install127_16.yaml:48-59
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - configmaps
  52 │   verbs:
  53 │   - create
  54 │   - update
  55 │   - delete
  56 └   - get
  ..   
────────────────────────────────────────



install127_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 111, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install127_17.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install127_17.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────


AVD-KSV-0053 (HIGH): ClusterRole 'argocd-server' should not have access to resource '["pods/exec"]' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to exec into a container with privileged access to the host or with an attached SA with higher RBAC permissions is a common escalation path to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv053
────────────────────────────────────────
 install127_17.yaml:52-57
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods/exec
  56 │   verbs:
  57 └   - create
────────────────────────────────────────



install127_44.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 105, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 13, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install127_44.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install127_44.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install127_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.14.5
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install127_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install127_45.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install127_45.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install127_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install127_46.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install127_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install127_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install127_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install127_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install127_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install127_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install127_46.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install128_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install128_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install128_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install128_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install128_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install128_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install128_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install128_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install128_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install128_14.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install128_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install128_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install128_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install128_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install128_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install128_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install128_16.yaml:60-67
────────────────────────────────────────
  60 ┌ - apiGroups:
  61 │   - ''
  62 │   resources:
  63 │   - secrets
  64 │   verbs:
  65 │   - get
  66 │   - list
  67 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install128_16.yaml:48-59
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - configmaps
  52 │   verbs:
  53 │   - create
  54 │   - update
  55 │   - delete
  56 └   - get
  ..   
────────────────────────────────────────



install128_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install128_17.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install128_17.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install128_44.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 104, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 13, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'copyutil' of Deployment 'argocd-dex-server' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install128_44.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install128_44.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install128_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install128_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install128_45.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install128_45.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install128_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install128_46.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install128_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install128_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install128_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install128_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install128_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install128_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.15-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install128_46.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install129_10.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'route53pilot-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'route53pilot-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install129_10.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'route53pilot-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'route53pilot-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install129_10.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'route53pilot-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'route53pilot-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install129_10.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'route53pilot-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install129_10.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "route53pilot-controller-manager" in "route53pilot-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install129_10.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "route53pilot-controller-manager" in "route53pilot-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment route53pilot-controller-manager (namespace: route53pilot-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install129_10.yaml:50-82
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: arulprasath/route53pilot:latest
  57 │         imagePullPolicy: Always
  58 └         livenessProbe:
  ..   
────────────────────────────────────────



install129_2.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'route53pilot-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install129_2.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install129_3.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'route53pilot-manager-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 install129_3.yaml:6-17
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - networking.k8s.io
   8 │   resources:
   9 │   - ingresses
  10 │   verbs:
  11 │   - create
  12 │   - delete
  13 │   - get
  14 └   - list
  ..   
────────────────────────────────────────



install130_1.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 112, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'disk-auto-scaler-cr' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install130_1.yaml:6-20
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - pods
  10 │   - pods/exec
  11 │   - persistentvolumes
  12 │   - persistentvolumeclaims
  13 │   verbs:
  14 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'disk-auto-scaler-cr' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install130_1.yaml:21-30
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - apps
  23 │   resources:
  24 │   - deployments
  25 │   - deployments/scale
  26 │   verbs:
  27 │   - get
  28 │   - list
  29 │   - update
  30 └   - patch
────────────────────────────────────────


AVD-KSV-0053 (HIGH): ClusterRole 'disk-auto-scaler-cr' should not have access to resource '["pods/exec"]' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to exec into a container with privileged access to the host or with an attached SA with higher RBAC permissions is a common escalation path to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv053
────────────────────────────────────────
 install130_1.yaml:6-20
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - pods
  10 │   - pods/exec
  11 │   - persistentvolumes
  12 │   - persistentvolumeclaims
  13 │   verbs:
  14 └   - get
  ..   
────────────────────────────────────────



install130_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 108, FAILURES: 6)
Failures: 6 (UNKNOWN: 0, LOW: 4, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'disk-autoscaler' of Deployment 'disk-autoscaler' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install130_4.yaml:16-40
────────────────────────────────────────
  16 ┌       - env:
  17 │         - name: DAS_LOG_LEVEL
  18 │           value: info
  19 │         - name: DAS_COST_MODEL_PATH
  20 │           value: http://kubecost-cost-analyzer.kubecost:9090/model
  21 │         - name: DAS_EXCLUDE_NAMESPACES
  22 │           value: kubecost,kube-*,openshift-*
  23 │         - name: DAS_AUDIT_MODE
  24 └           value: 'true'
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'disk-autoscaler' of Deployment 'disk-autoscaler' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install130_4.yaml:16-40
────────────────────────────────────────
  16 ┌       - env:
  17 │         - name: DAS_LOG_LEVEL
  18 │           value: info
  19 │         - name: DAS_COST_MODEL_PATH
  20 │           value: http://kubecost-cost-analyzer.kubecost:9090/model
  21 │         - name: DAS_EXCLUDE_NAMESPACES
  22 │           value: kubecost,kube-*,openshift-*
  23 │         - name: DAS_AUDIT_MODE
  24 └           value: 'true'
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'disk-autoscaler' of Deployment 'disk-autoscaler' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install130_4.yaml:16-40
────────────────────────────────────────
  16 ┌       - env:
  17 │         - name: DAS_LOG_LEVEL
  18 │           value: info
  19 │         - name: DAS_COST_MODEL_PATH
  20 │           value: http://kubecost-cost-analyzer.kubecost:9090/model
  21 │         - name: DAS_EXCLUDE_NAMESPACES
  22 │           value: kubecost,kube-*,openshift-*
  23 │         - name: DAS_AUDIT_MODE
  24 └           value: 'true'
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'disk-autoscaler' of Deployment 'disk-autoscaler' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install130_4.yaml:16-40
────────────────────────────────────────
  16 ┌       - env:
  17 │         - name: DAS_LOG_LEVEL
  18 │           value: info
  19 │         - name: DAS_COST_MODEL_PATH
  20 │           value: http://kubecost-cost-analyzer.kubecost:9090/model
  21 │         - name: DAS_EXCLUDE_NAMESPACES
  22 │           value: kubecost,kube-*,openshift-*
  23 │         - name: DAS_AUDIT_MODE
  24 └           value: 'true'
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'disk-autoscaler' of Deployment 'disk-autoscaler' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install130_4.yaml:16-40
────────────────────────────────────────
  16 ┌       - env:
  17 │         - name: DAS_LOG_LEVEL
  18 │           value: info
  19 │         - name: DAS_COST_MODEL_PATH
  20 │           value: http://kubecost-cost-analyzer.kubecost:9090/model
  21 │         - name: DAS_EXCLUDE_NAMESPACES
  22 │           value: kubecost,kube-*,openshift-*
  23 │         - name: DAS_AUDIT_MODE
  24 └           value: 'true'
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'disk-autoscaler' of Deployment 'disk-autoscaler' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install130_4.yaml:16-40
────────────────────────────────────────
  16 ┌       - env:
  17 │         - name: DAS_LOG_LEVEL
  18 │           value: info
  19 │         - name: DAS_COST_MODEL_PATH
  20 │           value: http://kubecost-cost-analyzer.kubecost:9090/model
  21 │         - name: DAS_EXCLUDE_NAMESPACES
  22 │           value: kubecost,kube-*,openshift-*
  23 │         - name: DAS_AUDIT_MODE
  24 └           value: 'true'
  ..   
────────────────────────────────────────



install131_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install131_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_11.yaml:50-58
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   - configmaps
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



install131_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install131_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install131_14.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-redis' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_14.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resourceNames:
  13 │   - argocd-redis
  14 │   resources:
  15 │   - secrets
  16 │   verbs:
  17 └   - get
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-redis' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_14.yaml:18-23
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - secrets
  22 │   verbs:
  23 └   - create
────────────────────────────────────────



install131_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install131_15.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install131_15.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install131_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install131_16.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install131_16.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install131_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install131_17.yaml:62-69
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - get
  68 │   - list
  69 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install131_17.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - configmaps
  54 │   verbs:
  55 │   - create
  56 │   - update
  57 │   - delete
  58 └   - get
  ..   
────────────────────────────────────────



install131_18.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install131_18.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install131_18.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install131_46.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 105, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 13, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install131_46.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install131_46.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install131_46.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.12.1
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install131_46.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install131_47.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install131_47.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install131_47.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install131_48.yaml (kubernetes)
===============================
Tests: 120 (SUCCESSES: 106, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 13, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install131_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install131_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install131_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install131_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install131_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install131_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install131_48.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container secret-init in deployment argocd-redis (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install131_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.12.1
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────



install132_12.yaml (kubernetes)
===============================
Tests: 119 (SUCCESSES: 107, FAILURES: 12)
Failures: 12 (UNKNOWN: 0, LOW: 6, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'repurika-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'repurika-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install132_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'repurika-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'repurika-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install132_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'repurika-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'repurika-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install132_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'repurika-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install132_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "repurika-controller-manager" in "repurika-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install132_12.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "repurika-controller-manager" in "repurika-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment repurika-controller-manager (namespace: repurika-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install132_12.yaml:50-81
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: docker.io/bonavadeur/repurika:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install132_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'repurika-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install132_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install132_5.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'repurika-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install132_5.yaml:32-43
────────────────────────────────────────
  32 ┌ - apiGroups:
  33 │   - ''
  34 │   resources:
  35 │   - pods
  36 │   verbs:
  37 │   - create
  38 │   - delete
  39 │   - get
  40 └   - list
  ..   
────────────────────────────────────────



install133_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install133_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install133_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install133_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install133_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install133_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install133_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install133_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install133_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install133_14.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install133_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install133_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install133_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install133_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install133_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install133_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install133_16.yaml:60-67
────────────────────────────────────────
  60 ┌ - apiGroups:
  61 │   - ''
  62 │   resources:
  63 │   - secrets
  64 │   verbs:
  65 │   - get
  66 │   - list
  67 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install133_16.yaml:48-59
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - configmaps
  52 │   verbs:
  53 │   - create
  54 │   - update
  55 │   - delete
  56 └   - get
  ..   
────────────────────────────────────────



install133_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install133_17.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install133_17.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install133_44.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 105, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 13, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install133_44.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install133_44.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install133_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.10.6
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install133_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install133_45.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install133_45.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install133_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install133_46.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install133_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install133_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install133_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install133_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install133_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install133_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install133_46.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install135_14.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'ceph-csi-operator-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install135_14.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



install135_19.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ceph-csi-operator-cephfs-ctrlplugin-cr' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install135_19.yaml:6-12
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   verbs:
  11 │   - get
  12 └   - list
────────────────────────────────────────



install135_20.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ceph-csi-operator-cephfs-nodeplugin-cr' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install135_20.yaml:12-17
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - secrets
  16 │   verbs:
  17 └   - get
────────────────────────────────────────



install135_27.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 111, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'ceph-csi-operator-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install135_27.yaml:30-41
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - apps
  32 │   resources:
  33 │   - daemonsets
  34 │   verbs:
  35 │   - create
  36 │   - delete
  37 │   - get
  38 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'ceph-csi-operator-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install135_27.yaml:42-53
────────────────────────────────────────
  42 ┌ - apiGroups:
  43 │   - apps
  44 │   resources:
  45 │   - deployments
  46 │   verbs:
  47 │   - create
  48 │   - delete
  49 │   - get
  50 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'ceph-csi-operator-manager-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install135_27.yaml:6-17
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - configmaps
  10 │   verbs:
  11 │   - create
  12 │   - delete
  13 │   - get
  14 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'ceph-csi-operator-manager-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 install135_27.yaml:18-29
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - services
  22 │   verbs:
  23 │   - create
  24 │   - delete
  25 │   - get
  26 └   - list
  ..   
────────────────────────────────────────



install135_30.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ceph-csi-operator-nfs-ctrlplugin-cr' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install135_30.yaml:74-79
────────────────────────────────────────
  74 ┌ - apiGroups:
  75 │   - ''
  76 │   resources:
  77 │   - secrets
  78 │   verbs:
  79 └   - get
────────────────────────────────────────



install135_34.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ceph-csi-operator-rbd-ctrlplugin-cr' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install135_34.yaml:6-13
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   verbs:
  11 │   - get
  12 │   - list
  13 └   - watch
────────────────────────────────────────



install135_35.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ceph-csi-operator-rbd-nodeplugin-cr' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install135_35.yaml:6-12
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   verbs:
  11 │   - get
  12 └   - list
────────────────────────────────────────



install135_48.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 108, FAILURES: 6)
Failures: 6 (UNKNOWN: 0, LOW: 3, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'ceph-csi-operator-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install135_48.yaml:23-62
────────────────────────────────────────
  23 ┌       - args:
  24 │         - --leader-elect
  25 │         command:
  26 │         - /manager
  27 │         env:
  28 │         - name: OPERATOR_NAMESPACE
  29 │           valueFrom:
  30 │             fieldRef:
  31 └               fieldPath: metadata.namespace
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'ceph-csi-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install135_48.yaml:23-62
────────────────────────────────────────
  23 ┌       - args:
  24 │         - --leader-elect
  25 │         command:
  26 │         - /manager
  27 │         env:
  28 │         - name: OPERATOR_NAMESPACE
  29 │           valueFrom:
  30 │             fieldRef:
  31 └               fieldPath: metadata.namespace
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'ceph-csi-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install135_48.yaml:23-62
────────────────────────────────────────
  23 ┌       - args:
  24 │         - --leader-elect
  25 │         command:
  26 │         - /manager
  27 │         env:
  28 │         - name: OPERATOR_NAMESPACE
  29 │           valueFrom:
  30 │             fieldRef:
  31 └               fieldPath: metadata.namespace
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install135_48.yaml:23-62
────────────────────────────────────────
  23 ┌       - args:
  24 │         - --leader-elect
  25 │         command:
  26 │         - /manager
  27 │         env:
  28 │         - name: OPERATOR_NAMESPACE
  29 │           valueFrom:
  30 │             fieldRef:
  31 └               fieldPath: metadata.namespace
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "ceph-csi-operator-controller-manager" in "ceph-csi-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install135_48.yaml:23-62
────────────────────────────────────────
  23 ┌       - args:
  24 │         - --leader-elect
  25 │         command:
  26 │         - /manager
  27 │         env:
  28 │         - name: OPERATOR_NAMESPACE
  29 │           valueFrom:
  30 │             fieldRef:
  31 └               fieldPath: metadata.namespace
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment ceph-csi-operator-controller-manager (namespace: ceph-csi-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install135_48.yaml:23-62
────────────────────────────────────────
  23 ┌       - args:
  24 │         - --leader-elect
  25 │         command:
  26 │         - /manager
  27 │         env:
  28 │         - name: OPERATOR_NAMESPACE
  29 │           valueFrom:
  30 │             fieldRef:
  31 └               fieldPath: metadata.namespace
  ..   
────────────────────────────────────────



install137.yaml (kubernetes)
============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'network-driver' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install137.yaml:12-17
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - resource.k8s.io
  14 │   resources:
  15 │   - '*'
  16 │   verbs:
  17 └   - '*'
────────────────────────────────────────



install137_3.yaml (kubernetes)
==============================
Tests: 124 (SUCCESSES: 90, FAILURES: 34)
Failures: 34 (UNKNOWN: 0, LOW: 16, MEDIUM: 10, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'enable-nri' of DaemonSet 'network-driver' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'network-driver' of DaemonSet 'network-driver' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'network-driver' of DaemonSet 'network-driver' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'enable-nri' of 'daemonset' 'network-driver' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'network-driver' of 'daemonset' 'network-driver' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0005 (HIGH): Container 'network-driver' of DaemonSet 'network-driver' should not include 'SYS_ADMIN' in 'securityContext.capabilities.add'
════════════════════════════════════════
SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root.

See https://avd.aquasec.com/misconfig/ksv005
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'network-driver' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 install137_3.yaml:11-92
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: network-driver
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: network-driver
  19 └         k8s-app: network-driver
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'network-driver' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 install137_3.yaml:11-92
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: network-driver
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: network-driver
  19 └         k8s-app: network-driver
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'enable-nri' of DaemonSet 'network-driver' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'network-driver' of DaemonSet 'network-driver' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'enable-nri' of DaemonSet 'network-driver' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'network-driver' of DaemonSet 'network-driver' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'enable-nri' of DaemonSet 'network-driver' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'network-driver' of DaemonSet 'network-driver' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'enable-nri' of DaemonSet 'network-driver' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'network-driver' of DaemonSet 'network-driver' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'network-driver' of DaemonSet 'network-driver' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'network-driver' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 install137_3.yaml:11-92
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: network-driver
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: network-driver
  19 └         k8s-app: network-driver
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'network-driver' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 install137_3.yaml:11-92
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: network-driver
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: network-driver
  19 └         k8s-app: network-driver
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "enable-nri" of daemonset "network-driver" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "network-driver" of daemonset "network-driver" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install137_3.yaml:29-39
────────────────────────────────────────
  29 ┌       - name: enable-nri
  30 │         image: busybox:stable
  31 │         volumeMounts:
  32 │         - mountPath: /etc
  33 │           name: etc
  34 │         securityContext:
  35 │           privileged: true
  36 │         command:
  37 └         - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset network-driver in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install137_3.yaml:21-92
────────────────────────────────────────
  21 ┌       hostNetwork: true
  22 │       hostPID: true
  23 │       hostIPC: false
  24 │       tolerations:
  25 │       - operator: Exists
  26 │         effect: NoSchedule
  27 │       serviceAccountName: network-driver
  28 │       initContainers:
  29 └       - name: enable-nri
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset network-driver in kube-system namespace shouldn't have volumes set to {"/etc"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 install137_3.yaml:11-92
────────────────────────────────────────
  11 ┌   selector:
  12 │     matchLabels:
  13 │       app: network-driver
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 │         tier: node
  18 │         app: network-driver
  19 └         k8s-app: network-driver
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container network-driver in daemonset network-driver (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install137_3.yaml:50-76
────────────────────────────────────────
  50 ┌       - name: network-driver
  51 │         args:
  52 │         - /driver
  53 │         - --v=4
  54 │         image: aojea/kube-network-driver:stable
  55 │         resources:
  56 │           requests:
  57 │             cpu: 100m
  58 └             memory: 50Mi
  ..   
────────────────────────────────────────



install138_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install138_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install138_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install138_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install138_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install138_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install138_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install138_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install138_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install138_14.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install138_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install138_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install138_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install138_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install138_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install138_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install138_16.yaml:60-67
────────────────────────────────────────
  60 ┌ - apiGroups:
  61 │   - ''
  62 │   resources:
  63 │   - secrets
  64 │   verbs:
  65 │   - get
  66 │   - list
  67 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install138_16.yaml:48-59
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - configmaps
  52 │   verbs:
  53 │   - create
  54 │   - update
  55 │   - delete
  56 └   - get
  ..   
────────────────────────────────────────



install138_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install138_17.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install138_17.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install138_44.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 104, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 13, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'copyutil' of Deployment 'argocd-dex-server' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install138_44.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install138_44.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install138_44.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install138_44.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install138_45.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install138_45.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install138_45.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install138_46.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install138_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install138_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install138_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install138_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install138_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install138_46.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install138_46.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install13_10.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install13_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install13_11.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install13_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install13_12.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install13_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install13_13.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install13_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install13_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install13_14.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install13_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install13_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install13_15.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install13_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install13_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install13_16.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install13_16.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install13_16.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install13_42.yaml (kubernetes)
==============================
Tests: 121 (SUCCESSES: 104, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 13, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'copyutil' of Deployment 'argocd-dex-server' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install13_42.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install13_42.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install13_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:latest
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install13_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install13_43.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install13_43.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install13_43.yaml:21-65
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install13_44.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install13_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install13_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install13_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install13_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install13_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install13_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.14-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install13_44.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install141_10.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 6, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'manager' of Deployment 'capsule-controller-manager' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'manager' of Deployment 'capsule-controller-manager' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'manager' of 'deployment' 'capsule-controller-manager' in 'capsule-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'manager' of Deployment 'capsule-controller-manager' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'capsule-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'capsule-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'capsule-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'capsule-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "capsule-controller-manager" in "capsule-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container capsule-controller-manager in capsule-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment capsule-controller-manager in capsule-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install141_10.yaml:18-55
────────────────────────────────────────
  18 ┌       containers:
  19 │       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment capsule-controller-manager (namespace: capsule-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install141_10.yaml:19-49
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --enable-leader-election
  21 │         - --zap-encoder=console
  22 │         - --zap-log-level=debug
  23 │         - --configuration-name=capsule-default
  24 │         env:
  25 │         - name: NAMESPACE
  26 │           valueFrom:
  27 └             fieldRef:
  ..   
────────────────────────────────────────



install141_5.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'capsule-manager-rolebinding' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 install141_5.yaml:4
────────────────────────────────────────
   4 [   name: capsule-manager-rolebinding
────────────────────────────────────────



install143_2.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'login-protector-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install143_2.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



install143_3.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'login-protector-manager-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install143_3.yaml:6-14
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - pods
  10 │   verbs:
  11 │   - get
  12 │   - list
  13 │   - update
  14 └   - watch
────────────────────────────────────────



install143_6.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 5, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'manager' of Deployment 'login-protector-controller-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'login-protector-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'manager' of Deployment 'login-protector-controller-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'login-protector-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'login-protector-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "login-protector-controller-manager" in "login-protector-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment login-protector-controller-manager (namespace: login-protector-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install143_6.yaml:24-58
────────────────────────────────────────
  24 ┌       - args:
  25 │         - --leader-elect
  26 │         command:
  27 │         - /login-protector
  28 │         image: ghcr.io/cybozu-go/login-protector:0.2.0
  29 │         imagePullPolicy: IfNotPresent
  30 │         livenessProbe:
  31 │           httpGet:
  32 └             path: /healthz
  ..   
────────────────────────────────────────



install144_13.yaml (kubernetes)
===============================
Tests: 128 (SUCCESSES: 98, FAILURES: 30)
Failures: 30 (UNKNOWN: 0, LOW: 16, MEDIUM: 11, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-init' of 'deployment' 'attestation-operator-controller-manager' in 'attestation-operator-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kube-rbac-proxy' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install144_13.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'manager' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'manager' of Deployment 'attestation-operator-controller-manager' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kube-rbac-proxy' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install144_13.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'manager' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kube-rbac-proxy' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install144_13.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'manager' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-init' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kube-rbac-proxy' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install144_13.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'manager' of Deployment 'attestation-operator-controller-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install144_13.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-init" of deployment "attestation-operator-controller-manager" in "attestation-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kube-rbac-proxy" of deployment "attestation-operator-controller-manager" in "attestation-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install144_13.yaml:27-49
────────────────────────────────────────
  27 ┌       - args:
  28 │         - --secure-listen-address=0.0.0.0:8443
  29 │         - --upstream=http://127.0.0.1:8080/
  30 │         - --logtostderr=true
  31 │         - --v=0
  32 │         image: gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
  33 │         name: kube-rbac-proxy
  34 │         ports:
  35 └         - containerPort: 8443
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "manager" of deployment "attestation-operator-controller-manager" in "attestation-operator-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-init in deployment attestation-operator-controller-manager (namespace: attestation-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install144_13.yaml:89-115
────────────────────────────────────────
  89 ┌       - command:
  90 │         - /bin/bash
  91 │         - -c
  92 │         - "KUBECTL_PREFIX=\"\"\ncommand -v kubectl\nif [[ $? -ne 0 ]]\nthen\n    pushd\
  93 │           \ /tmp\n    curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\n\
  94 │           \    chmod +x /tmp/kubectl\n    KUBECTL_PREFIX=\"/tmp/\"\n    popd\nfi\n\
  95 │           \nls -l /var/lib/controller/\n\n# now generate the CV CA\nopenssl req -nodes\
  96 │           \ -newkey rsa:2048 -keyout /var/lib/controller/certs/csr.key -out csr.csr\
  97 └           \ -subj \"/O=discover.idlab\"\n\nls -l /var/lib/controller/certs/\n\n${KUBECTL_PREFIX}kubectl\
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container manager in deployment attestation-operator-controller-manager (namespace: attestation-operator-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install144_13.yaml:50-85
────────────────────────────────────────
  50 ┌       - args:
  51 │         - --health-probe-bind-address=:8081
  52 │         - --metrics-bind-address=127.0.0.1:8080
  53 │         - --leader-elect
  54 │         command:
  55 │         - /manager
  56 │         image: gitlab.ilabt.imec.be:4567/edge-keylime/attestation-operator:latest
  57 │         livenessProbe:
  58 └           httpGet:
  ..   
────────────────────────────────────────



install144_4.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'attestation-operator-leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install144_4.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



install144_5.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'attestation-operator-manager-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install144_5.yaml:23-33
────────────────────────────────────────
  23 ┌ - apiGroups:
  24 │   - ''
  25 │   resources:
  26 │   - secrets
  27 │   - serviceaccounts
  28 │   - services
  29 │   verbs:
  30 │   - create
  31 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'attestation-operator-manager-role' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 install144_5.yaml:23-33
────────────────────────────────────────
  23 ┌ - apiGroups:
  24 │   - ''
  25 │   resources:
  26 │   - secrets
  27 │   - serviceaccounts
  28 │   - services
  29 │   verbs:
  30 │   - create
  31 └   - get
  ..   
────────────────────────────────────────



install145_10.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install145_11.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_11.yaml:50-58
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   - configmaps
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



install145_12.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install145_13.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install145_14.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-redis' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_14.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resourceNames:
  13 │   - argocd-redis
  14 │   resources:
  15 │   - secrets
  16 │   verbs:
  17 └   - get
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-redis' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_14.yaml:18-23
────────────────────────────────────────
  18 ┌ - apiGroups:
  19 │   - ''
  20 │   resources:
  21 │   - secrets
  22 │   verbs:
  23 └   - create
────────────────────────────────────────



install145_15.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install145_15.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install145_15.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install145_16.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install145_16.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install145_16.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install145_17.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argocd-applicationset-controller' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install145_17.yaml:62-69
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - ''
  64 │   resources:
  65 │   - secrets
  66 │   verbs:
  67 │   - get
  68 │   - list
  69 └   - watch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): ClusterRole 'argocd-applicationset-controller' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install145_17.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - configmaps
  54 │   verbs:
  55 │   - create
  56 │   - update
  57 │   - delete
  58 └   - get
  ..   
────────────────────────────────────────



install145_18.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install145_18.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install145_18.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install145_46.yaml (kubernetes)
===============================
Tests: 121 (SUCCESSES: 105, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 13, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install145_46.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install145_46.yaml:18-114
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install145_46.yaml:74-95
────────────────────────────────────────
  74 ┌       - command:
  75 │         - /bin/cp
  76 │         - -n
  77 │         - /usr/local/bin/argocd
  78 │         - /shared/argocd-dex
  79 │         image: quay.io/argoproj/argocd:v2.14.3
  80 │         imagePullPolicy: Always
  81 │         name: copyutil
  82 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install145_46.yaml:28-72
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_LOGFORMAT
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.log.format
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install145_47.yaml (kubernetes)
===============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install145_47.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install145_47.yaml:21-71
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install145_48.yaml (kubernetes)
===============================
Tests: 120 (SUCCESSES: 106, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 13, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install145_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install145_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install145_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install145_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install145_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install145_48.yaml:34-56
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         - --requirepass $(REDIS_PASSWORD)
  40 │         env:
  41 │         - name: REDIS_PASSWORD
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'secret-init' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install145_48.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container secret-init in deployment argocd-redis (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install145_48.yaml:58-73
────────────────────────────────────────
  58 ┌       - command:
  59 │         - argocd
  60 │         - admin
  61 │         - redis-initial-password
  62 │         image: quay.io/argoproj/argocd:v2.14.3
  63 │         imagePullPolicy: IfNotPresent
  64 │         name: secret-init
  65 │         securityContext:
  66 └           allowPrivilegeEscalation: false
  ..   
────────────────────────────────────────



install14_10.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install14_10.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install14_11.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install14_11.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install14_12.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install14_12.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install14_13.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install14_13.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install14_13.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install14_14.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install14_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install14_14.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install14_15.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install14_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install14_15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install14_16.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install14_16.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install14_16.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install14_42.yaml (kubernetes)
==============================
Tests: 121 (SUCCESSES: 105, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 13, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install14_42.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install14_42.yaml:18-100
────────────────────────────────────────
  18 ┌       affinity:
  19 │         podAntiAffinity:
  20 │           preferredDuringSchedulingIgnoredDuringExecution:
  21 │           - podAffinityTerm:
  22 │               labelSelector:
  23 │                 matchLabels:
  24 │                   app.kubernetes.io/part-of: argocd
  25 │               topologyKey: kubernetes.io/hostname
  26 └             weight: 5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install14_42.yaml:62-83
────────────────────────────────────────
  62 ┌       - command:
  63 │         - /bin/cp
  64 │         - -n
  65 │         - /usr/local/bin/argocd
  66 │         - /shared/argocd-dex
  67 │         image: quay.io/argoproj/argocd:v2.9.3
  68 │         imagePullPolicy: Always
  69 │         name: copyutil
  70 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install14_42.yaml:28-60
────────────────────────────────────────
  28 ┌       - command:
  29 │         - /shared/argocd-dex
  30 │         - rundex
  31 │         env:
  32 │         - name: ARGOCD_DEX_SERVER_DISABLE_TLS
  33 │           valueFrom:
  34 │             configMapKeyRef:
  35 │               key: dexserver.disable.tls
  36 └               name: argocd-cmd-params-cm
  ..   
────────────────────────────────────────



install14_43.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install14_43.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install14_43.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install14_44.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install14_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.11-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install14_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.11-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install14_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.11-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install14_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.11-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install14_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.11-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install14_44.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.0.11-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install14_44.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install17_10.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install17_10.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install17_11.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install17_11.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install17_11.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install17_12.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install17_12.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install17_12.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install17_13.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install17_13.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install17_13.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install17_14.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install17_14.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install17_14.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────



install17_34.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 106, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-notifications-controller' of Deployment 'argocd-notifications-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-notifications-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install17_34.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: notifications-controller
   6 │     app.kubernetes.io/name: argocd-notifications-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-notifications-controller
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-notifications-controller in deployment argocd-notifications-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install17_34.yaml:21-59
────────────────────────────────────────
  21 ┌       - args:
  22 │         - /usr/local/bin/argocd-notifications
  23 │         env:
  24 │         - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT
  25 │           valueFrom:
  26 │             configMapKeyRef:
  27 │               key: notificationscontroller.log.format
  28 │               name: argocd-cmd-params-cm
  29 └               optional: true
  ..   
────────────────────────────────────────



install17_35.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 107, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 7, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install17_35.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.4.2-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install17_35.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.4.2-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install17_35.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.4.2-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install17_35.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.4.2-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install17_35.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.4.2-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install17_35.yaml:34-49
────────────────────────────────────────
  34 ┌       - args:
  35 │         - --save
  36 │         - ''
  37 │         - --appendonly
  38 │         - 'no'
  39 │         image: redis:7.4.2-alpine
  40 │         imagePullPolicy: Always
  41 │         name: redis
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install17_35.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────



install17_9.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install17_9.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install18_17.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'argocd-ssh-known-hosts-cm' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



install18_26.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'argocd-application-controller' of 'deployment' 'argocd-application-controller' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-application-controller' of Deployment 'argocd-application-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "argocd-application-controller" of deployment "argocd-application-controller" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-application-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install18_26.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: application-controller
   6 │     app.kubernetes.io/name: argocd-application-controller
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-application-controller
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container argocd-application-controller in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-application-controller in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_26.yaml:20-44
────────────────────────────────────────
  20 ┌       containers:
  21 │       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 └         imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-application-controller in deployment argocd-application-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install18_26.yaml:21-43
────────────────────────────────────────
  21 ┌       - command:
  22 │         - argocd-application-controller
  23 │         - --status-processors
  24 │         - '20'
  25 │         - --operation-processors
  26 │         - '10'
  27 │         image: argoproj/argocd:v1.2.4
  28 │         imagePullPolicy: Always
  29 └         livenessProbe:
  ..   
────────────────────────────────────────



install18_27.yaml (kubernetes)
==============================
Tests: 131 (SUCCESSES: 97, FAILURES: 34)
Failures: 34 (UNKNOWN: 0, LOW: 21, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'copyutil' of 'deployment' 'argocd-dex-server' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'dex' of 'deployment' 'argocd-dex-server' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'copyutil' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'dex' of Deployment 'argocd-dex-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "copyutil" of deployment "argocd-dex-server" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "dex" of deployment "argocd-dex-server" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-dex-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install18_27.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: dex-server
   6 │     app.kubernetes.io/name: argocd-dex-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-dex-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container argocd-dex-server in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container argocd-dex-server in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-dex-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_27.yaml:18-45
────────────────────────────────────────
  18 ┌       containers:
  19 │       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 └         - containerPort: 5556
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container copyutil in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install18_27.yaml:32-41
────────────────────────────────────────
  32 ┌       - command:
  33 │         - cp
  34 │         - /usr/local/bin/argocd-util
  35 │         - /shared
  36 │         image: argoproj/argocd:v1.2.4
  37 │         imagePullPolicy: Always
  38 │         name: copyutil
  39 │         volumeMounts:
  40 │         - mountPath: /shared
  41 └           name: static-files
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container dex in deployment argocd-dex-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install18_27.yaml:19-30
────────────────────────────────────────
  19 ┌       - command:
  20 │         - /shared/argocd-util
  21 │         - rundex
  22 │         image: quay.io/dexidp/dex:v2.14.0
  23 │         imagePullPolicy: Always
  24 │         name: dex
  25 │         ports:
  26 │         - containerPort: 5556
  27 └         - containerPort: 5557
  ..   
────────────────────────────────────────



install18_28.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'redis' of Deployment 'argocd-redis' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'redis' of 'deployment' 'argocd-redis' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'argocd-redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "redis" of deployment "argocd-redis" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-redis in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install18_28.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: redis
   6 │     app.kubernetes.io/name: argocd-redis
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-redis
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container argocd-redis in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_28.yaml:19-28
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 │         name: redis
  27 │         ports:
  28 └         - containerPort: 6379
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-redis in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_28.yaml:18-28
────────────────────────────────────────
  18 ┌       containers:
  19 │       - args:
  20 │         - --save
  21 │         - ''
  22 │         - --appendonly
  23 │         - 'no'
  24 │         image: redis:5.0.3
  25 │         imagePullPolicy: Always
  26 └         name: redis
  ..   
────────────────────────────────────────



install18_29.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'argocd-repo-server' of 'deployment' 'argocd-repo-server' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-repo-server' of Deployment 'argocd-repo-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "argocd-repo-server" of deployment "argocd-repo-server" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-repo-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install18_29.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: repo-server
   6 │     app.kubernetes.io/name: argocd-repo-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-repo-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container argocd-repo-server in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-repo-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_29.yaml:18-52
────────────────────────────────────────
  18 ┌       automountServiceAccountToken: false
  19 │       containers:
  20 │       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 └         imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-repo-server in deployment argocd-repo-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install18_29.yaml:20-45
────────────────────────────────────────
  20 ┌       - command:
  21 │         - uid_entrypoint.sh
  22 │         - argocd-repo-server
  23 │         - --redis
  24 │         - argocd-redis:6379
  25 │         image: argoproj/argocd:v1.2.4
  26 │         imagePullPolicy: Always
  27 │         livenessProbe:
  28 └           initialDelaySeconds: 5
  ..   
────────────────────────────────────────



install18_30.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'argocd-server' of Deployment 'argocd-server' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'argocd-server' of 'deployment' 'argocd-server' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'argocd-server' of Deployment 'argocd-server' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'argocd-server' of Deployment 'argocd-server' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argocd-server' of Deployment 'argocd-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "argocd-server" of deployment "argocd-server" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment argocd-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 install18_30.yaml:4-8
────────────────────────────────────────
   4 ┌   labels:
   5 │     app.kubernetes.io/component: server
   6 │     app.kubernetes.io/name: argocd-server
   7 │     app.kubernetes.io/part-of: argocd
   8 └   name: argocd-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container argocd-server in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment argocd-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 install18_30.yaml:18-55
────────────────────────────────────────
  18 ┌       containers:
  19 │       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 └           httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argocd-server in deployment argocd-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install18_30.yaml:19-45
────────────────────────────────────────
  19 ┌       - command:
  20 │         - argocd-server
  21 │         - --staticassets
  22 │         - /shared/app
  23 │         image: argoproj/argocd:v1.2.4
  24 │         imagePullPolicy: Always
  25 │         livenessProbe:
  26 │           httpGet:
  27 └             path: /healthz
  ..   
────────────────────────────────────────



install18_5.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-application-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install18_5.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install18_6.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-dex-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install18_6.yaml:10-18
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - get
  17 │   - list
  18 └   - watch
────────────────────────────────────────



install18_7.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install18_7.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install18_7.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install18_8.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install18_8.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install18_8.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install18_9.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install18_9.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────



install19_11.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argo-role' shouldn't have access to manage secrets in namespace 'argo'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install19_11.yaml:15-20
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ''
  17 │   resources:
  18 │   - secrets
  19 │   verbs:
  20 └   - get
────────────────────────────────────────



install19_15.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'argo-cluster-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install19_15.yaml:6-18
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - pods
  10 │   - pods/exec
  11 │   verbs:
  12 │   - create
  13 │   - get
  14 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0053 (HIGH): ClusterRole 'argo-cluster-role' should not have access to resource '["pods/exec"]' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to exec into a container with privileged access to the host or with an attached SA with higher RBAC permissions is a common escalation path to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv053
────────────────────────────────────────
 install19_15.yaml:6-18
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - pods
  10 │   - pods/exec
  11 │   verbs:
  12 │   - create
  13 │   - get
  14 └   - list
  ..   
────────────────────────────────────────



install19_16.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 110, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 1, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'argo-server-cluster-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 install19_16.yaml:14-20
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ''
  16 │   resources:
  17 │   - secrets
  18 │   verbs:
  19 │   - get
  20 └   - create
────────────────────────────────────────


AVD-KSV-0042 (MEDIUM): ClusterRole 'argo-server-cluster-role' should not have access to resource 'pods/log' for verbs ["delete", "deletecollection", "*"]
════════════════════════════════════════
Used to cover attacker’s tracks, but most clusters ship logs quickly off-cluster.

See https://avd.aquasec.com/misconfig/ksv042
────────────────────────────────────────
 install19_16.yaml:21-31
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - pods
  25 │   - pods/exec
  26 │   - pods/log
  27 │   verbs:
  28 │   - get
  29 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argo-server-cluster-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install19_16.yaml:21-31
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - pods
  25 │   - pods/exec
  26 │   - pods/log
  27 │   verbs:
  28 │   - get
  29 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0053 (HIGH): ClusterRole 'argo-server-cluster-role' should not have access to resource '["pods/exec"]' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to exec into a container with privileged access to the host or with an attached SA with higher RBAC permissions is a common escalation path to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv053
────────────────────────────────────────
 install19_16.yaml:21-31
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - pods
  25 │   - pods/exec
  26 │   - pods/log
  27 │   verbs:
  28 │   - get
  29 └   - list
  ..   
────────────────────────────────────────



install19_23.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'argo-server' of Deployment 'argo-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'argo-server' of Deployment 'argo-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'argo-server' of Deployment 'argo-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'argo-server' of Deployment 'argo-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'argo-server' of Deployment 'argo-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'argo-server' of Deployment 'argo-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "argo-server" of deployment "argo-server" in "argo" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container argo-server in deployment argo-server (namespace: argo) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install19_23.yaml:16-41
────────────────────────────────────────
  16 ┌       - args:
  17 │         - server
  18 │         - --auth-mode=server
  19 │         env: []
  20 │         image: quay.io/argoproj/argocli:v3.5.0
  21 │         name: argo-server
  22 │         ports:
  23 │         - containerPort: 2746
  24 └           name: web
  ..   
────────────────────────────────────────



install19_24.yaml (kubernetes)
==============================
Tests: 114 (SUCCESSES: 105, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'workflow-controller' of Deployment 'workflow-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'workflow-controller' of Deployment 'workflow-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'workflow-controller' of Deployment 'workflow-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'workflow-controller' of Deployment 'workflow-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'workflow-controller' of Deployment 'workflow-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'workflow-controller' of Deployment 'workflow-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "workflow-controller" of deployment "workflow-controller" in "argo" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container workflow-controller in deployment workflow-controller (namespace: argo) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 install19_24.yaml:16-47
────────────────────────────────────────
  16 ┌       - args: []
  17 │         command:
  18 │         - workflow-controller
  19 │         env:
  20 │         - name: LEADER_ELECTION_IDENTITY
  21 │           valueFrom:
  22 │             fieldRef:
  23 │               apiVersion: v1
  24 └               fieldPath: metadata.name
  ..   
────────────────────────────────────────



install1_10.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 113, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-applicationset-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install1_10.yaml:48-56
────────────────────────────────────────
  48 ┌ - apiGroups:
  49 │   - ''
  50 │   resources:
  51 │   - secrets
  52 │   - configmaps
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 └   - watch
────────────────────────────────────────



install1_11.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install1_11.yaml:21-28
────────────────────────────────────────
  21 ┌ - apiGroups:
  22 │   - ''
  23 │   resources:
  24 │   - configmaps
  25 │   - secrets
  26 │   verbs:
  27 │   - list
  28 └   - watch
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-notifications-controller' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install1_11.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resourceNames:
  40 │   - argocd-notifications-secret
  41 │   resources:
  42 │   - secrets
  43 │   verbs:
  44 └   - get
────────────────────────────────────────



install1_12.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'argocd-server' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 install1_12.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'argocd-server' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 install1_12.yaml:10-22
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ''
  12 │   resources:
  13 │   - secrets
  14 │   - configmaps
  15 │   verbs:
  16 │   - create
  17 │   - get
  18 └   - list
  ..   
────────────────────────────────────────



install1_13.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 install1_13.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-application-controller' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install1_13.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



install1_14.yaml (kubernetes)
=============================
Tests: 114 (SUCCESSES: 112, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'argocd-server' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 install1_14.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 │   - delete
  16 │   - get
  17 └   - patch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'argocd-server' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 install1_14.yaml:40-45
────────────────────────────────────────
  40 ┌ - apiGroups:
  41 │   - batch
  42 │   resources:
  43 │   - jobs
  44 │   verbs:
  45 └   - create
────────────────────────────────────────


