
Report Summary

┌──────────────────────────────────────────┬────────────┬───────────────────┐
│                  Target                  │    Type    │ Misconfigurations │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ NSquota-resource.yaml                    │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ NSquota-resource_1.yaml                  │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ NSquota-resource_2.yaml                  │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ OAuth2_prox_deploy.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ OAuth2_prox_deploy_1.yaml                │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ OAuth2_prox_deploy_2.yaml                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ OAuth2_proxy_config.yaml                 │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ Oauth21.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ Oauth21_1.yaml                           │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ Oauth21_2.yaml                           │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns150_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns151.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns152.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns152_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns153_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns153_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns154.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns155.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns156.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns157.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns158.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns159.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns16.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns160.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns161.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns161_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns162.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns163.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns164.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns165.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns166.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns166_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns167.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns168.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns169.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns169_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns17.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns170.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns171.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns172.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns173.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns174.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns175.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns175_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns176.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns176_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns176_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns176_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns177.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns178.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns179.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns18.yaml                                │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns180.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns181.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns182.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns182_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns183.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns184.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns185.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns186.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns187.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns187_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns188.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns188_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns189.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns19.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns190.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns191.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns191_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns191_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns192.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns193.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns194.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns195.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns196.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns197.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns198.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns199.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns2.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns20.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns200.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns201.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns202.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns203.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns204.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns205.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns206.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns207.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns208.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns208_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns208_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns208_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns209.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns21.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns210.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns2100.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns2100_1.yaml                            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns2100_2.yaml                            │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns2100_3.yaml                            │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns2101.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns210_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns211.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns212.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns213.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns214.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns215.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns216.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns217.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns217_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns218.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns219.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns22.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns220.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns221.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns222.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns222_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns223.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns224.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns225.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns225_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns226.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns227.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns228.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns229.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns23.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns230.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns231.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns231_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns232.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns232_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns232_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns232_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns233.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns234.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns235.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns236.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns237.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns238.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns238_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns239.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns24.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns240.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns241.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns242.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns243.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns243_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns244.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns244_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns245.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns246.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns247.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns247_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns247_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns248.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns249.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns25.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns250.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns251.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns252.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns253.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns254.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns255.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns256.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns257.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns258.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns259.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns26.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns260.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns261.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns262.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns263.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns264.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns264_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns264_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns264_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns265.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns266.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns266_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns267.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns268.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns269.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns27.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns270.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns271.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns272.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns273.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns273_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns274.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns275.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns276.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns277.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns278.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns278_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns279.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns28.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns280.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns281.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns281_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns282.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns283.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns284.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns285.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns286.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns287.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns287_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns288.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns288_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns288_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns288_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns289.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns29.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns290.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns291.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns292.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns293.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns294.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns294_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns295.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns296.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns297.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns298.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns299.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns299_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns3.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns30.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns300.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns300_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns301.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns302.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns303.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns303_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns303_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns304.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns305.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns306.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns307.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns308.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns309.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns31.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns310.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns311.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns312.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns313.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns314.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns315.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns316.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns317.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns318.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns319.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns32.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns320.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns320_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns320_2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns320_3.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns321.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns322.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns322_1.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns323.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns324.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns325.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns326.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns327.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns328.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns329.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns33.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns330.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns331.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns332.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns333.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns334.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns335.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns336.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns337.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns338.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns339.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns34.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns340.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns341.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns342.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns343.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns344.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns345.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns346.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns347.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns348.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns349.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns35.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns350.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns351.yaml                               │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns352.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns353.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns354.yaml                               │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns355.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns356.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns357.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns358.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns359.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns36.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns360.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns362.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns363.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns364.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns365.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns366.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns367.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns368.yaml                               │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns369.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns37.yaml                                │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns370.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns371.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns372.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns373.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns374.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns375.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns376.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns377.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns378.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns379.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns38.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns380.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns381.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns382.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns383.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns39.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns4.yaml                                 │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns40.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns41.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns42.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns43.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns44.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns44_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns45.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns46.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns47.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns48.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns49.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns49_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns5.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns50.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns51.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns52.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns52_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns53.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns54.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns55.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns56.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns57.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns58.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns58_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns59.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns59_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns59_2.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns59_3.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns6.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns60.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns61.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns62.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns63.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns64.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns65.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns65_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns66.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns67.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns68.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns69.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns7.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns70.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns70_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns71.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns71_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns72.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns73.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns74.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns74_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns74_2.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns75.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns76.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns77.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns78.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns79.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns8.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns80.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns81.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns82.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns83.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns84.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns85.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns86.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns87.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns88.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns89.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns9.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns90.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns91.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns91_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns91_2.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns91_3.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns92.yaml                                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns93.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns93_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns94.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns95.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns96.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns97.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns98.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns99.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns9_1.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns9_2.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_1.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_2.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_and_pv.yaml                           │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_and_pv_1.yaml                         │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_external-secrets.yaml                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_external-secrets1.yaml                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_github-actions.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_github-actions1.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_headlamp.yaml                         │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_headlamp1.yaml                        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_paperless-ngx.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_paperless-ngx1.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_rtmp-server.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_rtmp-server1.yaml                     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_vault.yaml                            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ns_vault1.yaml                           │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nsbloodhound.yaml                        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nslookup.yaml                            │ kubernetes │        36         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nsm-configmap.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nsm-configmap1.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nsp-deny-pod.yaml                        │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nsp-deny-pod1.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nsp-deny-pod2.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nssf-deployment1.yaml                    │ kubernetes │        31         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nssf-service1.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ntnx-config-patch.yaml                   │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ntnx-config-patch_1.yaml                 │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ntnx-config-patch_2.yaml                 │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ntnx-config-patch_3.yaml                 │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ null-list1_1.yaml                        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ null-list_1.yaml                         │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ numbers-svc.yaml                         │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ numbers-svc1.yaml                        │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nut.yaml                                 │ kubernetes │        11         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nut2.yaml                                │ kubernetes │        11         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-daemonset-preloaded.yaml          │ kubernetes │        51         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-device-plugin.yaml                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-device-plugin1.yaml               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-device-plugin1_1.yaml             │ kubernetes │        15         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-device-plugin2.yaml               │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-device-plugin_1.yaml              │ kubernetes │        15         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer.yaml             │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer1.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer11.yaml           │ kubernetes │        37         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer12.yaml           │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer13.yaml           │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer14.yaml           │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer15.yaml           │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer16.yaml           │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer2.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer3.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer4.yaml            │ kubernetes │        37         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer5.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer6.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer7.yaml            │ kubernetes │        37         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer8.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-driver-installer9.yaml            │ kubernetes │        38         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-gpu-driver.yaml                   │ kubernetes │        33         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-smi.yaml                          │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-smi1.yaml                         │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-test.yaml                         │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidia-test_1.yaml                       │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidiamigadapter_editor_role.yaml        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvidiamigadapter_viewer_role.yaml        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvshare-system-quotas.yaml               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvshare-system-quotas_1.yaml             │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nvshare-system.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-anlf-configmap.yaml                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-anlf-deployment.yaml               │ kubernetes │        31         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-anlf-service.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-configmap.yaml                     │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-deployment.yaml                    │ kubernetes │        31         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-mtlf-configmap.yaml                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-mtlf-deployment.yaml               │ kubernetes │        31         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-mtlf-service.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwdaf-service.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwp.yaml                                 │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwujobs-app.yaml                         │ kubernetes │        22         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwujobs-app_1.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nwujobs-app_2.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nydus.yaml                               │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ nzb-ingress.yaml                         │ kubernetes │         3         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent.yaml                      │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_1.yaml                    │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_2.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_3.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_4.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_5.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_6.yaml                    │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-agent_7.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-cluster-service-manager.yaml    │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oakestra-cluster-service-manager_1.yaml  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-gitlab-pages.sops.yaml             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy.yaml                         │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy1.yaml                        │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy1_1.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy1_2.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy1_3.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy_1.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy_2.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth-proxy_3.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth.yaml                               │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth1.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth10.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth11.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth12.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth13.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth14.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth14_4.yaml                           │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth15.yaml                             │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth15_4.yaml                           │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy.yaml                        │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy1.yaml                       │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy1_1.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy1_2.yaml                     │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy1_3.yaml                     │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy1_4.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy2.yaml                       │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy2_1.yaml                     │ kubernetes │         3         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy4.yaml                       │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy4_1.yaml                     │ kubernetes │         3         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy4_2.yaml                     │ kubernetes │         3         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy4_3.yaml                     │ kubernetes │         3         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy5.yaml                       │ kubernetes │        17         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy5_1.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy5_2.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy_1.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy_2.yaml                      │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy_3.yaml                      │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-proxy_4.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2-route.yaml                        │ kubernetes │         3         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth2.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth3.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth4.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth5.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth6.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth6_4.yaml                            │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth7.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth7_4.yaml                            │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth8.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth9.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth_config.yaml                        │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oauth_config1.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ obese-httpd.yaml                         │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ obese-httpd_1.yaml                       │ kubernetes │        18         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ object-compute-quota.yaml                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ object-count-quota.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ object4_2.yaml                           │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ object_counts.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ objectbasetemplate.yaml                  │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ objecttemplate_editor_role.yaml          │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ objecttemplate_viewer_role.yaml          │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ observed-deploy-nginx.yaml               │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ obzev0resource_editor_role.yaml          │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ obzev0resource_viewer_role.yaml          │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ocean-nodemailer.yaml                    │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ocean-nodemailer_1.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oci-dummy-pod.yaml                       │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oci-helm-secret.yaml                     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oci-helm-secret1.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ octant.yaml                              │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ octant_1.yaml                            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ octant_2.yaml                            │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ octant_3.yaml                            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ octant_4.yaml                            │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ octant_5.yaml                            │ kubernetes │         8         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x].yaml                 │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]1.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]11.yaml               │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]12.yaml               │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]13.yaml               │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]14.yaml               │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]15.yaml               │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]2.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]3.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]4.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]5.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]6.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]7.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]8.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oddly-named-file[x]9.yaml                │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ odoo.yaml                                │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ odoo_svc.yaml                            │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ offers-service.yaml                      │ kubernetes │        18         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ offers-service_1.yaml                    │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ offers-service_2.yaml                    │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ogcapi_editor_role.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ogcapi_viewer_role.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oidc-credentials.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oidc-credentials1.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oidc-rbac.yaml                           │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oidc-rbac1.yaml                          │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oke-admin-service-account.yaml           │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oke-admin-service-account1.yaml          │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oke-kubeconfig-sa-token.yaml             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oke-kubeconfig-sa-token1.yaml            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oke-kubeconfig-sa-token2.yaml            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oknok-job.yaml                           │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller.yaml        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_1.yaml      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_10.yaml     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_11.yaml     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_12.yaml     │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_13.yaml     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_14.yaml     │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_15.yaml     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_16.yaml     │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_17.yaml     │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_2.yaml      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_3.yaml      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_4.yaml      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_5.yaml      │ kubernetes │         5         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_6.yaml      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_7.yaml      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_8.yaml      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingress-nginx-controller_9.yaml      │ kubernetes │        15         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl.yaml                     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_1.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_10.yaml                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_11.yaml                 │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_12.yaml                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_13.yaml                 │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_14.yaml                 │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_15.yaml                 │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_16.yaml                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_17.yaml                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_2.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_3.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_4.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_5.yaml                  │ kubernetes │         5         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_6.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_7.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_8.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl1_9.yaml                  │ kubernetes │        15         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_1.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_10.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_11.yaml                  │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_12.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_13.yaml                  │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_14.yaml                  │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_15.yaml                  │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_16.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_17.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_2.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_3.yaml                   │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_4.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_5.yaml                   │ kubernetes │         5         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_6.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_7.yaml                   │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_8.yaml                   │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-ingressctrl_9.yaml                   │ kubernetes │        15         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource.yaml                        │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource1.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource10.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource11.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource12.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource13.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource14.yaml                      │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource2.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource3.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource4.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource5.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource6.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource7.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource8.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ old-resource9.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oldnginx.yaml                            │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-claim0-persistentvolumeclaim.yaml │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-claim1-persistentvolumeclaim.yaml │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-config.yaml                       │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-config_1.yaml                     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-deployment.yaml                   │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-deployment1.yaml                  │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-deployment1_1.yaml                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-deployment2.yaml                  │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-deployment_1.yaml                 │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube.yaml                         │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube_1.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube_2.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube_3.yaml                       │ kubernetes │        16         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube_4.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube_5.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-kube_6.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-nvidia.yaml                       │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-pvc.yaml                          │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-pvc2.yaml                         │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-service.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-service1.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-service2.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-service3.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-service5.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-statefulset.yaml                  │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama-statefulset_1.yaml                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama2.yaml                             │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama2_1.yaml                           │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama2_2.yaml                           │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama5.yaml                             │ kubernetes │        22         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama5_1.yaml                           │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_gpu.yaml                          │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_gpu1.yaml                         │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_gpu1_1.yaml                       │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_gpu1_2.yaml                       │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_gpu_1.yaml                        │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_gpu_2.yaml                        │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollama_metrics.yaml                      │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ollamadeployment.yaml                    │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm.yaml                                 │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_2.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_3.yaml                              │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_4.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_5.yaml                              │ kubernetes │        17         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_6.yaml                              │ kubernetes │        17         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_7.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm1_8.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_1.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_2.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_3.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_4.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_6.yaml                              │ kubernetes │         8         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_7.yaml                              │ kubernetes │         8         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_8.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm2_9.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3_1.yaml                              │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3_2.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3_4.yaml                              │ kubernetes │         8         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3_5.yaml                              │ kubernetes │         8         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3_6.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm3_7.yaml                              │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm4.yaml                                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_1.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_2.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_3.yaml                               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_4.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_5.yaml                               │ kubernetes │        17         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_6.yaml                               │ kubernetes │        17         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_7.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ olm_8.yaml                               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omada-controller2.yaml                   │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omada-controller2_1.yaml                 │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omelete.yaml                             │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omelete1.yaml                            │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omji_editor_role.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omji_viewer_role.yaml                    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omni-es-deployment.yaml                  │ kubernetes │        17         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ omni-es-deployment_1.yaml                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-client-configmap.yaml                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-client-configmap1.yaml               │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-client-deployment.yaml               │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-client-deployment1.yaml              │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-client-service.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-client-service1.yaml                 │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-ingress.yaml                         │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-ingress1.yaml                        │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-server-configmap.yaml                │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-server-configmap1.yaml               │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-server-deployment.yaml               │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-server-deployment1.yaml              │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-server-service.yaml                  │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ oms-server-service1.yaml                 │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ondemand-sidecar-injector-role.yaml      │ kubernetes │         1         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ondevicedeploy.yaml                      │ kubernetes │        18         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ondevicedeploy1.yaml                     │ kubernetes │        18         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ondevicedeploy1_1.yaml                   │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ ondevicedeploy_1.yaml                    │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ one-app-deploy.yaml                      │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ one-app-deploy1.yaml                     │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ one-app-deploy2.yaml                     │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ one-app-deploy3.yaml                     │ kubernetes │        20         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ one-container-pod.yaml                   │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepage-initial.yaml                     │ kubernetes │        19         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepage-service.yaml                     │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepage.yaml                             │ kubernetes │        37         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepage_service-initial.yaml             │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepage_service-old.yaml                 │ kubernetes │         2         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect.secret.sops.yaml     │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect.secret.sops1.yaml    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect.secret.sops2.yaml    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect.secret.sops3.yaml    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect.secret.sops4.yaml    │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect.sops.yaml            │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-connect2.yaml                │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-secret.yaml                  │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onepassword-token.yaml                   │ kubernetes │         0         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onetimesecret.yaml                       │ kubernetes │        21         │
├──────────────────────────────────────────┼────────────┼───────────────────┤
│ onetimesecret_1.yaml                     │ kubernetes │         2         │
└──────────────────────────────────────────┴────────────┴───────────────────┘
Legend:
- '-': Not scanned
- '0': Clean (no security findings detected)


NSquota-resource.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 NSquota-resource.yaml:7-16
────────────────────────────────────────
   7 ┌   hard:
   8 │     requests.cpu: 1000m
   9 │     limits.cpu: 2000m
  10 │     requests.memory: 1Gi
  11 │     limits.memory: 2Gi
  12 │     count/deployments.apps: '1'
  13 │     count/pods: '4'
  14 │     count/replicasets.apps: '1'
  15 │     count/secrets: '4'
  16 └     count/services: '1'
────────────────────────────────────────



NSquota-resource_1.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 NSquota-resource_1.yaml:7-16
────────────────────────────────────────
   7 ┌   hard:
   8 │     requests.cpu: 1000m
   9 │     limits.cpu: 2000m
  10 │     requests.memory: 1Gi
  11 │     limits.memory: 2Gi
  12 │     count/deployments.apps: '1'
  13 │     count/pods: '4'
  14 │     count/replicasets.apps: '1'
  15 │     count/secrets: '4'
  16 └     count/services: '1'
────────────────────────────────────────



NSquota-resource_2.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 NSquota-resource_2.yaml:6-28
────────────────────────────────────────
   6 ┌   limits:
   7 │   - type: Pod
   8 │     max:
   9 │       cpu: 2
  10 │       memory: 1Gi
  11 │     min:
  12 │       cpu: 200m
  13 │       memory: 64Mi
  14 └   - type: Container
  ..   
────────────────────────────────────────



OAuth2_prox_deploy.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 OAuth2_prox_deploy.yaml:9-14
────────────────────────────────────────
   9 ┌   type: ClusterIP
  10 │   selector:
  11 │     app: oauth-proxy
  12 │   ports:
  13 │   - name: http-oauthproxy
  14 └     port: 4180
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 OAuth2_prox_deploy.yaml:9-14
────────────────────────────────────────
   9 ┌   type: ClusterIP
  10 │   selector:
  11 │     app: oauth-proxy
  12 │   ports:
  13 │   - name: http-oauthproxy
  14 └     port: 4180
────────────────────────────────────────



OAuth2_prox_deploy_1.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth-proxy' of 'deployment' 'oauth-proxy' in 'oauth2-proxy' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth-proxy' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth-proxy' of Deployment 'oauth-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:9-32
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: oauth-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: oauth-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:9-32
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: oauth-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: oauth-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth-proxy" of deployment "oauth-proxy" in "oauth2-proxy" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth-proxy in oauth2-proxy namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth-proxy in oauth2-proxy namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:18-32
────────────────────────────────────────
  18 ┌       volumes:
  19 │       - name: oauth2-proxy-config
  20 │         configMap:
  21 │           name: oauth2-proxy-config
  22 │       containers:
  23 │       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 └         - containerPort: 4180
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth-proxy in deployment oauth-proxy (namespace: oauth2-proxy) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 OAuth2_prox_deploy_1.yaml:23-32
────────────────────────────────────────
  23 ┌       - name: oauth-proxy
  24 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  25 │         ports:
  26 │         - containerPort: 4180
  27 │         volumeMounts:
  28 │         - name: oauth2-proxy-config
  29 │           mountPath: /etc/oauth2-proxy.cfg
  30 │           subPath: oauth2-proxy.cfg
  31 │         args:
  32 └         - --config=/etc/oauth2-proxy.cfg
────────────────────────────────────────



OAuth2_prox_deploy_2.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 OAuth2_prox_deploy_2.yaml:7-18
────────────────────────────────────────
   7 ┌   ingressClassName: nginx
   8 │   rules:
   9 │   - host: foo.bar.com
  10 │     http:
  11 │       paths:
  12 │       - backend:
  13 │           service:
  14 │             name: oauth-proxy
  15 └             port:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 OAuth2_prox_deploy_2.yaml:7-18
────────────────────────────────────────
   7 ┌   ingressClassName: nginx
   8 │   rules:
   9 │   - host: foo.bar.com
  10 │     http:
  11 │       paths:
  12 │       - backend:
  13 │           service:
  14 │             name: oauth-proxy
  15 └             port:
  ..   
────────────────────────────────────────



OAuth2_proxy_config.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0109 (HIGH): ConfigMap 'oauth2-proxy-config' in 'oauth2-proxy' namespace stores secrets in key(s) or value(s) '{"client_secret", "cookie_secret"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



Oauth21.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0109 (HIGH): ConfigMap 'oauth2-proxy-config' in 'default' namespace stores secrets in key(s) or value(s) '{"client_secret ", "cookie_secret "}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



Oauth21_1.yaml (kubernetes)
===========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth2-proxy' of 'deployment' 'oauth2-proxy' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 Oauth21_1.yaml:6-26
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: oauth2-proxy
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: oauth2-proxy
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 Oauth21_1.yaml:6-26
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: oauth2-proxy
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: oauth2-proxy
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth2-proxy" of deployment "oauth2-proxy" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment oauth2-proxy in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 Oauth21_1.yaml:4
────────────────────────────────────────
   4 [   name: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 Oauth21_1.yaml:15-26
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 │           mountPath: /etc/oauth2_proxy
  23 └       volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth2-proxy in deployment oauth2-proxy (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 Oauth21_1.yaml:16-22
────────────────────────────────────────
  16 ┌       - name: oauth2-proxy
  17 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0
  18 │         ports:
  19 │         - containerPort: 4180
  20 │         volumeMounts:
  21 │         - name: oauth2-proxy-config
  22 └           mountPath: /etc/oauth2_proxy
────────────────────────────────────────



Oauth21_2.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 Oauth21_2.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: oauth2-proxy
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 80
  11 └     targetPort: 4180
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 Oauth21_2.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: oauth2-proxy
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 80
  11 └     targetPort: 4180
────────────────────────────────────────



ns151.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns151.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns151.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns18.yaml (kubernetes)
======================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────



ns209.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns209.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns209.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns2100_2.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'deployment-dev' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'deployment-dev' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'deployment-dev' in 'dev' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'deployment-dev' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'deployment-dev' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'deployment-dev' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'deployment-dev' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'deployment-dev' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'deployment-dev' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'deployment-dev' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'deployment-dev' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns2100_2.yaml:11-24
────────────────────────────────────────
  11 ┌   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: front
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         app: front
  19 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns2100_2.yaml:11-24
────────────────────────────────────────
  11 ┌   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: front
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         app: front
  19 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "deployment-dev" in "dev" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment deployment-dev in dev namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment-dev in dev namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ns2100_2.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment-dev in dev namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ns2100_2.yaml:20-24
────────────────────────────────────────
  20 ┌       containers:
  21 │       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────



ns2100_3.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'deployment-prod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'deployment-prod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'deployment-prod' in 'prod' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'deployment-prod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'deployment-prod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'deployment-prod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'deployment-prod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'deployment-prod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'deployment-prod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'deployment-prod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'deployment-prod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns2100_3.yaml:11-24
────────────────────────────────────────
  11 ┌   replicas: 5
  12 │   selector:
  13 │     matchLabels:
  14 │       app: front
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         app: front
  19 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns2100_3.yaml:11-24
────────────────────────────────────────
  11 ┌   replicas: 5
  12 │   selector:
  13 │     matchLabels:
  14 │       app: front
  15 │   template:
  16 │     metadata:
  17 │       labels:
  18 │         app: front
  19 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "deployment-prod" in "prod" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment deployment-prod in prod namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment-prod in prod namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ns2100_3.yaml:21-24
────────────────────────────────────────
  21 ┌       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment-prod in prod namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ns2100_3.yaml:20-24
────────────────────────────────────────
  20 ┌       containers:
  21 │       - name: nginx
  22 │         image: nginx:alpine
  23 │         ports:
  24 └         - containerPort: 80
────────────────────────────────────────



ns265.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns265.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns265.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns321.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns321.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns321.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns323.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns323.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns323.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns351.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────



ns354.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────



ns368.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────



ns37.yaml (kubernetes)
======================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────



ns370.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns370.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns370.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns371.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns371.yaml:6
────────────────────────────────────────
   6 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns371.yaml:6
────────────────────────────────────────
   6 [ spec: {}
────────────────────────────────────────



ns372.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns372.yaml:6
────────────────────────────────────────
   6 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns372.yaml:6
────────────────────────────────────────
   6 [ spec: {}
────────────────────────────────────────



ns4.yaml (kubernetes)
=====================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────



ns92.yaml (kubernetes)
======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns92.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns92.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns_and_pv.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns_and_pv.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns_and_pv.yaml:5
────────────────────────────────────────
   5 [ spec: {}
────────────────────────────────────────



ns_and_pv_1.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ns_and_pv_1.yaml:6-18
────────────────────────────────────────
   6 ┌   capacity:
   7 │     storage: 5Gi
   8 │   volumeMode: Filesystem
   9 │   accessModes:
  10 │   - ReadWriteOnce
  11 │   persistentVolumeReclaimPolicy: Recycle
  12 │   storageClassName: slow
  13 │   mountOptions:
  14 └   - hard
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ns_and_pv_1.yaml:6-18
────────────────────────────────────────
   6 ┌   capacity:
   7 │     storage: 5Gi
   8 │   volumeMode: Filesystem
   9 │   accessModes:
  10 │   - ReadWriteOnce
  11 │   persistentVolumeReclaimPolicy: Recycle
  12 │   storageClassName: slow
  13 │   mountOptions:
  14 └   - hard
  ..   
────────────────────────────────────────



nslookup.yaml (kubernetes)
==========================
Tests: 133 (SUCCESSES: 97, FAILURES: 36)
Failures: 36 (UNKNOWN: 0, LOW: 23, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'init-myservice' of Pod 'myapp-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'myapp-container' of Pod 'myapp-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'init-myservice' of 'pod' 'myapp-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'myapp-container' of 'pod' 'myapp-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'init-myservice' of Pod 'myapp-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'myapp-container' of Pod 'myapp-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'init-myservice' of Pod 'myapp-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'myapp-container' of Pod 'myapp-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'init-myservice' of Pod 'myapp-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'myapp-container' of Pod 'myapp-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'init-myservice' of Pod 'myapp-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'myapp-container' of Pod 'myapp-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nslookup.yaml:8-14
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: myapp-container
  10 │       image: myapp
  11 │   initContainers:
  12 │     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nslookup.yaml:8-14
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: myapp-container
  10 │       image: myapp
  11 │   initContainers:
  12 │     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "init-myservice" of pod "myapp-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "myapp-container" of pod "myapp-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod myapp-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nslookup.yaml:6
────────────────────────────────────────
   6 [   name: myapp-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container myapp-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nslookup.yaml:9-10
────────────────────────────────────────
   9 ┌     - name: myapp-container
  10 └       image: myapp
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container myapp-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nslookup.yaml:12-14
────────────────────────────────────────
  12 ┌     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod myapp-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nslookup.yaml:8-14
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: myapp-container
  10 │       image: myapp
  11 │   initContainers:
  12 │     - name: init-myservice
  13 │       image: busybox
  14 └       command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
────────────────────────────────────────



nsp-deny-pod.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nsp-deny-pod.yaml:7-12
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │         run: pod01
  10 │   policyTypes:
  11 │   - Ingress
  12 └   - Egress
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nsp-deny-pod.yaml:7-12
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │         run: pod01
  10 │   policyTypes:
  11 │   - Ingress
  12 └   - Egress
────────────────────────────────────────



nsp-deny-pod1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nsp-deny-pod1.yaml:7-12
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │         run: pod01
  10 │   policyTypes:
  11 │   - Ingress
  12 └   - Egress
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nsp-deny-pod1.yaml:7-12
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │         run: pod01
  10 │   policyTypes:
  11 │   - Ingress
  12 └   - Egress
────────────────────────────────────────



nsp-deny-pod2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nsp-deny-pod2.yaml:7-12
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │         run: pod01
  10 │   policyTypes:
  11 │   - Ingress
  12 └   - Egress
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nsp-deny-pod2.yaml:7-12
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │         run: pod01
  10 │   policyTypes:
  11 │   - Ingress
  12 └   - Egress
────────────────────────────────────────



nssf-deployment1.yaml (kubernetes)
==================================
Tests: 128 (SUCCESSES: 97, FAILURES: 31)
Failures: 31 (UNKNOWN: 0, LOW: 19, MEDIUM: 7, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nssf' of Deployment 'free5gc-nssf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nssf' of Deployment 'free5gc-nssf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nssf' of 'deployment' 'free5gc-nssf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'wait-udr' of 'deployment' 'free5gc-nssf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nssf' of Deployment 'free5gc-nssf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nssf' of Deployment 'free5gc-nssf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nssf' of Deployment 'free5gc-nssf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nssf' of Deployment 'free5gc-nssf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'wait-udr' of Deployment 'free5gc-nssf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nssf-deployment1.yaml:9-54
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nssf
  13 │   replicas: 1
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 └         app: free5gc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nssf-deployment1.yaml:9-54
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nssf
  13 │   replicas: 1
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 └         app: free5gc
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nssf" of deployment "free5gc-nssf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "wait-udr" of deployment "free5gc-nssf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment free5gc-nssf in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nssf-deployment1.yaml:4-7
────────────────────────────────────────
   4 ┌   name: free5gc-nssf
   5 │   labels:
   6 │     app: free5gc
   7 └     nf: nssf
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nssf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nssf-deployment1.yaml:21-26
────────────────────────────────────────
  21 ┌         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 └           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nssf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment free5gc-nssf in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nssf-deployment1.yaml:20-54
────────────────────────────────────────
  20 ┌       initContainers:
  21 │         - name: wait-udr
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │           - name: DEPENDENCIES
  25 │             value: udr-nudr:8000
  26 │           command: ["sh", "-c", "until nc -z $DEPENDENCIES; do echo waiting for the UDR; sleep 2; done;"]
  27 │       containers:
  28 └       - image: edierbra/free5gc:v3.2.5
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nssf in deployment free5gc-nssf (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nssf-deployment1.yaml:28-46
────────────────────────────────────────
  28 ┌       - image: edierbra/free5gc:v3.2.5
  29 │         name: nssf
  30 │         ports:
  31 │         - containerPort: 8000
  32 │         command: ["./nssf"]
  33 │         args: ["--config", "config/nssfcfg.yaml"]   
  34 │         env:
  35 │           - name: GIN_MODE
  36 └             value: release
  ..   
────────────────────────────────────────



nssf-service1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nssf-service1.yaml:9-13
────────────────────────────────────────
   9 ┌   ports:
  10 │     - port: 8000
  11 │   selector:
  12 │     app: free5gc
  13 └     nf: nssf
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nssf-service1.yaml:9-13
────────────────────────────────────────
   9 ┌   ports:
  10 │     - port: 8000
  11 │   selector:
  12 │     app: free5gc
  13 └     nf: nssf
────────────────────────────────────────



ntnx-config-patch.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ml-pipeline-ui' of 'deployment' 'ml-pipeline-ui' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ml-pipeline-ui' of Deployment 'ml-pipeline-ui' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ntnx-config-patch.yaml:6-37
────────────────────────────────────────
   6 ┌   template:
   7 │     metadata:
   8 │       labels:
   9 │         app: ml-pipeline-ui
  10 │     spec:
  11 │       volumes:
  12 │       - name: config-volume
  13 │         configMap:
  14 └           name: ml-pipeline-ui-configmap
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ntnx-config-patch.yaml:6-37
────────────────────────────────────────
   6 ┌   template:
   7 │     metadata:
   8 │       labels:
   9 │         app: ml-pipeline-ui
  10 │     spec:
  11 │       volumes:
  12 │       - name: config-volume
  13 │         configMap:
  14 └           name: ml-pipeline-ui-configmap
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ml-pipeline-ui" of deployment "ml-pipeline-ui" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ml-pipeline-ui in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ntnx-config-patch.yaml:4
────────────────────────────────────────
   4 [   name: ml-pipeline-ui
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ml-pipeline-ui in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch.yaml:16-37
────────────────────────────────────────
  16 ┌       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 │           valueFrom:
  20 │             secretKeyRef:
  21 │               name: mlpipeline-minio-artifact
  22 │               key: accesskey
  23 │         - name: AWS_SECRET_ACCESS_KEY
  24 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ml-pipeline-ui in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch.yaml:11-37
────────────────────────────────────────
  11 ┌       volumes:
  12 │       - name: config-volume
  13 │         configMap:
  14 │           name: ml-pipeline-ui-configmap
  15 │       containers:
  16 │       - name: ml-pipeline-ui
  17 │         env:
  18 │         - name: AWS_ACCESS_KEY_ID
  19 └           valueFrom:
  ..   
────────────────────────────────────────



ntnx-config-patch_1.yaml (kubernetes)
=====================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ml-pipeline-api-server' of 'deployment' 'ml-pipeline' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ml-pipeline-api-server' of Deployment 'ml-pipeline' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ntnx-config-patch_1.yaml:6-30
────────────────────────────────────────
   6 ┌   template:
   7 │     metadata:
   8 │       labels:
   9 │         app: ml-pipeline
  10 │     spec:
  11 │       containers:
  12 │       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ntnx-config-patch_1.yaml:6-30
────────────────────────────────────────
   6 ┌   template:
   7 │     metadata:
   8 │       labels:
   9 │         app: ml-pipeline
  10 │     spec:
  11 │       containers:
  12 │       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ml-pipeline-api-server" of deployment "ml-pipeline" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ml-pipeline in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ntnx-config-patch_1.yaml:4
────────────────────────────────────────
   4 [   name: ml-pipeline
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ml-pipeline in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch_1.yaml:12-30
────────────────────────────────────────
  12 ┌       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 │           valueFrom:
  20 └             configMapKeyRef:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ml-pipeline in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch_1.yaml:11-30
────────────────────────────────────────
  11 ┌       containers:
  12 │       - env:
  13 │         - name: OBJECTSTORECONFIG_HOST
  14 │           valueFrom:
  15 │             configMapKeyRef:
  16 │               name: pipeline-install-config
  17 │               key: objStoreHost
  18 │         - name: OBJECTSTORECONFIG_BUCKETNAME
  19 └           valueFrom:
  ..   
────────────────────────────────────────



ntnx-config-patch_2.yaml (kubernetes)
=====================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'main' of Deployment 'metadata-writer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'main' of Deployment 'metadata-writer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'main' of 'deployment' 'metadata-writer' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'main' of Deployment 'metadata-writer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'main' of Deployment 'metadata-writer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'main' of Deployment 'metadata-writer' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'main' of Deployment 'metadata-writer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'main' of Deployment 'metadata-writer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'main' of Deployment 'metadata-writer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'main' of Deployment 'metadata-writer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'main' of Deployment 'metadata-writer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'main' of Deployment 'metadata-writer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ntnx-config-patch_2.yaml:6-25
────────────────────────────────────────
   6 ┌   template:
   7 │     metadata:
   8 │       labels:
   9 │         app: metadata-writer
  10 │     spec:
  11 │       containers:
  12 │       - name: main
  13 │         env:
  14 └         - name: OBJECT_STORE_ENDPOINT
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ntnx-config-patch_2.yaml:6-25
────────────────────────────────────────
   6 ┌   template:
   7 │     metadata:
   8 │       labels:
   9 │         app: metadata-writer
  10 │     spec:
  11 │       containers:
  12 │       - name: main
  13 │         env:
  14 └         - name: OBJECT_STORE_ENDPOINT
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "main" of deployment "metadata-writer" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment metadata-writer in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ntnx-config-patch_2.yaml:4
────────────────────────────────────────
   4 [   name: metadata-writer
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container metadata-writer in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch_2.yaml:12-25
────────────────────────────────────────
  12 ┌       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 │         - name: OBJECT_STORE_BUCKET
  20 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment metadata-writer in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch_2.yaml:11-25
────────────────────────────────────────
  11 ┌       containers:
  12 │       - name: main
  13 │         env:
  14 │         - name: OBJECT_STORE_ENDPOINT
  15 │           valueFrom:
  16 │             configMapKeyRef:
  17 │               name: pipeline-install-config
  18 │               key: objStoreHost
  19 └         - name: OBJECT_STORE_BUCKET
  ..   
────────────────────────────────────────



ntnx-config-patch_3.yaml (kubernetes)
=====================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'profile-controller' of 'deployment' 'kubeflow-pipelines-profile-controller' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'profile-controller' of Deployment 'kubeflow-pipelines-profile-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ntnx-config-patch_3.yaml:6-24
────────────────────────────────────────
   6 ┌   template:
   7 │     spec:
   8 │       containers:
   9 │       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 └               name: pipeline-install-config
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ntnx-config-patch_3.yaml:6-24
────────────────────────────────────────
   6 ┌   template:
   7 │     spec:
   8 │       containers:
   9 │       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 └               name: pipeline-install-config
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "profile-controller" of deployment "kubeflow-pipelines-profile-controller" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment kubeflow-pipelines-profile-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ntnx-config-patch_3.yaml:4
────────────────────────────────────────
   4 [   name: kubeflow-pipelines-profile-controller
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container kubeflow-pipelines-profile-controller in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch_3.yaml:9-24
────────────────────────────────────────
   9 ┌       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 │         - name: MINIO_SERVICE_HOST
  17 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment kubeflow-pipelines-profile-controller in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ntnx-config-patch_3.yaml:8-24
────────────────────────────────────────
   8 ┌       containers:
   9 │       - name: profile-controller
  10 │         env:
  11 │         - name: MINIO_SERVICE_REGION
  12 │           valueFrom:
  13 │             configMapKeyRef:
  14 │               name: pipeline-install-config
  15 │               key: objStoreRegion
  16 └         - name: MINIO_SERVICE_HOST
  ..   
────────────────────────────────────────



numbers-svc.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 numbers-svc.yaml:9-14
────────────────────────────────────────
   9 ┌   ports:
  10 │   - port: 80
  11 │     name: http
  12 │   selector:
  13 │     app: numbers-web
  14 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 numbers-svc.yaml:9-14
────────────────────────────────────────
   9 ┌   ports:
  10 │   - port: 80
  11 │     name: http
  12 │   selector:
  13 │     app: numbers-web
  14 └   type: ClusterIP
────────────────────────────────────────



numbers-svc1.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 numbers-svc1.yaml:9-14
────────────────────────────────────────
   9 ┌   ports:
  10 │   - port: 80
  11 │     name: http
  12 │   selector:
  13 │     app: numbers-web
  14 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 numbers-svc1.yaml:9-14
────────────────────────────────────────
   9 ┌   ports:
  10 │   - port: 80
  11 │     name: http
  12 │   selector:
  13 │     app: numbers-web
  14 └   type: ClusterIP
────────────────────────────────────────



nut.yaml (kubernetes)
=====================
Tests: 116 (SUCCESSES: 105, FAILURES: 11)
Failures: 11 (UNKNOWN: 0, LOW: 7, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'prometheus-nut-exporter' of Deployment 'prometheus-nut-exporter' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'prometheus-nut-exporter' of 'deployment' 'prometheus-nut-exporter' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'prometheus-nut-exporter' of Deployment 'prometheus-nut-exporter' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nut.yaml:8-50
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: prometheus-nut-exporter
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: prometheus-nut-exporter
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nut.yaml:8-50
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: prometheus-nut-exporter
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: prometheus-nut-exporter
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "prometheus-nut-exporter" of deployment "prometheus-nut-exporter" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment prometheus-nut-exporter in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nut.yaml:4-6
────────────────────────────────────────
   4 ┌   name: prometheus-nut-exporter
   5 │   labels:
   6 └     app: prometheus-nut-exporter
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment prometheus-nut-exporter in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nut.yaml:21-50
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 └             memory: 256Mi
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container prometheus-nut-exporter in deployment prometheus-nut-exporter (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nut.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────



nut2.yaml (kubernetes)
======================
Tests: 116 (SUCCESSES: 105, FAILURES: 11)
Failures: 11 (UNKNOWN: 0, LOW: 7, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'prometheus-nut-exporter' of Deployment 'prometheus-nut-exporter' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'prometheus-nut-exporter' of 'deployment' 'prometheus-nut-exporter' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'prometheus-nut-exporter' of Deployment 'prometheus-nut-exporter' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nut2.yaml:8-50
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: prometheus-nut-exporter
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: prometheus-nut-exporter
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nut2.yaml:8-50
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: prometheus-nut-exporter
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: prometheus-nut-exporter
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "prometheus-nut-exporter" of deployment "prometheus-nut-exporter" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment prometheus-nut-exporter in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nut2.yaml:4-6
────────────────────────────────────────
   4 ┌   name: prometheus-nut-exporter
   5 │   labels:
   6 └     app: prometheus-nut-exporter
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment prometheus-nut-exporter in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nut2.yaml:21-50
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 └             memory: 256Mi
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container prometheus-nut-exporter in deployment prometheus-nut-exporter (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nut2.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: prometheus-nut-exporter
  23 │         image: ghcr.io/druggeri/nut_exporter:3.1.3
  24 │         ports:
  25 │         - containerPort: 9199
  26 │           name: metrics
  27 │         resources:
  28 │           requests:
  29 │             memory: 256Mi
  30 └             cpu: 64m
  ..   
────────────────────────────────────────



nvidia-daemonset-preloaded.yaml (kubernetes)
============================================
Tests: 144 (SUCCESSES: 93, FAILURES: 51)
Failures: 51 (UNKNOWN: 0, LOW: 30, MEDIUM: 11, HIGH: 10, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'partition-gpus' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'partition-gpus' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "partition-gpus" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:77-114
────────────────────────────────────────
  77 ┌       - image: "cos-nvidia-installer:fixed"
  78 │         imagePullPolicy: Never
  79 │         name: nvidia-driver-installer
  80 │         resources:
  81 │           requests:
  82 │             cpu: 150m
  83 │         securityContext:
  84 │           privileged: true
  85 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:123-139
────────────────────────────────────────
 123 ┌       - image: "gcr.io/gke-release/nvidia-partition-gpu@sha256:e226275da6c45816959fe43cde907ee9a85c6a2aa8a429418a4cadef8ecdb86a"
 124 │         name: partition-gpus
 125 │         env:
 126 │           - name: LD_LIBRARY_PATH
 127 │             value: /usr/local/nvidia/lib64
 128 │         resources:
 129 │           requests:
 130 │             cpu: 150m
 131 └         securityContext:
 ...   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:141-142
────────────────────────────────────────
 141 ┌       - image: "gcr.io/google-containers/pause:2.0"
 142 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:43-142
────────────────────────────────────────
  43 ┌       priorityClassName: system-node-critical
  44 │       affinity:
  45 │         nodeAffinity:
  46 │           requiredDuringSchedulingIgnoredDuringExecution:
  47 │             nodeSelectorTerms:
  48 │             - matchExpressions:
  49 │               - key: cloud.google.com/gke-accelerator
  50 │                 operator: Exists
  51 └               - key: cloud.google.com/gke-gpu-driver-version
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-daemonset-preloaded.yaml:32-142
────────────────────────────────────────
  32 ┌   selector:
  33 │     matchLabels:
  34 │       k8s-app: nvidia-driver-installer
  35 │   updateStrategy:
  36 │     type: RollingUpdate
  37 │   template:
  38 │     metadata:
  39 │       labels:
  40 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────



nvidia-device-plugin1_1.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 101, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 9, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'nvidia-device-plugin' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:9-45
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: nvidia-device-plugin
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: nvidia-device-plugin
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:9-45
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: nvidia-device-plugin
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: nvidia-device-plugin
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:9-45
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: nvidia-device-plugin
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: nvidia-device-plugin
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-device-plugin-ctr" of deployment "nvidia-device-plugin" in "gpu-node" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nvidia-device-plugin in gpu-node namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:18-45
────────────────────────────────────────
  18 ┌       tolerations:
  19 │       - key: nvidia.com/gpu
  20 │         operator: Exists
  21 │         effect: NoSchedule
  22 │       priorityClassName: system-node-critical
  23 │       containers:
  24 │       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 └         name: nvidia-device-plugin-ctr
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-device-plugin-ctr in deployment nvidia-device-plugin (namespace: gpu-node) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-device-plugin1_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────



nvidia-device-plugin2.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 9, MEDIUM: 5, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-device-plugin-ctr' of DaemonSet 'nvidia-device-plugin-daemonset' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-device-plugin-daemonset' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-device-plugin2.yaml:21-63
────────────────────────────────────────
  21 ┌   selector:
  22 │     matchLabels:
  23 │       name: nvidia-device-plugin-ds
  24 │   updateStrategy:
  25 │     type: RollingUpdate
  26 │   template:
  27 │     metadata:
  28 │       labels:
  29 └         name: nvidia-device-plugin-ds
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-device-plugin-daemonset' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-device-plugin2.yaml:21-63
────────────────────────────────────────
  21 ┌   selector:
  22 │     matchLabels:
  23 │       name: nvidia-device-plugin-ds
  24 │   updateStrategy:
  25 │     type: RollingUpdate
  26 │   template:
  27 │     metadata:
  28 │       labels:
  29 └         name: nvidia-device-plugin-ds
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-device-plugin2.yaml:21-63
────────────────────────────────────────
  21 ┌   selector:
  22 │     matchLabels:
  23 │       name: nvidia-device-plugin-ds
  24 │   updateStrategy:
  25 │     type: RollingUpdate
  26 │   template:
  27 │     metadata:
  28 │       labels:
  29 └         name: nvidia-device-plugin-ds
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-device-plugin2.yaml:21-63
────────────────────────────────────────
  21 ┌   selector:
  22 │     matchLabels:
  23 │       name: nvidia-device-plugin-ds
  24 │   updateStrategy:
  25 │     type: RollingUpdate
  26 │   template:
  27 │     metadata:
  28 │       labels:
  29 └         name: nvidia-device-plugin-ds
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-device-plugin-ctr" of daemonset "nvidia-device-plugin-daemonset" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-device-plugin-daemonset in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-device-plugin2.yaml:31-63
────────────────────────────────────────
  31 ┌       tolerations:
  32 │       - key: nvidia.com/gpu
  33 │         operator: Exists
  34 │         effect: NoSchedule
  35 │       # Mark this pod as a critical add-on; when enabled, the critical add-on
  36 │       # scheduler reserves resources for critical add-on pods so that they can
  37 │       # be rescheduled after a failure.
  38 │       # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
  39 └       priorityClassName: "system-node-critical"
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-device-plugin-ctr in daemonset nvidia-device-plugin-daemonset (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-device-plugin2.yaml:41-59
────────────────────────────────────────
  41 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
  42 │         name: nvidia-device-plugin-ctr
  43 │         # This should not be needed with cdi
  44 │         # args:
  45 │         #   - "--device-discovery-strategy=tegra"
  46 │         env:
  47 │           - name: FAIL_ON_INIT_ERROR
  48 │             value: "false"
  49 └           - name: CUDA_VISIBLE_DEVICES
  ..   
────────────────────────────────────────



nvidia-device-plugin_1.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 101, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 9, MEDIUM: 4, HIGH: 2, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-device-plugin-ctr' of Deployment 'nvidia-device-plugin' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'nvidia-device-plugin' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:9-45
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: nvidia-device-plugin
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: nvidia-device-plugin
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:9-45
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: nvidia-device-plugin
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: nvidia-device-plugin
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:9-45
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: nvidia-device-plugin
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: nvidia-device-plugin
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-device-plugin-ctr" of deployment "nvidia-device-plugin" in "gpu-node" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nvidia-device-plugin in gpu-node namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:18-45
────────────────────────────────────────
  18 ┌       tolerations:
  19 │       - key: nvidia.com/gpu
  20 │         operator: Exists
  21 │         effect: NoSchedule
  22 │       priorityClassName: system-node-critical
  23 │       containers:
  24 │       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 └         name: nvidia-device-plugin-ctr
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-device-plugin-ctr in deployment nvidia-device-plugin (namespace: gpu-node) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-device-plugin_1.yaml:24-39
────────────────────────────────────────
  24 ┌       - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-ubi8
  25 │         imagePullPolicy: IfNotPresent
  26 │         name: nvidia-device-plugin-ctr
  27 │         env:
  28 │         - name: FAIL_ON_INIT_ERROR
  29 │           value: 'false'
  30 │         - name: DP_DISABLE_HEALTHCHECKS
  31 │           value: xids
  32 └         securityContext:
  ..   
────────────────────────────────────────



nvidia-driver-installer.yaml (kubernetes)
=========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer1.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer1.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer1.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer1.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer1.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer11.yaml (kubernetes)
===========================================
Tests: 130 (SUCCESSES: 93, FAILURES: 37)
Failures: 37 (UNKNOWN: 0, LOW: 21, MEDIUM: 8, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer11.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer@sha256:1cf2701dc2c3944a93fd06cb6c9eedfabf323425483ba3af294510621bb37d0e
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer11.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.4.1"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer11.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer11.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────



nvidia-driver-installer12.yaml (kubernetes)
===========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer12.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer12.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer12.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer12.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer13.yaml (kubernetes)
===========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer13.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer13.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer13.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer13.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer14.yaml (kubernetes)
===========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer14.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer14.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer14.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer14.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer15.yaml (kubernetes)
===========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer15.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer15.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer15.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer15.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer16.yaml (kubernetes)
===========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer16.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer16.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer16.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer16.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.10"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer2.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer2.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer2.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer2.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer2.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer3.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer3.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer3.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer3.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer3.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.8"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer4.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 93, FAILURES: 37)
Failures: 37 (UNKNOWN: 0, LOW: 21, MEDIUM: 8, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer4.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer4.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer4.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer4.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────



nvidia-driver-installer5.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer5.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer5.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer5.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer5.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer6.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer6.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.9
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer6.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer6.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer6.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer7.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 93, FAILURES: 37)
Failures: 37 (UNKNOWN: 0, LOW: 21, MEDIUM: 8, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer7.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.0.5
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer7.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "k8s.gcr.io/pause:3.7"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer7.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer7.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────



nvidia-driver-installer8.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer8.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer8.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer8.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer8.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-driver-installer9.yaml (kubernetes)
==========================================
Tests: 130 (SUCCESSES: 92, FAILURES: 38)
Failures: 38 (UNKNOWN: 0, LOW: 21, MEDIUM: 9, HIGH: 8, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pause' of 'daemonset' 'nvidia-driver-installer' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0009 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pause' of DaemonSet 'nvidia-driver-installer' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-driver-installer' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pause" of daemonset "nvidia-driver-installer" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer9.yaml:56-82
────────────────────────────────────────
  56 ┌       - image: gcr.io/cos-cloud/cos-gpu-installer:v2.1.10
  57 │         name: nvidia-driver-installer
  58 │         resources:
  59 │           requests:
  60 │             cpu: 0.15
  61 │         securityContext:
  62 │           privileged: true
  63 │         env:
  64 └           - name: NVIDIA_INSTALL_DIR_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-driver-installer in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-driver-installer in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-driver-installer9.yaml:26-85
────────────────────────────────────────
  26 ┌       affinity:
  27 │         nodeAffinity:
  28 │           requiredDuringSchedulingIgnoredDuringExecution:
  29 │             nodeSelectorTerms:
  30 │             - matchExpressions:
  31 │               - key: cloud.google.com/gke-accelerator
  32 │                 operator: Exists
  33 │       tolerations:
  34 └       - operator: "Exists"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-driver-installer in kube-system namespace shouldn't have volumes set to {"/", "/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-driver-installer9.yaml:15-85
────────────────────────────────────────
  15 ┌   selector:
  16 │     matchLabels:
  17 │       k8s-app: nvidia-driver-installer
  18 │   updateStrategy:
  19 │     type: RollingUpdate
  20 │   template:
  21 │     metadata:
  22 │       labels:
  23 └         name: nvidia-driver-installer
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pause in daemonset nvidia-driver-installer (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-driver-installer9.yaml:84-85
────────────────────────────────────────
  84 ┌       - image: "registry.k8s.io/pause:3.9"
  85 └         name: pause
────────────────────────────────────────



nvidia-gpu-driver.yaml (kubernetes)
===================================
Tests: 128 (SUCCESSES: 95, FAILURES: 33)
Failures: 33 (UNKNOWN: 0, LOW: 17, MEDIUM: 10, HIGH: 6, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-driver-installer' of 'daemonset' 'nvidia-gpu-device-plugin' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-gpu-device-plugin' of 'daemonset' 'nvidia-gpu-device-plugin' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-driver-installer' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-gpu-device-plugin' of DaemonSet 'nvidia-gpu-device-plugin' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'nvidia-gpu-device-plugin' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 nvidia-gpu-driver.yaml:9-69
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app.kubernetes.io/name: nvidia-gpu-device-plugin
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app.kubernetes.io/name: nvidia-gpu-device-plugin
  16 │     spec:
  17 └       priorityClassName: system-node-critical
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): DaemonSet 'nvidia-gpu-device-plugin' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 nvidia-gpu-driver.yaml:9-69
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app.kubernetes.io/name: nvidia-gpu-device-plugin
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app.kubernetes.io/name: nvidia-gpu-device-plugin
  16 │     spec:
  17 └       priorityClassName: system-node-critical
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-gpu-driver.yaml:9-69
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app.kubernetes.io/name: nvidia-gpu-device-plugin
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app.kubernetes.io/name: nvidia-gpu-device-plugin
  16 │     spec:
  17 └       priorityClassName: system-node-critical
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-gpu-driver.yaml:9-69
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app.kubernetes.io/name: nvidia-gpu-device-plugin
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app.kubernetes.io/name: nvidia-gpu-device-plugin
  16 │     spec:
  17 └       priorityClassName: system-node-critical
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-driver-installer" of daemonset "nvidia-gpu-device-plugin" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-gpu-device-plugin" of daemonset "nvidia-gpu-device-plugin" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset nvidia-gpu-device-plugin in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-gpu-driver.yaml:17-67
────────────────────────────────────────
  17 ┌       priorityClassName: system-node-critical
  18 │       nodeSelector:
  19 │         kubernetes.io/os: linux
  20 │         feature.node.kubernetes.io/pci-10de.present: "true"
  21 │       tolerations:
  22 │       - operator: "Exists"
  23 │         effect: "NoExecute"
  24 │       - operator: "Exists"
  25 └         effect: "NoSchedule"
  ..   
────────────────────────────────────────


AVD-KSV-0121 (HIGH): daemonset nvidia-gpu-device-plugin in kube-system namespace shouldn't have volumes set to {"/dev"}
════════════════════════════════════════
HostPath present many security risks and as a security practice it is better to avoid critical host paths mounts.

See https://avd.aquasec.com/misconfig/ksv121
────────────────────────────────────────
 nvidia-gpu-driver.yaml:9-69
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app.kubernetes.io/name: nvidia-gpu-device-plugin
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app.kubernetes.io/name: nvidia-gpu-device-plugin
  16 │     spec:
  17 └       priorityClassName: system-node-critical
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-driver-installer in daemonset nvidia-gpu-device-plugin (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-gpu-driver.yaml:37-49
────────────────────────────────────────
  37 ┌       - image: "virtuozzo/nvidia-driver-installer:39-6.6.13-200.fc39.x86_64-550.107.02"
  38 │         imagePullPolicy: IfNotPresent
  39 │         name: nvidia-driver-installer
  40 │         resources:
  41 │           requests:
  42 │             cpu: 0.15
  43 │         securityContext:
  44 │           privileged: true
  45 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-gpu-device-plugin in daemonset nvidia-gpu-device-plugin (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-gpu-driver.yaml:51-67
────────────────────────────────────────
  51 ┌       - image: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:ebd0d26fb21ae9c7d11da156737006ceff085029c697a3a163774188c3328dbe"
  52 │         command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia-driver"]
  53 │         name: nvidia-gpu-device-plugin
  54 │         resources:
  55 │           requests:
  56 │             cpu: 50m
  57 │             memory: 10Mi
  58 │           limits:
  59 └             cpu: 50m
  ..   
────────────────────────────────────────



nvidia-smi.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-smi' of 'pod' 'nvidia-smi' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-smi' of Pod 'nvidia-smi' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-smi.yaml:6-16
────────────────────────────────────────
   6 ┌   nodeSelector:
   7 │     nvidia.com/gpu.present: "true"
   8 │   runtimeClassName: nvidia
   9 │   restartPolicy: OnFailure
  10 │   containers:
  11 │     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 └       resources:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-smi.yaml:6-16
────────────────────────────────────────
   6 ┌   nodeSelector:
   7 │     nvidia.com/gpu.present: "true"
   8 │   runtimeClassName: nvidia
   9 │   restartPolicy: OnFailure
  10 │   containers:
  11 │     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 └       resources:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-smi" of pod "nvidia-smi" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nvidia-smi in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nvidia-smi.yaml:4
────────────────────────────────────────
   4 [   name: nvidia-smi
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-smi in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nvidia-smi in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-smi.yaml:6-16
────────────────────────────────────────
   6 ┌   nodeSelector:
   7 │     nvidia.com/gpu.present: "true"
   8 │   runtimeClassName: nvidia
   9 │   restartPolicy: OnFailure
  10 │   containers:
  11 │     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 └       resources:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-smi in pod nvidia-smi (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-smi.yaml:11-16
────────────────────────────────────────
  11 ┌     - name: nvidia-smi
  12 │       image: nvidia/cuda:12.1.0-base-ubuntu22.04
  13 │       command: ['sh', '-c', "nvidia-smi"]
  14 │       resources:
  15 │         limits:
  16 └             nvidia.com/gpu: "1"
────────────────────────────────────────



nvidia-smi1.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nvidia-smi' of 'job' 'nvidia-smi' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nvidia-smi' of Job 'nvidia-smi' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-smi1.yaml:6-20
────────────────────────────────────────
   6 ┌   template:
   7 │     spec:
   8 │       containers:
   9 │       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-smi1.yaml:6-20
────────────────────────────────────────
   6 ┌   template:
   7 │     spec:
   8 │       containers:
   9 │       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nvidia-smi" of job "nvidia-smi" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0110 (LOW): job nvidia-smi in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nvidia-smi1.yaml:4
────────────────────────────────────────
   4 [   name: nvidia-smi
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nvidia-smi in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────


AVD-KSV-0118 (HIGH): job nvidia-smi in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-smi1.yaml:8-19
────────────────────────────────────────
   8 ┌       containers:
   9 │       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 │             nvidia.com/gpu: 1
  15 │       tolerations:
  16 └       - key: nvidia.com/gpu
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nvidia-smi in job nvidia-smi (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-smi1.yaml:9-14
────────────────────────────────────────
   9 ┌       - name: nvidia-smi
  10 │         image: nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
  11 │         command: ["nvidia-smi"]
  12 │         resources:
  13 │           limits:
  14 └             nvidia.com/gpu: 1
────────────────────────────────────────



nvidia-test_1.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cuda-container' of 'pod' 'nbody-gpu-benchmark' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cuda-container' of Pod 'nbody-gpu-benchmark' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvidia-test_1.yaml:7-23
────────────────────────────────────────
   7 ┌   restartPolicy: OnFailure
   8 │   runtimeClassName: nvidia
   9 │   containers:
  10 │   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 └     - -benchmark
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvidia-test_1.yaml:7-23
────────────────────────────────────────
   7 ┌   restartPolicy: OnFailure
   8 │   runtimeClassName: nvidia
   9 │   containers:
  10 │   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 └     - -benchmark
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cuda-container" of pod "nbody-gpu-benchmark" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nbody-gpu-benchmark in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nvidia-test_1.yaml:4-5
────────────────────────────────────────
   4 ┌   name: nbody-gpu-benchmark
   5 └   namespace: default
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nbody-gpu-benchmark in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nbody-gpu-benchmark in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nvidia-test_1.yaml:7-23
────────────────────────────────────────
   7 ┌   restartPolicy: OnFailure
   8 │   runtimeClassName: nvidia
   9 │   containers:
  10 │   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 └     - -benchmark
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cuda-container in pod nbody-gpu-benchmark (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nvidia-test_1.yaml:10-23
────────────────────────────────────────
  10 ┌   - name: cuda-container
  11 │     image: nvcr.io/nvidia/k8s/cuda-sample:nbody
  12 │     args:
  13 │     - nbody
  14 │     - -gpu
  15 │     - -benchmark
  16 │     resources:
  17 │       limits:
  18 └         nvidia.com/gpu: 1
  ..   
────────────────────────────────────────



nvshare-system-quotas.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvshare-system-quotas.yaml:7-12
────────────────────────────────────────
   7 ┌   scopeSelector:
   8 │     matchExpressions:
   9 │     - operator: In
  10 │       scopeName: PriorityClass
  11 │       values:
  12 └       - system-cluster-critical
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvshare-system-quotas.yaml:7-12
────────────────────────────────────────
   7 ┌   scopeSelector:
   8 │     matchExpressions:
   9 │     - operator: In
  10 │       scopeName: PriorityClass
  11 │       values:
  12 └       - system-cluster-critical
────────────────────────────────────────



nvshare-system-quotas_1.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nvshare-system-quotas_1.yaml:7-12
────────────────────────────────────────
   7 ┌   scopeSelector:
   8 │     matchExpressions:
   9 │     - operator: In
  10 │       scopeName: PriorityClass
  11 │       values:
  12 └       - system-node-critical
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nvshare-system-quotas_1.yaml:7-12
────────────────────────────────────────
   7 ┌   scopeSelector:
   8 │     matchExpressions:
   9 │     - operator: In
  10 │       scopeName: PriorityClass
  11 │       values:
  12 └       - system-node-critical
────────────────────────────────────────



nwdaf-anlf-deployment.yaml (kubernetes)
=======================================
Tests: 128 (SUCCESSES: 97, FAILURES: 31)
Failures: 31 (UNKNOWN: 0, LOW: 19, MEDIUM: 7, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nwdaf-anlf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nwdaf-anlf' of Deployment 'free5gc-nwdaf-anlf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nwdaf-anlf' of 'deployment' 'free5gc-nwdaf-anlf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'wait-nrf' of 'deployment' 'free5gc-nwdaf-anlf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nwdaf-anlf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nwdaf-anlf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nwdaf-anlf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nwdaf-anlf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf-anlf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:9-63
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nwdaf
  13 │       name: anlf
  14 │   replicas: 1
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:9-63
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nwdaf
  13 │       name: anlf
  14 │   replicas: 1
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nwdaf-anlf" of deployment "free5gc-nwdaf-anlf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "wait-nrf" of deployment "free5gc-nwdaf-anlf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment free5gc-nwdaf-anlf in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:4-7
────────────────────────────────────────
   4 ┌   name: free5gc-nwdaf-anlf
   5 │   labels:
   6 │     app: free5gc
   7 └     nf: nwdaf
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nwdaf-anlf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nwdaf-anlf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment free5gc-nwdaf-anlf in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:22-63
────────────────────────────────────────
  22 ┌       initContainers:
  23 │         - name: wait-nrf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 └               "sh",
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nwdaf-anlf in deployment free5gc-nwdaf-anlf (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nwdaf-anlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-anlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-anlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8080
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafanlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────



nwdaf-anlf-service.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwdaf-anlf-service.yaml:10-20
────────────────────────────────────────
  10 ┌   type: NodePort
  11 │   ports:
  12 │     - name: nwdaf
  13 │       port: 8000
  14 │     - name: openapi 
  15 │       port: 8080
  16 │       nodePort: 30080
  17 │   selector:
  18 └     app: free5gc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwdaf-anlf-service.yaml:10-20
────────────────────────────────────────
  10 ┌   type: NodePort
  11 │   ports:
  12 │     - name: nwdaf
  13 │       port: 8000
  14 │     - name: openapi 
  15 │       port: 8080
  16 │       nodePort: 30080
  17 │   selector:
  18 └     app: free5gc
  ..   
────────────────────────────────────────



nwdaf-configmap.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'nwdaf-configmap' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"      key"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



nwdaf-deployment.yaml (kubernetes)
==================================
Tests: 128 (SUCCESSES: 97, FAILURES: 31)
Failures: 31 (UNKNOWN: 0, LOW: 19, MEDIUM: 7, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nwdaf' of Deployment 'free5gc-nwdaf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nwdaf' of Deployment 'free5gc-nwdaf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nwdaf' of 'deployment' 'free5gc-nwdaf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'wait-nrf' of 'deployment' 'free5gc-nwdaf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nwdaf' of Deployment 'free5gc-nwdaf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nwdaf' of Deployment 'free5gc-nwdaf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nwdaf' of Deployment 'free5gc-nwdaf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nwdaf' of Deployment 'free5gc-nwdaf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'wait-nrf' of Deployment 'free5gc-nwdaf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwdaf-deployment.yaml:9-59
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nwdaf
  13 │   replicas: 1
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 └         app: free5gc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwdaf-deployment.yaml:9-59
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nwdaf
  13 │   replicas: 1
  14 │   template:
  15 │     metadata:
  16 │       labels:
  17 └         app: free5gc
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nwdaf" of deployment "free5gc-nwdaf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "wait-nrf" of deployment "free5gc-nwdaf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment free5gc-nwdaf in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nwdaf-deployment.yaml:4-7
────────────────────────────────────────
   4 ┌   name: free5gc-nwdaf
   5 │   labels:
   6 │     app: free5gc
   7 └     nf: nwdaf
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nwdaf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-deployment.yaml:21-30
────────────────────────────────────────
  21 ┌         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 │               "sh",
  29 │               "-c",
  30 └               "until nc -z $DEPENDENCIES; do echo waiting for the SMF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nwdaf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment free5gc-nwdaf in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-deployment.yaml:20-59
────────────────────────────────────────
  20 ┌       initContainers:
  21 │         - name: wait-nrf
  22 │           image: busybox:1.32.0
  23 │           env:
  24 │             - name: DEPENDENCIES
  25 │               value: pcf-npcf:8000
  26 │           command:
  27 │             [
  28 └               "sh",
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nwdaf in deployment free5gc-nwdaf (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nwdaf-deployment.yaml:33-51
────────────────────────────────────────
  33 ┌         - image: edierbra/free5gc:v3.2.1
  34 │           name: nwdaf
  35 │           ports:
  36 │             - containerPort: 29531
  37 │           command: ["./nwdaf"]
  38 │           args: ["--config", "config/nwdafcfg.yaml"]
  39 │           env:
  40 │             - name: GIN_MODE
  41 └               value: release
  ..   
────────────────────────────────────────



nwdaf-mtlf-deployment.yaml (kubernetes)
=======================================
Tests: 128 (SUCCESSES: 97, FAILURES: 31)
Failures: 31 (UNKNOWN: 0, LOW: 19, MEDIUM: 7, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nwdaf-mtlf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nwdaf-mtlf' of Deployment 'free5gc-nwdaf-mtlf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nwdaf-mtlf' of 'deployment' 'free5gc-nwdaf-mtlf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'wait-pcf' of 'deployment' 'free5gc-nwdaf-mtlf' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nwdaf-mtlf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nwdaf-mtlf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nwdaf-mtlf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nwdaf-mtlf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'wait-pcf' of Deployment 'free5gc-nwdaf-mtlf' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:9-63
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nwdaf
  13 │       name: mtlf
  14 │   replicas: 1
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:9-63
────────────────────────────────────────
   9 ┌   selector:
  10 │     matchLabels:
  11 │       app: free5gc
  12 │       nf: nwdaf
  13 │       name: mtlf
  14 │   replicas: 1
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nwdaf-mtlf" of deployment "free5gc-nwdaf-mtlf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "wait-pcf" of deployment "free5gc-nwdaf-mtlf" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment free5gc-nwdaf-mtlf in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:4-7
────────────────────────────────────────
   4 ┌   name: free5gc-nwdaf-mtlf
   5 │   labels:
   6 │     app: free5gc
   7 └     nf: nwdaf
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nwdaf-mtlf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:23-32
────────────────────────────────────────
  23 ┌         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 │               "sh",
  31 │               "-c",
  32 └               "until nc -z $DEPENDENCIES; do echo waiting for the PCF; sleep 2; done;",
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container free5gc-nwdaf-mtlf in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment free5gc-nwdaf-mtlf in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:22-63
────────────────────────────────────────
  22 ┌       initContainers:
  23 │         - name: wait-pcf
  24 │           image: busybox:1.32.0
  25 │           env:
  26 │             - name: DEPENDENCIES
  27 │               value: pcf-npcf:8000
  28 │           command:
  29 │             [
  30 └               "sh",
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nwdaf-mtlf in deployment free5gc-nwdaf-mtlf (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nwdaf-mtlf-deployment.yaml:35-55
────────────────────────────────────────
  35 ┌         - image: edierbra/nwdaf-mtlf:v3.0.5 # edierbra/free5gc:v3.2.2
  36 │           imagePullPolicy: IfNotPresent
  37 │           name: nwdaf-mtlf
  38 │           ports:
  39 │             - containerPort: 8000
  40 │             - containerPort: 8081
  41 │           command: ["/bin/bash", "-c"]
  42 │           args: ["cd /free5gc/nwdafmtlf && python3 -m openapi_server"]
  43 └           env:
  ..   
────────────────────────────────────────



nwdaf-mtlf-service.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwdaf-mtlf-service.yaml:10-20
────────────────────────────────────────
  10 ┌   type: NodePort
  11 │   ports:
  12 │     - name: mtlf
  13 │       port: 8000
  14 │     - name: openapi 
  15 │       port: 8081
  16 │       nodePort: 30081
  17 │   selector:
  18 └     app: free5gc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwdaf-mtlf-service.yaml:10-20
────────────────────────────────────────
  10 ┌   type: NodePort
  11 │   ports:
  12 │     - name: mtlf
  13 │       port: 8000
  14 │     - name: openapi 
  15 │       port: 8081
  16 │       nodePort: 30081
  17 │   selector:
  18 └     app: free5gc
  ..   
────────────────────────────────────────



nwdaf-service.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwdaf-service.yaml:9-13
────────────────────────────────────────
   9 ┌   ports:
  10 │     - port: 29531
  11 │   selector:
  12 │     app: free5gc
  13 └     nf: nwdaf
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwdaf-service.yaml:9-13
────────────────────────────────────────
   9 ┌   ports:
  10 │     - port: 29531
  11 │   selector:
  12 │     app: free5gc
  13 └     nf: nwdaf
────────────────────────────────────────



nwp.yaml (kubernetes)
=====================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwp.yaml:7-10
────────────────────────────────────────
   7 ┌   podSelector: {}
   8 │   ingress:
   9 │   - from:
  10 └     - podSelector: {}
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwp.yaml:7-10
────────────────────────────────────────
   7 ┌   podSelector: {}
   8 │   ingress:
   9 │   - from:
  10 └     - podSelector: {}
────────────────────────────────────────



nwujobs-app.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 95, FAILURES: 22)
Failures: 22 (UNKNOWN: 0, LOW: 13, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nwujobs' of Deployment 'nwujobs' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nwujobs' of 'deployment' 'nwujobs' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nwujobs' of Deployment 'nwujobs' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'nwujobs' of Deployment 'nwujobs' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nwujobs' of Deployment 'nwujobs' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nwujobs' of Deployment 'nwujobs' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwujobs-app.yaml:8-21
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: nwujobs
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: nwujobs
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwujobs-app.yaml:8-21
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: nwujobs
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: nwujobs
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nwujobs" of deployment "nwujobs" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nwujobs in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nwujobs-app.yaml:4-6
────────────────────────────────────────
   4 ┌   name: nwujobs
   5 │   labels:
   6 └     app: nwujobs
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nwujobs in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nwujobs in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nwujobs in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nwujobs-app.yaml:17-21
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nwujobs in deployment nwujobs (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nwujobs-app.yaml:18-21
────────────────────────────────────────
  18 ┌       - name: nwujobs
  19 │         image: bjnandi/nwujobs-nwu-final-year-project:latest
  20 │         ports:
  21 └         - containerPort: 80
────────────────────────────────────────



nwujobs-app_1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwujobs-app_1.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: nwujobs
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 80
  11 └     targetPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwujobs-app_1.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: nwujobs
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 80
  11 └     targetPort: 80
────────────────────────────────────────



nwujobs-app_2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nwujobs-app_2.yaml:8-19
────────────────────────────────────────
   8 ┌   ingressClassName: nginx
   9 │   rules:
  10 │   - host: nwujobs.bjtechlife.com
  11 │     http:
  12 │       paths:
  13 │       - path: /
  14 │         pathType: Prefix
  15 │         backend:
  16 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nwujobs-app_2.yaml:8-19
────────────────────────────────────────
   8 ┌   ingressClassName: nginx
   9 │   rules:
  10 │   - host: nwujobs.bjtechlife.com
  11 │     http:
  12 │       paths:
  13 │       - path: /
  14 │         pathType: Prefix
  15 │         backend:
  16 └           service:
  ..   
────────────────────────────────────────



nydus.yaml (kubernetes)
=======================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Pod 'nydus-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Pod 'nydus-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'pod' 'nydus-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Pod 'nydus-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Pod 'nydus-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Pod 'nydus-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Pod 'nydus-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Pod 'nydus-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Pod 'nydus-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Pod 'nydus-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Pod 'nydus-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nydus.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nydus.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of pod "nydus-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nydus-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 nydus.yaml:4
────────────────────────────────────────
   4 [   name: nydus-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nydus-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nydus-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 nydus.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nginx in pod nydus-pod (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 nydus.yaml:7-12
────────────────────────────────────────
   7 ┌     - name: nginx
   8 │       image: ghcr.io/dragonflyoss/image-service/nginx:nydus-latest
   9 │       imagePullPolicy: Always
  10 │       command: ["sh", "-c"]
  11 │       args:
  12 └         - tail -f /dev/null
────────────────────────────────────────



nzb-ingress.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 113, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 nzb-ingress.yaml:7-11
────────────────────────────────────────
   7 ┌   type: ExternalName
   8 │   externalName: cyan.rainbow.tuckerthomas.com
   9 │   ports:
  10 │   - port: 1080
  11 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 nzb-ingress.yaml:7-11
────────────────────────────────────────
   7 ┌   type: ExternalName
   8 │   externalName: cyan.rainbow.tuckerthomas.com
   9 │   ports:
  10 │   - port: 1080
  11 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0108 (HIGH): Service 'nzb' in 'apps' namespace should not set external IPs or external Name
════════════════════════════════════════
Services with external IP addresses allows direct access from the internet and might expose risk for CVE-2020-8554

See https://avd.aquasec.com/misconfig/avd-ksv-0108
────────────────────────────────────────



oakestra-agent.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 12, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oakestra-agent' of 'deployment' 'oakestra-agent' in 'oakestra-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oakestra-agent' of Deployment 'oakestra-agent' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oakestra-agent' of Deployment 'oakestra-agent' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oakestra-agent.yaml:7-44
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oakestra-agent
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oakestra-agent
  15 └         plugin: oakestra
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oakestra-agent.yaml:7-44
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oakestra-agent
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oakestra-agent
  15 └         plugin: oakestra
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oakestra-agent" of deployment "oakestra-agent" in "oakestra-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment oakestra-agent in oakestra-system namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oakestra-agent in oakestra-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oakestra-agent in oakestra-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oakestra-agent.yaml:17-44
────────────────────────────────────────
  17 ┌       serviceAccountName: oakestra-agent
  18 │       containers:
  19 │       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 └         - name: ROOT_SYSTEM_MANAGER_IP
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oakestra-agent in deployment oakestra-agent (namespace: oakestra-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oakestra-agent.yaml:19-44
────────────────────────────────────────
  19 ┌       - name: oakestra-agent
  20 │         image: ghcr.io/oakestra/plugin-kubernetes/plugin-kubernetes/oakestra-agent:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 80
  24 │         env:
  25 │         - name: ROOT_SYSTEM_MANAGER_IP
  26 │           value: 10.100.253.57
  27 └         - name: ROOT_SYSTEM_MANAGER_PORT
  ..   
────────────────────────────────────────



oakestra-agent_1.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oakestra-agent_1.yaml:7-14
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oakestra-agent
   9 │   type: NodePort
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 10100
  13 │     targetPort: 10100
  14 └     nodePort: 30000
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oakestra-agent_1.yaml:7-14
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oakestra-agent
   9 │   type: NodePort
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 10100
  13 │     targetPort: 10100
  14 └     nodePort: 30000
────────────────────────────────────────



oakestra-agent_6.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): ClusterRole 'oakestra-agent' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 oakestra-agent_6.yaml:30-39
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ''
  32 │   resources:
  33 │   - configmaps
  34 │   verbs:
  35 │   - get
  36 │   - list
  37 │   - create
  38 │   - update
  39 └   - delete
────────────────────────────────────────



oakestra-cluster-service-manager.yaml (kubernetes)
==================================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cluster-service-manager' of 'deployment' 'cluster-netmanager' in 'oakestra-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cluster-service-manager' of Deployment 'cluster-netmanager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:7-50
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: cluster
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: cluster
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:7-50
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: cluster
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: cluster
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cluster-service-manager" of deployment "cluster-netmanager" in "oakestra-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cluster-netmanager in oakestra-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment cluster-netmanager in oakestra-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:16-50
────────────────────────────────────────
  16 ┌       affinity:
  17 │         podAffinity:
  18 │           requiredDuringSchedulingIgnoredDuringExecution:
  19 │           - labelSelector:
  20 │               matchExpressions:
  21 │               - key: plugin
  22 │                 operator: In
  23 │                 values:
  24 └                 - oakestra
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cluster-service-manager in deployment cluster-netmanager (namespace: oakestra-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oakestra-cluster-service-manager.yaml:27-50
────────────────────────────────────────
  27 ┌       - name: cluster-service-manager
  28 │         image: ghcr.io/jakobke/oakestra-cluster-service-manager:0.2-amd
  29 │         imagePullPolicy: Always
  30 │         ports:
  31 │         - containerPort: 10110
  32 │         env:
  33 │         - name: MY_PORT
  34 │           value: '10110'
  35 └         - name: MQTT_BROKER_PORT
  ..   
────────────────────────────────────────



oakestra-cluster-service-manager_1.yaml (kubernetes)
====================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oakestra-cluster-service-manager_1.yaml:7-14
────────────────────────────────────────
   7 ┌   type: NodePort
   8 │   selector:
   9 │     app: cluster
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 10110
  13 │     targetPort: 10110
  14 └     nodePort: 30330
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oakestra-cluster-service-manager_1.yaml:7-14
────────────────────────────────────────
   7 ┌   type: NodePort
   8 │   selector:
   9 │     app: cluster
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 10110
  13 │     targetPort: 10110
  14 └     nodePort: 30330
────────────────────────────────────────



oauth-proxy.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth2-proxy' of 'deployment' 'oauth2-proxy' in 'auth-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy.yaml:9-38
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy.yaml:9-38
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth2-proxy" of deployment "oauth2-proxy" in "auth-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in auth-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in auth-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth-proxy.yaml:18-38
────────────────────────────────────────
  18 ┌       containers:
  19 │       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 └         - --cookie-secure=false
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth2-proxy in deployment oauth2-proxy (namespace: auth-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth-proxy.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────



oauth-proxy1.yaml (kubernetes)
==============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth2-proxy' of 'deployment' 'oauth2-proxy' in 'auth-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy1.yaml:9-38
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy1.yaml:9-38
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth2-proxy" of deployment "oauth2-proxy" in "auth-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in auth-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in auth-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth-proxy1.yaml:18-38
────────────────────────────────────────
  18 ┌       containers:
  19 │       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 └         - --cookie-secure=false
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth2-proxy in deployment oauth2-proxy (namespace: auth-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth-proxy1.yaml:19-38
────────────────────────────────────────
  19 ┌       - name: oauth2-proxy
  20 │         image: quay.io/oauth2-proxy/oauth2-proxy:latest
  21 │         imagePullPolicy: Always
  22 │         ports:
  23 │         - containerPort: 4180
  24 │           protocol: TCP
  25 │         args:
  26 │         - --cookie-secure=false
  27 └         - --provider=oidc
  ..   
────────────────────────────────────────



oauth-proxy1_1.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy1_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy1_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth-proxy1_2.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy1_2.yaml:10-25
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   tls:
  12 │   - secretName: k8s-dashboard-external-tls
  13 │     hosts:
  14 │     - dashboard.172.30.2.2.nip.io
  15 │   rules:
  16 │   - host: dashboard.172.30.2.2.nip.io
  17 │     http:
  18 └       paths:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy1_2.yaml:10-25
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   tls:
  12 │   - secretName: k8s-dashboard-external-tls
  13 │     hosts:
  14 │     - dashboard.172.30.2.2.nip.io
  15 │   rules:
  16 │   - host: dashboard.172.30.2.2.nip.io
  17 │     http:
  18 └       paths:
  ..   
────────────────────────────────────────



oauth-proxy1_3.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy1_3.yaml:12-23
────────────────────────────────────────
  12 ┌   ingressClassName: nginx
  13 │   rules:
  14 │   - host: dashboard.172.30.2.2.nip.io
  15 │     http:
  16 │       paths:
  17 │       - path: /
  18 │         pathType: Prefix
  19 │         backend:
  20 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy1_3.yaml:12-23
────────────────────────────────────────
  12 ┌   ingressClassName: nginx
  13 │   rules:
  14 │   - host: dashboard.172.30.2.2.nip.io
  15 │     http:
  16 │       paths:
  17 │       - path: /
  18 │         pathType: Prefix
  19 │         backend:
  20 └           service:
  ..   
────────────────────────────────────────



oauth-proxy_1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth-proxy_2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy_2.yaml:10-25
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   tls:
  12 │   - secretName: k8s-dashboard-external-tls
  13 │     hosts:
  14 │     - dashboard.172.30.2.2.nip.io
  15 │   rules:
  16 │   - host: dashboard.172.30.2.2.nip.io
  17 │     http:
  18 └       paths:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy_2.yaml:10-25
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   tls:
  12 │   - secretName: k8s-dashboard-external-tls
  13 │     hosts:
  14 │     - dashboard.172.30.2.2.nip.io
  15 │   rules:
  16 │   - host: dashboard.172.30.2.2.nip.io
  17 │     http:
  18 └       paths:
  ..   
────────────────────────────────────────



oauth-proxy_3.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth-proxy_3.yaml:12-23
────────────────────────────────────────
  12 ┌   ingressClassName: nginx
  13 │   rules:
  14 │   - host: dashboard.172.30.2.2.nip.io
  15 │     http:
  16 │       paths:
  17 │       - path: /
  18 │         pathType: Prefix
  19 │         backend:
  20 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth-proxy_3.yaml:12-23
────────────────────────────────────────
  12 ┌   ingressClassName: nginx
  13 │   rules:
  14 │   - host: dashboard.172.30.2.2.nip.io
  15 │     http:
  16 │       paths:
  17 │       - path: /
  18 │         pathType: Prefix
  19 │         backend:
  20 └           service:
  ..   
────────────────────────────────────────



oauth.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'kube-dashboard-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth1.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'stg-kube-dashboard-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth10.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'longhorn-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth11.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'longhorn-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth12.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'open-webui-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth13.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'open-webui-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth14.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'alertmanager-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth14_4.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'prometheus-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth15.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'alertmanager-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth15_4.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'prometheus-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth2-proxy.yaml (kubernetes)
==============================
Tests: 117 (SUCCESSES: 101, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 10, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'redis' of Deployment 'redis' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'redis' of Deployment 'redis' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'redis' of 'deployment' 'redis' in 'security' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'redis' of Deployment 'redis' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'redis' of Deployment 'redis' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy.yaml:7-31
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       app: redis
  10 │   replicas: 1
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: redis
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy.yaml:7-31
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       app: redis
  10 │   replicas: 1
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: redis
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "redis" of deployment "redis" in "security" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container redis in security namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment redis in security namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy.yaml:16-31
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 └             memory: 512Mi
  ..   
────────────────────────────────────────



oauth2-proxy1.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 101, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 10, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'redis' of Deployment 'redis' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'redis' of Deployment 'redis' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'redis' of 'deployment' 'redis' in 'security' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'redis' of Deployment 'redis' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'redis' of Deployment 'redis' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis' of Deployment 'redis' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis' of Deployment 'redis' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis' of Deployment 'redis' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis' of Deployment 'redis' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy1.yaml:7-31
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       app: redis
  10 │   replicas: 1
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: redis
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy1.yaml:7-31
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       app: redis
  10 │   replicas: 1
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: redis
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "redis" of deployment "redis" in "security" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container redis in security namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy1.yaml:17-31
────────────────────────────────────────
  17 ┌       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 │             memory: 512Mi
  25 └         command:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment redis in security namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy1.yaml:16-31
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: redis
  18 │         image: redis:7.2.4
  19 │         ports:
  20 │         - containerPort: 6379
  21 │         resources:
  22 │           limits:
  23 │             cpu: '0.5'
  24 └             memory: 512Mi
  ..   
────────────────────────────────────────



oauth2-proxy1_1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy1_1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: redis
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 6379
  12 └     targetPort: 6379
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy1_1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: redis
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 6379
  12 └     targetPort: 6379
────────────────────────────────────────



oauth2-proxy1_2.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0109 (HIGH): ConfigMap 'oauth2-proxy-config' in 'security' namespace stores secrets in key(s) or value(s) '{"client_secret", "cookie_secret", "pass_access_token"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



oauth2-proxy1_3.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth-proxy' of 'deployment' 'oauth2-proxy' in 'security' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0007 (LOW): 'deployment' 'oauth2-proxy' in 'security' namespace should not set spec.template.spec.hostAliases
════════════════════════════════════════
Managing /etc/hosts aliases can prevent the container engine from modifying the file after a pod’s containers have already been started.

See https://avd.aquasec.com/misconfig/ksv007
────────────────────────────────────────
 oauth2-proxy1_3.yaml:18-46
────────────────────────────────────────
  18 ┌       volumes:
  19 │       - name: oauth2-proxy-config
  20 │         configMap:
  21 │           name: oauth2-proxy-config
  22 │       hostAliases:
  23 │       - ip: 172.19.1.100
  24 │         hostnames:
  25 │         - k2s.cluster.local
  26 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy1_3.yaml:9-46
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy1_3.yaml:9-46
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth-proxy" of deployment "oauth2-proxy" in "security" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in security namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in security namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy1_3.yaml:18-46
────────────────────────────────────────
  18 ┌       volumes:
  19 │       - name: oauth2-proxy-config
  20 │         configMap:
  21 │           name: oauth2-proxy-config
  22 │       hostAliases:
  23 │       - ip: 172.19.1.100
  24 │         hostnames:
  25 │         - k2s.cluster.local
  26 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth-proxy in deployment oauth2-proxy (namespace: security) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth2-proxy1_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────



oauth2-proxy1_4.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy1_4.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy1_4.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth2-proxy2.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 12, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth2-proxy' of 'deployment' 'oauth2-proxy' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'oauth2-proxy' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 oauth2-proxy2.yaml:9-36
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy2.yaml:9-36
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy2.yaml:9-36
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth2-proxy" of deployment "oauth2-proxy" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy2.yaml:18-36
────────────────────────────────────────
  18 ┌       containers:
  19 │       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: <Client ID>
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth2-proxy in deployment oauth2-proxy (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth2-proxy2.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────



oauth2-proxy2_1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 113, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 2, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0037 (MEDIUM): Service 'oauth2-proxy' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 oauth2-proxy2_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy2_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy2_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth2-proxy4.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 12, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth2-proxy' of 'deployment' 'oauth2-proxy' in 'kube-system' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0037 (MEDIUM): Deployment 'oauth2-proxy' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 oauth2-proxy4.yaml:9-36
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy4.yaml:9-36
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy4.yaml:9-36
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth2-proxy" of deployment "oauth2-proxy" in "kube-system" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in kube-system namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in kube-system namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy4.yaml:18-36
────────────────────────────────────────
  18 ┌       containers:
  19 │       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: <Client ID>
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth2-proxy in deployment oauth2-proxy (namespace: kube-system) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth2-proxy4.yaml:19-36
────────────────────────────────────────
  19 ┌       - args:
  20 │         - --provider=github
  21 │         - --email-domain=*
  22 │         - --upstream=file:///dev/null
  23 │         - --http-address=0.0.0.0:4180
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 │           value: <Client ID>
  27 └         - name: OAUTH2_PROXY_CLIENT_SECRET
  ..   
────────────────────────────────────────



oauth2-proxy4_1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 113, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 2, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0037 (MEDIUM): Service 'oauth2-proxy' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 oauth2-proxy4_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy4_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy4_1.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth2-proxy4_2.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 113, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 2, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0037 (MEDIUM): Ingress 'oauth2-proxy' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 oauth2-proxy4_2.yaml:7-22
────────────────────────────────────────
   7 ┌   ingressClassName: nginx
   8 │   rules:
   9 │   - host: __INGRESS_HOST__
  10 │     http:
  11 │       paths:
  12 │       - path: /oauth2
  13 │         pathType: Prefix
  14 │         backend:
  15 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy4_2.yaml:7-22
────────────────────────────────────────
   7 ┌   ingressClassName: nginx
   8 │   rules:
   9 │   - host: __INGRESS_HOST__
  10 │     http:
  11 │       paths:
  12 │       - path: /oauth2
  13 │         pathType: Prefix
  14 │         backend:
  15 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy4_2.yaml:7-22
────────────────────────────────────────
   7 ┌   ingressClassName: nginx
   8 │   rules:
   9 │   - host: __INGRESS_HOST__
  10 │     http:
  11 │       paths:
  12 │       - path: /oauth2
  13 │         pathType: Prefix
  14 │         backend:
  15 └           service:
  ..   
────────────────────────────────────────



oauth2-proxy4_3.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 113, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 2, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0037 (MEDIUM): Ingress 'external-auth-oauth2' should not be set with 'kube-system' namespace
════════════════════════════════════════
ensure that user resources are not placed in kube-system namespace

See https://avd.aquasec.com/misconfig/no-user-pods-in-system-namespace
────────────────────────────────────────
 oauth2-proxy4_3.yaml:10-21
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   rules:
  12 │   - host: __INGRESS_HOST__
  13 │     http:
  14 │       paths:
  15 │       - path: /
  16 │         pathType: Prefix
  17 │         backend:
  18 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy4_3.yaml:10-21
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   rules:
  12 │   - host: __INGRESS_HOST__
  13 │     http:
  14 │       paths:
  15 │       - path: /
  16 │         pathType: Prefix
  17 │         backend:
  18 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy4_3.yaml:10-21
────────────────────────────────────────
  10 ┌   ingressClassName: nginx
  11 │   rules:
  12 │   - host: __INGRESS_HOST__
  13 │     http:
  14 │       paths:
  15 │       - path: /
  16 │         pathType: Prefix
  17 │         backend:
  18 └           service:
  ..   
────────────────────────────────────────



oauth2-proxy5.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 9, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth2-proxy' of 'deployment' 'oauth2-proxy' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth2-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy5.yaml:8-43
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       k8s-app: oauth2-proxy
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         k8s-app: oauth2-proxy
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy5.yaml:8-43
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       k8s-app: oauth2-proxy
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         k8s-app: oauth2-proxy
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth2-proxy" of deployment "oauth2-proxy" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment oauth2-proxy in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oauth2-proxy5.yaml:4-6
────────────────────────────────────────
   4 ┌   labels:
   5 │     k8s-app: oauth2-proxy
   6 └   name: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy5.yaml:17-43
────────────────────────────────────────
  17 ┌       containers:
  18 │       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 └         - name: OAUTH2_PROXY_CLIENT_ID
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth2-proxy in deployment oauth2-proxy (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth2-proxy5.yaml:18-43
────────────────────────────────────────
  18 ┌       - args:
  19 │         - --provider=google
  20 │         - --email-domain=elastisys.com
  21 │         - --upstream=file:///dev/null
  22 │         - --http-address=0.0.0.0:4180
  23 │         - --set-authorization-header
  24 │         env:
  25 │         - name: OAUTH2_PROXY_CLIENT_ID
  26 └           value: i-didnt-read-the-docs-above
  ..   
────────────────────────────────────────



oauth2-proxy5_1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy5_1.yaml:8-14
────────────────────────────────────────
   8 ┌   ports:
   9 │   - name: http
  10 │     port: 4180
  11 │     protocol: TCP
  12 │     targetPort: 4180
  13 │   selector:
  14 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy5_1.yaml:8-14
────────────────────────────────────────
   8 ┌   ports:
   9 │   - name: http
  10 │     port: 4180
  11 │     protocol: TCP
  12 │     targetPort: 4180
  13 │   selector:
  14 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth2-proxy5_2.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy5_2.yaml:6-21
────────────────────────────────────────
   6 ┌   ingressClassName: nginx
   7 │   rules:
   8 │   - host: __INGRESS_HOST__
   9 │     http:
  10 │       paths:
  11 │       - path: /oauth2
  12 │         pathType: Prefix
  13 │         backend:
  14 └           service:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy5_2.yaml:6-21
────────────────────────────────────────
   6 ┌   ingressClassName: nginx
   7 │   rules:
   8 │   - host: __INGRESS_HOST__
   9 │     http:
  10 │       paths:
  11 │       - path: /oauth2
  12 │         pathType: Prefix
  13 │         backend:
  14 └           service:
  ..   
────────────────────────────────────────



oauth2-proxy_1.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy_1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: redis
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 6379
  12 └     targetPort: 6379
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy_1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: redis
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 6379
  12 └     targetPort: 6379
────────────────────────────────────────



oauth2-proxy_2.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0109 (HIGH): ConfigMap 'oauth2-proxy-config' in 'security' namespace stores secrets in key(s) or value(s) '{"client_secret", "cookie_secret", "pass_access_token"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



oauth2-proxy_3.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oauth-proxy' of 'deployment' 'oauth2-proxy' in 'security' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0007 (LOW): 'deployment' 'oauth2-proxy' in 'security' namespace should not set spec.template.spec.hostAliases
════════════════════════════════════════
Managing /etc/hosts aliases can prevent the container engine from modifying the file after a pod’s containers have already been started.

See https://avd.aquasec.com/misconfig/ksv007
────────────────────────────────────────
 oauth2-proxy_3.yaml:18-46
────────────────────────────────────────
  18 ┌       volumes:
  19 │       - name: oauth2-proxy-config
  20 │         configMap:
  21 │           name: oauth2-proxy-config
  22 │       hostAliases:
  23 │       - ip: 172.19.1.100
  24 │         hostnames:
  25 │         - k2s.cluster.local
  26 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oauth-proxy' of Deployment 'oauth2-proxy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy_3.yaml:9-46
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy_3.yaml:9-46
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       k8s-app: oauth2-proxy
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: oauth2-proxy
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oauth-proxy" of deployment "oauth2-proxy" in "security" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oauth2-proxy in security namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oauth2-proxy in security namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oauth2-proxy_3.yaml:18-46
────────────────────────────────────────
  18 ┌       volumes:
  19 │       - name: oauth2-proxy-config
  20 │         configMap:
  21 │           name: oauth2-proxy-config
  22 │       hostAliases:
  23 │       - ip: 172.19.1.100
  24 │         hostnames:
  25 │         - k2s.cluster.local
  26 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oauth-proxy in deployment oauth2-proxy (namespace: security) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oauth2-proxy_3.yaml:27-44
────────────────────────────────────────
  27 ┌       - name: oauth-proxy
  28 │         image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
  29 │         ports:
  30 │         - containerPort: 4180
  31 │         volumeMounts:
  32 │         - name: oauth2-proxy-config
  33 │           mountPath: /etc/oauth2-proxy.cfg
  34 │           subPath: oauth2-proxy.cfg
  35 └         args:
  ..   
────────────────────────────────────────



oauth2-proxy_4.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-proxy_4.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-proxy_4.yaml:9-15
────────────────────────────────────────
   9 ┌   ports:
  10 │   - name: http
  11 │     port: 4180
  12 │     protocol: TCP
  13 │     targetPort: 4180
  14 │   selector:
  15 └     k8s-app: oauth2-proxy
────────────────────────────────────────



oauth2-route.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 113, FAILURES: 3)
Failures: 3 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oauth2-route.yaml:6-7
────────────────────────────────────────
   6 ┌   type: ExternalName
   7 └   externalName: auth-proxy.internal.paulfriedrich.me
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oauth2-route.yaml:6-7
────────────────────────────────────────
   6 ┌   type: ExternalName
   7 └   externalName: auth-proxy.internal.paulfriedrich.me
────────────────────────────────────────


AVD-KSV-0108 (HIGH): Service 'oauth2-proxy' in 'default' namespace should not set external IPs or external Name
════════════════════════════════════════
Services with external IP addresses allows direct access from the internet and might expose risk for CVE-2020-8554

See https://avd.aquasec.com/misconfig/avd-ksv-0108
────────────────────────────────────────



oauth2.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'longhorn-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth3.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'longhorn-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth4.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'open-webui-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth5.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'open-webui-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth6.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'alertmanager-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth6_4.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'prometheus-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth7.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'alertmanager-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth7_4.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'prometheus-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth8.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'kube-dashboard-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth9.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'stg-kube-dashboard-oauth2-proxy-conf-1' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"authenticated_emails_file"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



oauth_config.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'pl-oauth-config' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"PL_AUTH_EMAIL_PASSWORD_CONN"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────
 oauth_config.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────


AVD-KSV-0109 (HIGH): ConfigMap 'pl-oauth-config' in 'default' namespace stores secrets in key(s) or value(s) '{"PL_AUTH_EMAIL_PASSWORD_CONN"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────
 oauth_config.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────



oauth_config1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'pl-oauth-config' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"PL_AUTH_EMAIL_PASSWORD_CONN"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────
 oauth_config1.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────


AVD-KSV-0109 (HIGH): ConfigMap 'pl-oauth-config' in 'default' namespace stores secrets in key(s) or value(s) '{"PL_AUTH_EMAIL_PASSWORD_CONN"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────
 oauth_config1.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────



obese-httpd.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 obese-httpd.yaml:7-13
────────────────────────────────────────
   7 ┌   type: NodePort
   8 │   selector:
   9 │     app: obese_httpd_app
  10 │   ports:
  11 │   - port: 80
  12 │     targetPort: 80
  13 └     nodePort: 30004
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 obese-httpd.yaml:7-13
────────────────────────────────────────
   7 ┌   type: NodePort
   8 │   selector:
   9 │     app: obese_httpd_app
  10 │   ports:
  11 │   - port: 80
  12 │     targetPort: 80
  13 └     nodePort: 30004
────────────────────────────────────────



obese-httpd_1.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 10, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'obese-httpd-container' of 'deployment' 'obese-httpd-deployment' in 'limited' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'obese-httpd-container' of Deployment 'obese-httpd-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 obese-httpd_1.yaml:9-26
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: obese_httpd_app
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: obese_httpd_app
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 obese-httpd_1.yaml:9-26
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: obese_httpd_app
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: obese_httpd_app
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "obese-httpd-container" of deployment "obese-httpd-deployment" in "limited" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment obese-httpd-deployment in limited namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container obese-httpd-deployment in limited namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment obese-httpd-deployment in limited namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 obese-httpd_1.yaml:18-26
────────────────────────────────────────
  18 ┌       containers:
  19 │       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container obese-httpd-container in deployment obese-httpd-deployment (namespace: limited) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 obese-httpd_1.yaml:19-26
────────────────────────────────────────
  19 ┌       - name: obese-httpd-container
  20 │         image: $REGISTRY_IP_DOMAIN/mfranzil/obese-httpd:50
  21 │         ports:
  22 │         - containerPort: 80
  23 │         resources:
  24 │           limits:
  25 │             cpu: 500m
  26 └             memory: 0.5G
────────────────────────────────────────



object-compute-quota.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 object-compute-quota.yaml:6-10
────────────────────────────────────────
   6 ┌   hard:
   7 │     count/configmaps: 10
   8 │     count/persistentvolumeclaims: 4
   9 │     count/jobs.batch: 20
  10 └     count/secrets: 3
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 object-compute-quota.yaml:7-10
────────────────────────────────────────
   7 ┌     count/configmaps: 10
   8 │     count/persistentvolumeclaims: 4
   9 │     count/jobs.batch: 20
  10 └     count/secrets: 3
────────────────────────────────────────



object-count-quota.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 object-count-quota.yaml:6-10
────────────────────────────────────────
   6 ┌   hard: 
   7 │     count/configmaps: 10
   8 │     count/persistentvolumeclaims: 4
   9 │     count/jobs.batch: 20
  10 └     count/secrets: 3
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 object-count-quota.yaml:7-10
────────────────────────────────────────
   7 ┌     count/configmaps: 10
   8 │     count/persistentvolumeclaims: 4
   9 │     count/jobs.batch: 20
  10 └     count/secrets: 3
────────────────────────────────────────



object_counts.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 object_counts.yaml:6-13
────────────────────────────────────────
   6 ┌   hard:
   7 │     configmaps: "10"
   8 │     persistentvolumeclaims: "4"
   9 │     pods: "4"
  10 │     replicationcontrollers: "20"
  11 │     secrets: "10"
  12 │     services: "10"
  13 └     services.loadbalancers: "2"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 object_counts.yaml:7-13
────────────────────────────────────────
   7 ┌     configmaps: "10"
   8 │     persistentvolumeclaims: "4"
   9 │     pods: "4"
  10 │     replicationcontrollers: "20"
  11 │     secrets: "10"
  12 │     services: "10"
  13 └     services.loadbalancers: "2"
────────────────────────────────────────



objectbasetemplate.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Pod 'firstpod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Pod 'firstpod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'pod' 'firstpod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Pod 'firstpod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Pod 'firstpod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'nginx' of Pod 'firstpod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Pod 'firstpod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Pod 'firstpod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Pod 'firstpod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Pod 'firstpod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Pod 'firstpod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Pod 'firstpod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 objectbasetemplate.yaml:9-13
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 objectbasetemplate.yaml:9-13
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of pod "firstpod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod firstpod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 objectbasetemplate.yaml:4-7
────────────────────────────────────────
   4 ┌   name: firstpod
   5 │   labels: 
   6 │    app: front-end
   7 └    team: developer 
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod firstpod in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container firstpod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 objectbasetemplate.yaml:10-13
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod firstpod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 objectbasetemplate.yaml:9-13
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 └     - containerPort: 80
────────────────────────────────────────



observed-deploy-nginx.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'nginx' of Deployment 'nginx' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 observed-deploy-nginx.yaml:8-25
────────────────────────────────────────
   8 ┌   replicas: 3
   9 │   paused: true
  10 │   selector:
  11 │     matchLabels:
  12 │       app: nginx
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 └         app: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 observed-deploy-nginx.yaml:8-25
────────────────────────────────────────
   8 ┌   replicas: 3
   9 │   paused: true
  10 │   selector:
  11 │     matchLabels:
  12 │       app: nginx
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 └         app: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 observed-deploy-nginx.yaml:4-6
────────────────────────────────────────
   4 ┌   name: nginx
   5 │   labels:
   6 └     app: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 observed-deploy-nginx.yaml:21-25
────────────────────────────────────────
  21 ┌       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 observed-deploy-nginx.yaml:18-25
────────────────────────────────────────
  18 ┌       nodeSelector:
  19 │         foo: bar
  20 │       containers:
  21 │       - image: nginx
  22 │         name: nginx
  23 │         resources:
  24 │           limits:
  25 └             cpu: 100m
────────────────────────────────────────



ocean-nodemailer.yaml (kubernetes)
==================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ocean-nodemailer' of 'deployment' 'nodemailer-backend' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ocean-nodemailer' of Deployment 'nodemailer-backend' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ocean-nodemailer.yaml:9-28
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: nodemailer-backend
  13 │   strategy: {}
  14 │   template:
  15 │     metadata:
  16 │       creationTimestamp: null
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ocean-nodemailer.yaml:9-28
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: nodemailer-backend
  13 │   strategy: {}
  14 │   template:
  15 │     metadata:
  16 │       creationTimestamp: null
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ocean-nodemailer" of deployment "nodemailer-backend" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nodemailer-backend in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ocean-nodemailer.yaml:4-7
────────────────────────────────────────
   4 ┌   creationTimestamp: null
   5 │   labels:
   6 │     app: nodemailer-backend
   7 └   name: nodemailer-backend
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nodemailer-backend in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nodemailer-backend in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ocean-nodemailer.yaml:20-28
────────────────────────────────────────
  20 ┌       containers:
  21 │       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ocean-nodemailer in deployment nodemailer-backend (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ocean-nodemailer.yaml:21-28
────────────────────────────────────────
  21 ┌       - image: devarsh10/ocean-nodemailer:0.2
  22 │         name: ocean-nodemailer
  23 │         ports:
  24 │         - containerPort: 5000
  25 │         command:
  26 │         - node
  27 │         - app.js
  28 └         resources: {}
────────────────────────────────────────



ocean-nodemailer_1.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ocean-nodemailer_1.yaml:6-12
────────────────────────────────────────
   6 ┌   type: NodePort
   7 │   selector:
   8 │     app: nodemailer-backend
   9 │   ports:
  10 │   - port: 5000
  11 │     targetPort: 5000
  12 └     nodePort: 32007
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ocean-nodemailer_1.yaml:6-12
────────────────────────────────────────
   6 ┌   type: NodePort
   7 │   selector:
   8 │     app: nodemailer-backend
   9 │   ports:
  10 │   - port: 5000
  11 │     targetPort: 5000
  12 └     nodePort: 32007
────────────────────────────────────────



oci-dummy-pod.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oci-dummy' of Pod 'oci-dummy' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oci-dummy' of 'pod' 'oci-dummy' in 'services' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oci-dummy' of Pod 'oci-dummy' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oci-dummy' of Pod 'oci-dummy' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oci-dummy' of Pod 'oci-dummy' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oci-dummy-pod.yaml:9-14
────────────────────────────────────────
   9 ┌   imagePullSecrets:
  10 │   - name: docker-io
  11 │   containers:
  12 │   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oci-dummy-pod.yaml:9-14
────────────────────────────────────────
   9 ┌   imagePullSecrets:
  10 │   - name: docker-io
  11 │   containers:
  12 │   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oci-dummy" of pod "oci-dummy" in "services" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oci-dummy in services namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod oci-dummy in services namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oci-dummy-pod.yaml:9-14
────────────────────────────────────────
   9 ┌   imagePullSecrets:
  10 │   - name: docker-io
  11 │   containers:
  12 │   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oci-dummy in pod oci-dummy (namespace: services) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oci-dummy-pod.yaml:12-14
────────────────────────────────────────
  12 ┌   - name: oci-dummy
  13 │     image: docker.io/snyk/runtime-fixtures:oci-dummy
  14 └     command: ['/app/sample']
────────────────────────────────────────



octant.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 octant.yaml:8-9
────────────────────────────────────────
   8 ┌   finalizers:
   9 └   - kubernetes
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 octant.yaml:8-9
────────────────────────────────────────
   8 ┌   finalizers:
   9 └   - kubernetes
────────────────────────────────────────



octant_2.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 octant_2.yaml:11-16
────────────────────────────────────────
  11 ┌ - apiGroups:
  12 │   - '*'
  13 │   resources:
  14 │   - '*'
  15 │   verbs:
  16 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'octant-readonly-everything' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 octant_2.yaml:11-16
────────────────────────────────────────
  11 ┌ - apiGroups:
  12 │   - '*'
  13 │   resources:
  14 │   - '*'
  15 │   verbs:
  16 └   - '*'
────────────────────────────────────────



octant_4.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 octant_4.yaml:9-17
────────────────────────────────────────
   9 ┌   type: ClusterIP
  10 │   ports:
  11 │   - port: 8000
  12 │     targetPort: http
  13 │     protocol: TCP
  14 │     name: http
  15 │   selector:
  16 │     app.kubernetes.io/name: octant
  17 └     app.kubernetes.io/instance: octant-dashboard
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 octant_4.yaml:9-17
────────────────────────────────────────
   9 ┌   type: ClusterIP
  10 │   ports:
  11 │   - port: 8000
  12 │     targetPort: http
  13 │     protocol: TCP
  14 │     name: http
  15 │   selector:
  16 │     app.kubernetes.io/name: octant
  17 └     app.kubernetes.io/instance: octant-dashboard
────────────────────────────────────────



octant_5.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 108, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 5, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'octant' of Deployment 'octant-dashboard' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 octant_5.yaml:23-63
────────────────────────────────────────
  23 ┌       - name: octant
  24 │         securityContext:
  25 │           capabilities:
  26 │             drop:
  27 │             - ALL
  28 │           readOnlyRootFilesystem: true
  29 │           runAsNonRoot: true
  30 │           runAsUser: 1000
  31 └         image: aleveille/octant-dashboard:v0.25.1
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'octant' of Deployment 'octant-dashboard' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 octant_5.yaml:23-63
────────────────────────────────────────
  23 ┌       - name: octant
  24 │         securityContext:
  25 │           capabilities:
  26 │             drop:
  27 │             - ALL
  28 │           readOnlyRootFilesystem: true
  29 │           runAsNonRoot: true
  30 │           runAsUser: 1000
  31 └         image: aleveille/octant-dashboard:v0.25.1
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'octant' of Deployment 'octant-dashboard' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 octant_5.yaml:23-63
────────────────────────────────────────
  23 ┌       - name: octant
  24 │         securityContext:
  25 │           capabilities:
  26 │             drop:
  27 │             - ALL
  28 │           readOnlyRootFilesystem: true
  29 │           runAsNonRoot: true
  30 │           runAsUser: 1000
  31 └         image: aleveille/octant-dashboard:v0.25.1
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 octant_5.yaml:23-63
────────────────────────────────────────
  23 ┌       - name: octant
  24 │         securityContext:
  25 │           capabilities:
  26 │             drop:
  27 │             - ALL
  28 │           readOnlyRootFilesystem: true
  29 │           runAsNonRoot: true
  30 │           runAsUser: 1000
  31 └         image: aleveille/octant-dashboard:v0.25.1
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 octant_5.yaml:7-68
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app.kubernetes.io/name: octant
  11 │       app.kubernetes.io/instance: octant-dashboard
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         app.kubernetes.io/name: octant
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 octant_5.yaml:7-68
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app.kubernetes.io/name: octant
  11 │       app.kubernetes.io/instance: octant-dashboard
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         app.kubernetes.io/name: octant
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "octant" of deployment "octant-dashboard" in "octant" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 octant_5.yaml:23-63
────────────────────────────────────────
  23 ┌       - name: octant
  24 │         securityContext:
  25 │           capabilities:
  26 │             drop:
  27 │             - ALL
  28 │           readOnlyRootFilesystem: true
  29 │           runAsNonRoot: true
  30 │           runAsUser: 1000
  31 └         image: aleveille/octant-dashboard:v0.25.1
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container octant in deployment octant-dashboard (namespace: octant) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 octant_5.yaml:23-63
────────────────────────────────────────
  23 ┌       - name: octant
  24 │         securityContext:
  25 │           capabilities:
  26 │             drop:
  27 │             - ALL
  28 │           readOnlyRootFilesystem: true
  29 │           runAsNonRoot: true
  30 │           runAsUser: 1000
  31 └         image: aleveille/octant-dashboard:v0.25.1
  ..   
────────────────────────────────────────



oddly-named-file[x].yaml (kubernetes)
=====================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x].yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x].yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x].yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x].yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x].yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]1.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]1.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]1.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]1.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]1.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]1.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]11.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]11.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]11.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]11.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]11.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]11.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]12.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]12.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]12.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]12.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]12.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]12.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]13.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]13.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]13.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]13.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]13.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]13.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]14.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]14.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]14.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]14.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]14.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]14.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]15.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]15.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]15.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]15.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]15.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]15.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]2.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]2.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]2.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]2.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]2.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]2.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]3.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]3.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]3.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]3.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]3.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]3.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]4.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]4.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]4.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]4.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]4.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]4.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]5.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]5.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]5.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]5.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]5.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]5.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]6.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]6.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]6.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]6.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]6.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]6.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]7.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]7.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]7.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]7.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]7.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]7.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]8.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]8.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]8.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]8.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]8.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]8.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



oddly-named-file[x]9.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'busybox' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'busybox' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'busybox' of Pod 'busybox' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'busybox' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'busybox' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'busybox' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oddly-named-file[x]9.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oddly-named-file[x]9.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "busybox" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod busybox in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oddly-named-file[x]9.yaml:4-6
────────────────────────────────────────
   4 ┌   name: busybox
   5 │   labels:
   6 └     name: busybox
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod busybox in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container busybox in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]9.yaml:9-12
────────────────────────────────────────
   9 ┌     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod busybox in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oddly-named-file[x]9.yaml:8-12
────────────────────────────────────────
   8 ┌   containers:
   9 │     - name: busybox
  10 │       image: busybox
  11 │       ports:
  12 └         - containerPort: 80
────────────────────────────────────────



odoo.yaml (kubernetes)
======================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'odoo' of Deployment 'odoo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'odoo' of Deployment 'odoo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'odoo' of 'deployment' 'odoo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'odoo' of Deployment 'odoo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'odoo' of Deployment 'odoo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'odoo' of Deployment 'odoo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'odoo' of Deployment 'odoo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'odoo' of Deployment 'odoo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'odoo' of Deployment 'odoo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'odoo' of Deployment 'odoo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'odoo' of Deployment 'odoo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 odoo.yaml:9-41
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: odoo
  13 │   strategy: {}
  14 │   template:
  15 │     metadata:
  16 │       creationTimestamp: null
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 odoo.yaml:9-41
────────────────────────────────────────
   9 ┌   replicas: 1
  10 │   selector:
  11 │     matchLabels:
  12 │       app: odoo
  13 │   strategy: {}
  14 │   template:
  15 │     metadata:
  16 │       creationTimestamp: null
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "odoo" of deployment "odoo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment odoo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 odoo.yaml:4-7
────────────────────────────────────────
   4 ┌   creationTimestamp: null
   5 │   labels:
   6 │     app: odoo
   7 └   name: odoo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container odoo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment odoo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 odoo.yaml:20-41
────────────────────────────────────────
  20 ┌       containers:
  21 │       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 └             - name: ODOO_DATABASE_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container odoo in deployment odoo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 odoo.yaml:21-38
────────────────────────────────────────
  21 ┌       - image: docker.io/bitnami/odoo:16
  22 │         name: odoo
  23 │         ports:
  24 │             - containerPort: 8069
  25 │         env:
  26 │             - name: ODOO_DATABASE_NAME
  27 │               value: k8smaestro
  28 │             - name: ODOO_DATABASE_PASSWORD
  29 └               value: k8smaestro
  ..   
────────────────────────────────────────



odoo_svc.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 odoo_svc.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: odoo
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 └       targetPort: 8069
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 odoo_svc.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: odoo
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 └       targetPort: 8069
────────────────────────────────────────



offers-service.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'offers-service' of Deployment 'offers-service' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'offers-service' of Deployment 'offers-service' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'offers-service' of 'deployment' 'offers-service' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'offers-service' of Deployment 'offers-service' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'offers-service' of Deployment 'offers-service' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'offers-service' of Deployment 'offers-service' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'offers-service' of Deployment 'offers-service' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'offers-service' of Deployment 'offers-service' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'offers-service' of Deployment 'offers-service' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'offers-service' of Deployment 'offers-service' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 offers-service.yaml:6-32
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: offers-service
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: offers-service
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 offers-service.yaml:6-32
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: offers-service
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: offers-service
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "offers-service" of deployment "offers-service" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment offers-service in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 offers-service.yaml:4
────────────────────────────────────────
   4 [   name: offers-service
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container offers-service in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 offers-service.yaml:20-32
────────────────────────────────────────
  20 ┌       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 │         - containerPort: 8084
  24 │         imagePullPolicy: Never
  25 │         volumeMounts:
  26 │         - mountPath: /var/log
  27 │           name: log-pv-storage
  28 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment offers-service in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 offers-service.yaml:15-32
────────────────────────────────────────
  15 ┌       volumes:
  16 │       - name: log-pv-storage
  17 │         persistentVolumeClaim:
  18 │           claimName: log-pv-claim
  19 │       containers:
  20 │       - name: offers-service
  21 │         image: offers-service:latest
  22 │         ports:
  23 └         - containerPort: 8084
  ..   
────────────────────────────────────────



offers-service_1.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 offers-service_1.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: offers-service
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 8084
  11 └     targetPort: 8084
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 offers-service_1.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: offers-service
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 8084
  11 └     targetPort: 8084
────────────────────────────────────────



offers-service_2.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 offers-service_2.yaml:6-18
────────────────────────────────────────
   6 ┌   scaleTargetRef:
   7 │     apiVersion: apps/v1
   8 │     kind: Deployment
   9 │     name: offers-service
  10 │   minReplicas: 5
  11 │   maxReplicas: 10
  12 │   metrics:
  13 │   - type: Resource
  14 └     resource:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 offers-service_2.yaml:6-18
────────────────────────────────────────
   6 ┌   scaleTargetRef:
   7 │     apiVersion: apps/v1
   8 │     kind: Deployment
   9 │     name: offers-service
  10 │   minReplicas: 5
  11 │   maxReplicas: 10
  12 │   metrics:
  13 │   - type: Resource
  14 └     resource:
  ..   
────────────────────────────────────────



oidc-rbac.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'oidc-group-gabernetes-cluster-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 oidc-rbac.yaml:4
────────────────────────────────────────
   4 [   name: oidc-group-gabernetes-cluster-admin
────────────────────────────────────────



oidc-rbac1.yaml (kubernetes)
============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'oidc-group-gabernetes-cluster-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 oidc-rbac1.yaml:4
────────────────────────────────────────
   4 [   name: oidc-group-gabernetes-cluster-admin
────────────────────────────────────────



oknok-job.yaml (kubernetes)
===========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oknok' of Job 'oknok' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oknok' of Job 'oknok' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oknok' of 'job' 'oknok' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oknok' of Job 'oknok' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oknok' of Job 'oknok' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'oknok' of Job 'oknok' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oknok' of Job 'oknok' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oknok' of Job 'oknok' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oknok' of Job 'oknok' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oknok' of Job 'oknok' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oknok' of Job 'oknok' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oknok' of Job 'oknok' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oknok-job.yaml:6-15
────────────────────────────────────────
   6 ┌   backoffLimit: 10
   7 │   completions: 3
   8 │   parallelism: 1
   9 │   template:
  10 │     spec:
  11 │       containers:
  12 │       - name: oknok
  13 │         image: centos
  14 │         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
  15 └       restartPolicy: Never
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oknok-job.yaml:6-15
────────────────────────────────────────
   6 ┌   backoffLimit: 10
   7 │   completions: 3
   8 │   parallelism: 1
   9 │   template:
  10 │     spec:
  11 │       containers:
  12 │       - name: oknok
  13 │         image: centos
  14 │         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
  15 └       restartPolicy: Never
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oknok" of job "oknok" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): job oknok in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oknok-job.yaml:4
────────────────────────────────────────
   4 [   name: oknok
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oknok in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oknok-job.yaml:12-14
────────────────────────────────────────
  12 ┌       - name: oknok
  13 │         image: centos
  14 └         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): job oknok in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oknok-job.yaml:11-15
────────────────────────────────────────
  11 ┌       containers:
  12 │       - name: oknok
  13 │         image: centos
  14 │         command: ["sh", "-c", "exit $(($(date +%s)%2))"]
  15 └       restartPolicy: Never
────────────────────────────────────────



old-ingress-nginx-controller_12.yaml (kubernetes)
=================================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0114 (CRITICAL): ClusterRole 'ingress-nginx-admission' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 old-ingress-nginx-controller_12.yaml:16-22
────────────────────────────────────────
  16 ┌ - apiGroups:
  17 │   - admissionregistration.k8s.io
  18 │   resources:
  19 │   - validatingwebhookconfigurations
  20 │   verbs:
  21 │   - get
  22 └   - update
────────────────────────────────────────



old-ingress-nginx-controller_14.yaml (kubernetes)
=================================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'ingress-nginx-admission' shouldn't have access to manage secrets in namespace 'ingress-nginx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 old-ingress-nginx-controller_14.yaml:17-23
────────────────────────────────────────
  17 ┌ - apiGroups:
  18 │   - ''
  19 │   resources:
  20 │   - secrets
  21 │   verbs:
  22 │   - get
  23 └   - create
────────────────────────────────────────



old-ingress-nginx-controller_16.yaml (kubernetes)
=================================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'create' of 'job' 'ingress-nginx-admission-create' in 'ingress-nginx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:17-46
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-create
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-3.10.1
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.41.2
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:17-46
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-create
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-3.10.1
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.41.2
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "create" of job "ingress-nginx-admission-create" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container create in job ingress-nginx-admission-create (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingress-nginx-controller_16.yaml:29-41
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --secret-name=ingress-nginx-admission
  37 └         env:
  ..   
────────────────────────────────────────



old-ingress-nginx-controller_17.yaml (kubernetes)
=================================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'patch' of 'job' 'ingress-nginx-admission-patch' in 'ingress-nginx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:17-48
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-patch
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-3.10.1
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.41.2
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:17-48
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-patch
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-3.10.1
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.41.2
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "patch" of job "ingress-nginx-admission-patch" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container patch in job ingress-nginx-admission-patch (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingress-nginx-controller_17.yaml:29-43
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: docker.io/jettech/kube-webhook-certgen:v1.5.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=$(POD_NAMESPACE)
  36 │         - --patch-mutating=false
  37 └         - --secret-name=ingress-nginx-admission
  ..   
────────────────────────────────────────



old-ingress-nginx-controller_3.yaml (kubernetes)
================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ingress-nginx' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 old-ingress-nginx-controller_3.yaml:12-22
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   - endpoints
  17 │   - nodes
  18 │   - pods
  19 │   - secrets
  20 └   verbs:
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingress-nginx-controller_3.yaml:29-37
────────────────────────────────────────
  29 ┌ - apiGroups:
  30 │   - ''
  31 │   resources:
  32 │   - services
  33 │   verbs:
  34 │   - get
  35 │   - list
  36 │   - update
  37 └   - watch
────────────────────────────────────────



old-ingress-nginx-controller_5.yaml (kubernetes)
================================================
Tests: 118 (SUCCESSES: 113, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'ingress-nginx' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 old-ingress-nginx-controller_5.yaml:64-72
────────────────────────────────────────
  64 ┌ - apiGroups:
  65 │   - ''
  66 │   resources:
  67 │   - configmaps
  68 │   resourceNames:
  69 │   - ingress-controller-leader-nginx
  70 │   verbs:
  71 │   - get
  72 └   - update
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'ingress-nginx' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 old-ingress-nginx-controller_5.yaml:73-78
────────────────────────────────────────
  73 ┌ - apiGroups:
  74 │   - ''
  75 │   resources:
  76 │   - configmaps
  77 │   verbs:
  78 └   - create
────────────────────────────────────────


AVD-KSV-0056 (HIGH): Role 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingress-nginx-controller_5.yaml:31-39
────────────────────────────────────────
  31 ┌ - apiGroups:
  32 │   - ''
  33 │   resources:
  34 │   - services
  35 │   verbs:
  36 │   - get
  37 │   - list
  38 │   - update
  39 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): Role 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingress-nginx-controller_5.yaml:79-86
────────────────────────────────────────
  79 ┌ - apiGroups:
  80 │   - ''
  81 │   resources:
  82 │   - endpoints
  83 │   verbs:
  84 │   - create
  85 │   - get
  86 └   - update
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'ingress-nginx' shouldn't have access to manage secrets in namespace 'ingress-nginx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 old-ingress-nginx-controller_5.yaml:20-30
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - configmaps
  24 │   - pods
  25 │   - secrets
  26 │   - endpoints
  27 │   verbs:
  28 └   - get
  ..   
────────────────────────────────────────



old-ingress-nginx-controller_7.yaml (kubernetes)
================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingress-nginx-controller_7.yaml:14-22
────────────────────────────────────────
  14 ┌   type: ClusterIP
  15 │   ports:
  16 │   - name: https-webhook
  17 │     port: 443
  18 │     targetPort: webhook
  19 │   selector:
  20 │     app.kubernetes.io/name: ingress-nginx
  21 │     app.kubernetes.io/instance: ingress-nginx
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingress-nginx-controller_7.yaml:14-22
────────────────────────────────────────
  14 ┌   type: ClusterIP
  15 │   ports:
  16 │   - name: https-webhook
  17 │     port: 443
  18 │     targetPort: webhook
  19 │   selector:
  20 │     app.kubernetes.io/name: ingress-nginx
  21 │     app.kubernetes.io/instance: ingress-nginx
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



old-ingress-nginx-controller_8.yaml (kubernetes)
================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingress-nginx-controller_8.yaml:15-28
────────────────────────────────────────
  15 ┌   type: NodePort
  16 │   ports:
  17 │   - name: http
  18 │     port: 80
  19 │     protocol: TCP
  20 │     targetPort: http
  21 │   - name: https
  22 │     port: 443
  23 └     protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingress-nginx-controller_8.yaml:15-28
────────────────────────────────────────
  15 ┌   type: NodePort
  16 │   ports:
  17 │   - name: http
  18 │     port: 80
  19 │     protocol: TCP
  20 │     targetPort: http
  21 │   - name: https
  22 │     port: 443
  23 └     protocol: TCP
  ..   
────────────────────────────────────────



old-ingress-nginx-controller_9.yaml (kubernetes)
================================================
Tests: 116 (SUCCESSES: 101, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 7, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0009 (HIGH): Deployment 'ingress-nginx-controller' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:14-111
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:14-111
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:14-111
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "controller" of deployment "ingress-nginx-controller" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:31-103
────────────────────────────────────────
  31 ┌       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 │             exec:
  37 │               command:
  38 │               - /wait-shutdown
  39 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment ingress-nginx-controller in ingress-nginx namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ingress-nginx-controller in ingress-nginx namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 old-ingress-nginx-controller_9.yaml:28-111
────────────────────────────────────────
  28 ┌       hostNetwork: true
  29 │       dnsPolicy: ClusterFirst
  30 │       containers:
  31 │       - name: controller
  32 │         image: k8s.gcr.io/ingress-nginx/controller:v0.41.2@sha256:1f4f402b9c14f3ae92b11ada1dfe9893a88f0faeb0b2f4b903e2c67a0c3bf0de
  33 │         imagePullPolicy: IfNotPresent
  34 │         lifecycle:
  35 │           preStop:
  36 └             exec:
  ..   
────────────────────────────────────────



old-ingressctrl1_11.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0114 (CRITICAL): ClusterRole 'ingress-nginx-admission' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 old-ingressctrl1_11.yaml:17-23
────────────────────────────────────────
  17 ┌ - apiGroups:
  18 │   - admissionregistration.k8s.io
  19 │   resources:
  20 │   - validatingwebhookconfigurations
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



old-ingressctrl1_13.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'create' of 'job' 'ingress-nginx-admission-create' in 'ingress-nginx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl1_13.yaml:17-41
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-create
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl1_13.yaml:17-41
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-create
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "create" of job "ingress-nginx-admission-create" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container create in job ingress-nginx-admission-create (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingressctrl1_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────



old-ingressctrl1_14.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'patch' of 'job' 'ingress-nginx-admission-patch' in 'ingress-nginx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl1_14.yaml:17-43
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-patch
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl1_14.yaml:17-43
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-patch
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "patch" of job "ingress-nginx-admission-patch" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container patch in job ingress-nginx-admission-patch (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingressctrl1_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────



old-ingressctrl1_15.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'ingress-nginx-admission' shouldn't have access to manage secrets in namespace 'ingress-nginx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 old-ingressctrl1_15.yaml:17-23
────────────────────────────────────────
  17 ┌ - apiGroups:
  18 │   - ''
  19 │   resources:
  20 │   - secrets
  21 │   verbs:
  22 │   - get
  23 └   - create
────────────────────────────────────────



old-ingressctrl1_3.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ingress-nginx' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 old-ingressctrl1_3.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - configmaps
  17 │   - endpoints
  18 │   - nodes
  19 │   - pods
  20 │   - secrets
  21 └   verbs:
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingressctrl1_3.yaml:30-38
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ''
  32 │   resources:
  33 │   - services
  34 │   verbs:
  35 │   - get
  36 │   - list
  37 │   - update
  38 └   - watch
────────────────────────────────────────



old-ingressctrl1_5.yaml (kubernetes)
====================================
Tests: 118 (SUCCESSES: 113, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'ingress-nginx' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 old-ingressctrl1_5.yaml:64-72
────────────────────────────────────────
  64 ┌ - apiGroups:
  65 │   - ''
  66 │   resources:
  67 │   - configmaps
  68 │   resourceNames:
  69 │   - ingress-controller-leader-nginx
  70 │   verbs:
  71 │   - get
  72 └   - update
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'ingress-nginx' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 old-ingressctrl1_5.yaml:73-78
────────────────────────────────────────
  73 ┌ - apiGroups:
  74 │   - ''
  75 │   resources:
  76 │   - configmaps
  77 │   verbs:
  78 └   - create
────────────────────────────────────────


AVD-KSV-0056 (HIGH): Role 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingressctrl1_5.yaml:31-39
────────────────────────────────────────
  31 ┌ - apiGroups:
  32 │   - ''
  33 │   resources:
  34 │   - services
  35 │   verbs:
  36 │   - get
  37 │   - list
  38 │   - update
  39 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): Role 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingressctrl1_5.yaml:79-86
────────────────────────────────────────
  79 ┌ - apiGroups:
  80 │   - ''
  81 │   resources:
  82 │   - endpoints
  83 │   verbs:
  84 │   - create
  85 │   - get
  86 └   - update
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'ingress-nginx' shouldn't have access to manage secrets in namespace 'ingress-nginx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 old-ingressctrl1_5.yaml:20-30
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - configmaps
  24 │   - pods
  25 │   - secrets
  26 │   - endpoints
  27 │   verbs:
  28 └   - get
  ..   
────────────────────────────────────────



old-ingressctrl1_7.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl1_7.yaml:14-22
────────────────────────────────────────
  14 ┌   type: ClusterIP
  15 │   ports:
  16 │   - name: https-webhook
  17 │     port: 443
  18 │     targetPort: webhook
  19 │   selector:
  20 │     app.kubernetes.io/name: ingress-nginx
  21 │     app.kubernetes.io/instance: ingress-nginx
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl1_7.yaml:14-22
────────────────────────────────────────
  14 ┌   type: ClusterIP
  15 │   ports:
  16 │   - name: https-webhook
  17 │     port: 443
  18 │     targetPort: webhook
  19 │   selector:
  20 │     app.kubernetes.io/name: ingress-nginx
  21 │     app.kubernetes.io/instance: ingress-nginx
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



old-ingressctrl1_8.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl1_8.yaml:19-33
────────────────────────────────────────
  19 ┌   type: LoadBalancer
  20 │   externalTrafficPolicy: Local
  21 │   ports:
  22 │   - name: http
  23 │     port: 80
  24 │     protocol: TCP
  25 │     targetPort: http
  26 │   - name: https
  27 └     port: 443
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl1_8.yaml:19-33
────────────────────────────────────────
  19 ┌   type: LoadBalancer
  20 │   externalTrafficPolicy: Local
  21 │   ports:
  22 │   - name: http
  23 │     port: 80
  24 │     protocol: TCP
  25 │     targetPort: http
  26 │   - name: https
  27 └     port: 443
  ..   
────────────────────────────────────────



old-ingressctrl1_9.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 101, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 7, MEDIUM: 6, HIGH: 2, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl1_9.yaml:14-107
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl1_9.yaml:14-107
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "controller" of deployment "ingress-nginx-controller" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment ingress-nginx-controller in ingress-nginx namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ingress-nginx-controller in ingress-nginx namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 old-ingressctrl1_9.yaml:28-107
────────────────────────────────────────
  28 ┌       dnsPolicy: ClusterFirst
  29 │       containers:
  30 │       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 └               command:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container controller in deployment ingress-nginx-controller (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingressctrl1_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────



old-ingressctrl_11.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0114 (CRITICAL): ClusterRole 'ingress-nginx-admission' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 old-ingressctrl_11.yaml:17-23
────────────────────────────────────────
  17 ┌ - apiGroups:
  18 │   - admissionregistration.k8s.io
  19 │   resources:
  20 │   - validatingwebhookconfigurations
  21 │   verbs:
  22 │   - get
  23 └   - update
────────────────────────────────────────



old-ingressctrl_13.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'create' of 'job' 'ingress-nginx-admission-create' in 'ingress-nginx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'create' of Job 'ingress-nginx-admission-create' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl_13.yaml:17-41
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-create
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl_13.yaml:17-41
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-create
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "create" of job "ingress-nginx-admission-create" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container create in job ingress-nginx-admission-create (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingressctrl_13.yaml:29-36
────────────────────────────────────────
  29 ┌       - name: create
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: IfNotPresent
  32 │         args:
  33 │         - create
  34 │         - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc
  35 │         - --namespace=ingress-nginx
  36 └         - --secret-name=ingress-nginx-admission
────────────────────────────────────────



old-ingressctrl_14.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'patch' of 'job' 'ingress-nginx-admission-patch' in 'ingress-nginx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'patch' of Job 'ingress-nginx-admission-patch' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl_14.yaml:17-43
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-patch
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl_14.yaml:17-43
────────────────────────────────────────
  17 ┌   template:
  18 │     metadata:
  19 │       name: ingress-nginx-admission-patch
  20 │       labels:
  21 │         helm.sh/chart: ingress-nginx-2.0.3
  22 │         app.kubernetes.io/name: ingress-nginx
  23 │         app.kubernetes.io/instance: ingress-nginx
  24 │         app.kubernetes.io/version: 0.32.0
  25 └         app.kubernetes.io/managed-by: Helm
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "patch" of job "ingress-nginx-admission-patch" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container patch in job ingress-nginx-admission-patch (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingressctrl_14.yaml:29-38
────────────────────────────────────────
  29 ┌       - name: patch
  30 │         image: jettech/kube-webhook-certgen:v1.2.0
  31 │         imagePullPolicy: null
  32 │         args:
  33 │         - patch
  34 │         - --webhook-name=ingress-nginx-admission
  35 │         - --namespace=ingress-nginx
  36 │         - --patch-mutating=false
  37 │         - --secret-name=ingress-nginx-admission
  38 └         - --patch-failure-policy=Fail
────────────────────────────────────────



old-ingressctrl_15.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'ingress-nginx-admission' shouldn't have access to manage secrets in namespace 'ingress-nginx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 old-ingressctrl_15.yaml:17-23
────────────────────────────────────────
  17 ┌ - apiGroups:
  18 │   - ''
  19 │   resources:
  20 │   - secrets
  21 │   verbs:
  22 │   - get
  23 └   - create
────────────────────────────────────────



old-ingressctrl_3.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'ingress-nginx' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 old-ingressctrl_3.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - configmaps
  17 │   - endpoints
  18 │   - nodes
  19 │   - pods
  20 │   - secrets
  21 └   verbs:
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingressctrl_3.yaml:30-38
────────────────────────────────────────
  30 ┌ - apiGroups:
  31 │   - ''
  32 │   resources:
  33 │   - services
  34 │   verbs:
  35 │   - get
  36 │   - list
  37 │   - update
  38 └   - watch
────────────────────────────────────────



old-ingressctrl_5.yaml (kubernetes)
===================================
Tests: 118 (SUCCESSES: 113, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 3, HIGH: 2, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'ingress-nginx' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 old-ingressctrl_5.yaml:64-72
────────────────────────────────────────
  64 ┌ - apiGroups:
  65 │   - ''
  66 │   resources:
  67 │   - configmaps
  68 │   resourceNames:
  69 │   - ingress-controller-leader-nginx
  70 │   verbs:
  71 │   - get
  72 └   - update
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'ingress-nginx' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 old-ingressctrl_5.yaml:73-78
────────────────────────────────────────
  73 ┌ - apiGroups:
  74 │   - ''
  75 │   resources:
  76 │   - configmaps
  77 │   verbs:
  78 └   - create
────────────────────────────────────────


AVD-KSV-0056 (HIGH): Role 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingressctrl_5.yaml:31-39
────────────────────────────────────────
  31 ┌ - apiGroups:
  32 │   - ''
  33 │   resources:
  34 │   - services
  35 │   verbs:
  36 │   - get
  37 │   - list
  38 │   - update
  39 └   - watch
────────────────────────────────────────


AVD-KSV-0056 (HIGH): Role 'ingress-nginx' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 old-ingressctrl_5.yaml:79-86
────────────────────────────────────────
  79 ┌ - apiGroups:
  80 │   - ''
  81 │   resources:
  82 │   - endpoints
  83 │   verbs:
  84 │   - create
  85 │   - get
  86 └   - update
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'ingress-nginx' shouldn't have access to manage secrets in namespace 'ingress-nginx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 old-ingressctrl_5.yaml:20-30
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - configmaps
  24 │   - pods
  25 │   - secrets
  26 │   - endpoints
  27 │   verbs:
  28 └   - get
  ..   
────────────────────────────────────────



old-ingressctrl_7.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl_7.yaml:14-22
────────────────────────────────────────
  14 ┌   type: ClusterIP
  15 │   ports:
  16 │   - name: https-webhook
  17 │     port: 443
  18 │     targetPort: webhook
  19 │   selector:
  20 │     app.kubernetes.io/name: ingress-nginx
  21 │     app.kubernetes.io/instance: ingress-nginx
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl_7.yaml:14-22
────────────────────────────────────────
  14 ┌   type: ClusterIP
  15 │   ports:
  16 │   - name: https-webhook
  17 │     port: 443
  18 │     targetPort: webhook
  19 │   selector:
  20 │     app.kubernetes.io/name: ingress-nginx
  21 │     app.kubernetes.io/instance: ingress-nginx
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



old-ingressctrl_8.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl_8.yaml:19-33
────────────────────────────────────────
  19 ┌   type: LoadBalancer
  20 │   externalTrafficPolicy: Local
  21 │   ports:
  22 │   - name: http
  23 │     port: 80
  24 │     protocol: TCP
  25 │     targetPort: http
  26 │   - name: https
  27 └     port: 443
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl_8.yaml:19-33
────────────────────────────────────────
  19 ┌   type: LoadBalancer
  20 │   externalTrafficPolicy: Local
  21 │   ports:
  22 │   - name: http
  23 │     port: 80
  24 │     protocol: TCP
  25 │     targetPort: http
  26 │   - name: https
  27 └     port: 443
  ..   
────────────────────────────────────────



old-ingressctrl_9.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 101, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 7, MEDIUM: 6, HIGH: 2, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'controller' of Deployment 'ingress-nginx-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'controller' of Deployment 'ingress-nginx-controller' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 old-ingressctrl_9.yaml:14-107
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 old-ingressctrl_9.yaml:14-107
────────────────────────────────────────
  14 ┌   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: ingress-nginx
  17 │       app.kubernetes.io/instance: ingress-nginx
  18 │       app.kubernetes.io/component: controller
  19 │   revisionHistoryLimit: 10
  20 │   minReadySeconds: 0
  21 │   template:
  22 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "controller" of deployment "ingress-nginx-controller" in "ingress-nginx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment ingress-nginx-controller in ingress-nginx namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ingress-nginx-controller in ingress-nginx namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 old-ingressctrl_9.yaml:28-107
────────────────────────────────────────
  28 ┌       dnsPolicy: ClusterFirst
  29 │       containers:
  30 │       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 └               command:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container controller in deployment ingress-nginx-controller (namespace: ingress-nginx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 old-ingressctrl_9.yaml:30-101
────────────────────────────────────────
  30 ┌       - name: controller
  31 │         image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         lifecycle:
  34 │           preStop:
  35 │             exec:
  36 │               command:
  37 │               - /wait-shutdown
  38 └         args:
  ..   
────────────────────────────────────────



oldnginx.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 13, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'oldnginx' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'oldnginx' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'oldnginx' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'oldnginx' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'oldnginx' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'oldnginx' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'oldnginx' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'oldnginx' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'oldnginx' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'oldnginx' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'oldnginx' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oldnginx.yaml:10-32
────────────────────────────────────────
  10 ┌   replicas: 3
  11 │   selector:
  12 │     matchLabels:
  13 │       app: oldnginx
  14 │   strategy: {}
  15 │   template:
  16 │     metadata:
  17 │       creationTimestamp: null
  18 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oldnginx.yaml:10-32
────────────────────────────────────────
  10 ┌   replicas: 3
  11 │   selector:
  12 │     matchLabels:
  13 │       app: oldnginx
  14 │   strategy: {}
  15 │   template:
  16 │     metadata:
  17 │       creationTimestamp: null
  18 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "oldnginx" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment oldnginx in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 oldnginx.yaml:4-8
────────────────────────────────────────
   4 ┌   creationTimestamp: null
   5 │   labels:
   6 │     app: oldnginx
   7 │     type: canary
   8 └   name: oldnginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oldnginx in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oldnginx.yaml:23-28
────────────────────────────────────────
  23 ┌       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 └           mountPath: /usr/share/nginx/html/
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oldnginx in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oldnginx.yaml:22-32
────────────────────────────────────────
  22 ┌       containers:
  23 │       - image: nginx:1.15
  24 │         name: nginx
  25 │         resources: {}
  26 │         volumeMounts:
  27 │         - name: configfile
  28 │           mountPath: /usr/share/nginx/html/
  29 │       volumes:
  30 └       - name: configfile
  ..   
────────────────────────────────────────



ollama-claim0-persistentvolumeclaim.yaml (kubernetes)
=====================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-claim0-persistentvolumeclaim.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-claim0-persistentvolumeclaim.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────



ollama-claim1-persistentvolumeclaim.yaml (kubernetes)
=====================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-claim1-persistentvolumeclaim.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-claim1-persistentvolumeclaim.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────



ollama-deployment.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-deployment.yaml:6-40
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       name: ollama
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-deployment.yaml:6-40
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       name: ollama
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ollama in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ollama-deployment.yaml:4
────────────────────────────────────────
   4 [   name: ollama
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-deployment.yaml:16-40
────────────────────────────────────────
  16 ┌       affinity:
  17 │         podAntiAffinity:
  18 │           requiredDuringSchedulingIgnoredDuringExecution:
  19 │           - labelSelector:
  20 │               matchExpressions:
  21 │               - key: name
  22 │                 operator: In
  23 │                 values:
  24 └                 - ollama
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama-deployment.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────



ollama-deployment1.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-deployment1.yaml:6-40
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       name: ollama
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-deployment1.yaml:6-40
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       name: ollama
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ollama in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ollama-deployment1.yaml:4
────────────────────────────────────────
   4 [   name: ollama
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-deployment1.yaml:16-40
────────────────────────────────────────
  16 ┌       affinity:
  17 │         podAntiAffinity:
  18 │           requiredDuringSchedulingIgnoredDuringExecution:
  19 │           - labelSelector:
  20 │               matchExpressions:
  21 │               - key: name
  22 │                 operator: In
  23 │                 values:
  24 └                 - ollama
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama-deployment1.yaml:35-40
────────────────────────────────────────
  35 ┌       - name: ollama
  36 │         image: ollama/ollama:latest
  37 │         ports:
  38 │         - name: http
  39 │           containerPort: 11434
  40 └           protocol: TCP
────────────────────────────────────────



ollama-deployment1_1.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-deployment1_1.yaml:6-9
────────────────────────────────────────
   6 ┌   minAvailable: 1
   7 │   selector:
   8 │     matchLabels:
   9 └       app: ollama
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-deployment1_1.yaml:6-9
────────────────────────────────────────
   6 ┌   minAvailable: 1
   7 │   selector:
   8 │     matchLabels:
   9 └       app: ollama
────────────────────────────────────────



ollama-deployment2.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-deployment2.yaml:13-51
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       io.kompose.service: ollama
  17 │   strategy:
  18 │     type: Recreate
  19 │   template:
  20 │     metadata:
  21 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-deployment2.yaml:13-51
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       io.kompose.service: ollama
  17 │   strategy:
  18 │     type: Recreate
  19 │   template:
  20 │     metadata:
  21 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ollama in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ollama-deployment2.yaml:4-11
────────────────────────────────────────
   4 ┌   annotations:
   5 │     kompose.cmd: kompose convert
   6 │     kompose.version: 1.22.0 (955b78124)
   7 │   creationTimestamp: null
   8 │   labels:
   9 │     io.kompose.network/net: "true"
  10 │     io.kompose.service: ollama
  11 └   name: ollama
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-deployment2.yaml:29-51
────────────────────────────────────────
  29 ┌       containers:
  30 │         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 └               memory: 200Mi
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama-deployment2.yaml:30-43
────────────────────────────────────────
  30 ┌         - image: ollama/ollama:latest
  31 │           name: ollama
  32 │           ports:
  33 │             - containerPort: 11434
  34 │           resources:
  35 │             limits:
  36 │               cpu: 10m
  37 │               memory: 200Mi
  38 └           tty: true
  ..   
────────────────────────────────────────



ollama-deployment_1.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-deployment_1.yaml:6-9
────────────────────────────────────────
   6 ┌   minAvailable: 1
   7 │   selector:
   8 │     matchLabels:
   9 └       app: ollama
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-deployment_1.yaml:6-9
────────────────────────────────────────
   6 ┌   minAvailable: 1
   7 │   selector:
   8 │     matchLabels:
   9 └       app: ollama
────────────────────────────────────────



ollama-kube_1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-kube_1.yaml:7-15
────────────────────────────────────────
   7 ┌   storageClassName: ollama-sc
   8 │   capacity:
   9 │     storage: <STORAGE>
  10 │   accessModes:
  11 │   - ReadWriteMany
  12 │   persistentVolumeReclaimPolicy: Retain
  13 │   nfs:
  14 │     path: <NFS_PATH>
  15 └     server: <SERVER_IP>
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-kube_1.yaml:7-15
────────────────────────────────────────
   7 ┌   storageClassName: ollama-sc
   8 │   capacity:
   9 │     storage: <STORAGE>
  10 │   accessModes:
  11 │   - ReadWriteMany
  12 │   persistentVolumeReclaimPolicy: Retain
  13 │   nfs:
  14 │     path: <NFS_PATH>
  15 └     server: <SERVER_IP>
────────────────────────────────────────



ollama-kube_2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-kube_2.yaml:7-12
────────────────────────────────────────
   7 ┌   storageClassName: ollama-sc
   8 │   accessModes:
   9 │   - ReadWriteMany
  10 │   resources:
  11 │     requests:
  12 └       storage: <STORAGE>
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-kube_2.yaml:7-12
────────────────────────────────────────
   7 ┌   storageClassName: ollama-sc
   8 │   accessModes:
   9 │   - ReadWriteMany
  10 │   resources:
  11 │     requests:
  12 └       storage: <STORAGE>
────────────────────────────────────────



ollama-kube_3.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 101, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 8, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama-deployment' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama-deployment' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-kube_3.yaml:7-38
────────────────────────────────────────
   7 ┌   replicas: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       name: ollama
  11 │   strategy:
  12 │     type: Recreate
  13 │   template:
  14 │     metadata:
  15 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-kube_3.yaml:7-38
────────────────────────────────────────
   7 ┌   replicas: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       name: ollama
  11 │   strategy:
  12 │     type: Recreate
  13 │   template:
  14 │     metadata:
  15 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama-deployment" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama-deployment in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama-deployment in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-kube_3.yaml:18-38
────────────────────────────────────────
  18 ┌       containers:
  19 │       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 └             cpu: '2'
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama-deployment (namespace: ollama) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama-kube_3.yaml:19-33
────────────────────────────────────────
  19 ┌       - env: null
  20 │         image: ollama/ollama:latest
  21 │         name: ollama
  22 │         ports:
  23 │         - containerPort: 11434
  24 │         resources:
  25 │           limits:
  26 │             cpu: '2'
  27 └             memory: 6Gi
  ..   
────────────────────────────────────────



ollama-kube_4.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-kube_4.yaml:7-15
────────────────────────────────────────
   7 ┌   type: LoadBalancer
   8 │   externalTrafficPolicy: Cluster
   9 │   ports:
  10 │   - name: http
  11 │     protocol: TCP
  12 │     port: 80
  13 │     targetPort: 11434
  14 │   selector:
  15 └     name: ollama
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-kube_4.yaml:7-15
────────────────────────────────────────
   7 ┌   type: LoadBalancer
   8 │   externalTrafficPolicy: Cluster
   9 │   ports:
  10 │   - name: http
  11 │     protocol: TCP
  12 │     port: 80
  13 │     targetPort: 11434
  14 │   selector:
  15 └     name: ollama
────────────────────────────────────────



ollama-kube_5.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-kube_5.yaml:7
────────────────────────────────────────
   7 [   controller: nginx.org/ingress-controller
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-kube_5.yaml:7
────────────────────────────────────────
   7 [   controller: nginx.org/ingress-controller
────────────────────────────────────────



ollama-kube_6.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-kube_6.yaml:7-27
────────────────────────────────────────
   7 ┌   defaultBackend:
   8 │     service:
   9 │       name: ollama-service
  10 │       port:
  11 │         number: 80
  12 │   ingressClassName: ollama-ingressclass
  13 │   tls:
  14 │   - hosts:
  15 └     - ollama.nb41.tech
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-kube_6.yaml:7-27
────────────────────────────────────────
   7 ┌   defaultBackend:
   8 │     service:
   9 │       name: ollama-service
  10 │       port:
  11 │         number: 80
  12 │   ingressClassName: ollama-ingressclass
  13 │   tls:
  14 │   - hosts:
  15 └     - ollama.nb41.tech
  ..   
────────────────────────────────────────



ollama-nvidia.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-nvidia.yaml:7-25
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       name: ollama
  10 │   replicas: 2
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         name: ollama
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-nvidia.yaml:7-25
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       name: ollama
  10 │   replicas: 2
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         name: ollama
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-nvidia.yaml:16-25
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: ollama) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama-nvidia.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────



ollama-pvc.yaml (kubernetes)
============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-pvc.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-pvc.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────



ollama-pvc2.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-pvc2.yaml:7-11
────────────────────────────────────────
   7 ┌   accessModes:
   8 │     - ReadWriteOnce
   9 │   resources:
  10 │     requests:
  11 └       storage: 30Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-pvc2.yaml:7-11
────────────────────────────────────────
   7 ┌   accessModes:
   8 │     - ReadWriteOnce
   9 │   resources:
  10 │     requests:
  11 └       storage: 30Gi
────────────────────────────────────────



ollama-service.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-service.yaml:7-13
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 11434
  12 │     name: http
  13 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-service.yaml:7-13
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 11434
  12 │     name: http
  13 └     protocol: TCP
────────────────────────────────────────



ollama-service1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-service1.yaml:7-13
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 11434
  12 │     name: http
  13 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-service1.yaml:7-13
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 11434
  12 │     name: http
  13 └     protocol: TCP
────────────────────────────────────────



ollama-service2.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-service2.yaml:7-18
────────────────────────────────────────
   7 ┌   ipFamilies:
   8 │     - IPv4
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 8501
  12 │       targetPort: 8501
  13 │   internalTrafficPolicy: Cluster
  14 │   type: ClusterIP
  15 └   ipFamilyPolicy: SingleStack
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-service2.yaml:7-18
────────────────────────────────────────
   7 ┌   ipFamilies:
   8 │     - IPv4
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 8501
  12 │       targetPort: 8501
  13 │   internalTrafficPolicy: Cluster
  14 │   type: ClusterIP
  15 └   ipFamilyPolicy: SingleStack
  ..   
────────────────────────────────────────



ollama-service3.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-service3.yaml:12-17
────────────────────────────────────────
  12 ┌   ports:
  13 │     - name: "11434"
  14 │       port: 11434
  15 │       targetPort: 11434
  16 │   selector:
  17 └     io.kompose.service: ollama
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-service3.yaml:12-17
────────────────────────────────────────
  12 ┌   ports:
  13 │     - name: "11434"
  14 │       port: 11434
  15 │       targetPort: 11434
  16 │   selector:
  17 └     io.kompose.service: ollama
────────────────────────────────────────



ollama-service5.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-service5.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ollama
   8 │   ports:
   9 │   - port: 80
  10 │     targetPort: 80
  11 └   type: LoadBalancer
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-service5.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ollama
   8 │   ports:
   9 │   - port: 80
  10 │     targetPort: 80
  11 └   type: LoadBalancer
────────────────────────────────────────



ollama-statefulset.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of StatefulSet 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of StatefulSet 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'statefulset' 'ollama' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of StatefulSet 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of StatefulSet 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of StatefulSet 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of StatefulSet 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of StatefulSet 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of StatefulSet 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of StatefulSet 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of StatefulSet 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of StatefulSet 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-statefulset.yaml:7-35
────────────────────────────────────────
   7 ┌   serviceName: ollama
   8 │   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         app: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-statefulset.yaml:7-35
────────────────────────────────────────
   7 ┌   serviceName: ollama
   8 │   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         app: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of statefulset "ollama" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0118 (HIGH): statefulset ollama in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama-statefulset.yaml:17-25
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in statefulset ollama (namespace: ollama) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama-statefulset.yaml:18-25
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         ports:
  21 │         - containerPort: 11434
  22 │         volumeMounts:
  23 │         - name: ollama-volume
  24 │           mountPath: /root/.ollama
  25 └         tty: true
────────────────────────────────────────



ollama-statefulset_1.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama-statefulset_1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: ollama
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 11434
  12 └     targetPort: 11434
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama-statefulset_1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: ollama
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 11434
  12 └     targetPort: 11434
────────────────────────────────────────



ollama2_1.yaml (kubernetes)
===========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama2_1.yaml:7-24
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       name: ollama
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         name: ollama
  14 │     spec:
  15 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama2_1.yaml:7-24
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       name: ollama
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         name: ollama
  14 │     spec:
  15 └       containers:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama2_1.yaml:15-24
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: ollama) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama2_1.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: ollama
  17 │         image: ollama/ollama
  18 │         ports:
  19 │         - name: http
  20 │           containerPort: 8080
  21 │           protocol: TCP
  22 │         env:
  23 │         - name: OLLAMA_HOST
  24 └           value: 0.0.0.0:8080
────────────────────────────────────────



ollama2_2.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama2_2.yaml:7-14
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 8080
  12 │     name: http
  13 │     targetPort: http
  14 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama2_2.yaml:7-14
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 8080
  12 │     name: http
  13 │     targetPort: http
  14 └     protocol: TCP
────────────────────────────────────────



ollama5.yaml (kubernetes)
=========================
Tests: 117 (SUCCESSES: 95, FAILURES: 22)
Failures: 22 (UNKNOWN: 0, LOW: 13, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'ollama' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 ollama5.yaml:8-39
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: ollama
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama5.yaml:8-39
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: ollama
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama5.yaml:8-39
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: ollama
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ollama in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ollama5.yaml:4-6
────────────────────────────────────────
   4 ┌   name: ollama
   5 │   labels:
   6 └     app: ollama
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama5.yaml:17-39
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 └           mountPath: /code
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollama5.yaml:18-30
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: ollama/ollama:latest
  20 │         imagePullPolicy: IfNotPresent
  21 │         ports:
  22 │         - containerPort: 11434
  23 │         volumeMounts:
  24 │         - name: code
  25 │           mountPath: /code
  26 └         - name: ollama
  ..   
────────────────────────────────────────



ollama5_1.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama5_1.yaml:6-13
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ollama
   8 │   type: NodePort
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 11434
  12 │     targetPort: 11434
  13 └     nodePort: 30722
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama5_1.yaml:6-13
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ollama
   8 │   type: NodePort
   9 │   ports:
  10 │   - protocol: TCP
  11 │     port: 11434
  12 │     targetPort: 11434
  13 └     nodePort: 30722
────────────────────────────────────────



ollama_gpu1_1.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama_gpu1_1.yaml:7-38
────────────────────────────────────────
   7 ┌   strategy:
   8 │     type: Recreate
   9 │   selector:
  10 │     matchLabels:
  11 │       name: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama_gpu1_1.yaml:7-38
────────────────────────────────────────
   7 ┌   strategy:
   8 │     type: Recreate
   9 │   selector:
  10 │     matchLabels:
  11 │       name: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama_gpu1_1.yaml:18-34
────────────────────────────────────────
  18 ┌       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  26 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama_gpu1_1.yaml:17-38
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: ollama
  19 │         image: rag:latest
  20 │         imagePullPolicy: Never
  21 │         env:
  22 │         - name: PATH
  23 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  24 │         - name: LD_LIBRARY_PATH
  25 └           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  ..   
────────────────────────────────────────



ollama_gpu1_2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama_gpu1_2.yaml:7-14
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 80
  12 │     name: http
  13 │     targetPort: http
  14 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama_gpu1_2.yaml:7-14
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 80
  12 │     name: http
  13 │     targetPort: http
  14 └     protocol: TCP
────────────────────────────────────────



ollama_gpu_1.yaml (kubernetes)
==============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama_gpu_1.yaml:7-45
────────────────────────────────────────
   7 ┌   strategy:
   8 │     type: Recreate
   9 │   selector:
  10 │     matchLabels:
  11 │       name: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama_gpu_1.yaml:7-45
────────────────────────────────────────
   7 ┌   strategy:
   8 │     type: Recreate
   9 │   selector:
  10 │     matchLabels:
  11 │       name: ollama
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 └         name: ollama
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama_gpu_1.yaml:22-41
────────────────────────────────────────
  22 ┌       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 │           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  30 └         - name: NVIDIA_DRIVER_CAPABILITIES
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollama_gpu_1.yaml:21-45
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: ollama
  23 │         image: ollama-metrics:latest
  24 │         imagePullPolicy: Never
  25 │         env:
  26 │         - name: PATH
  27 │           value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  28 │         - name: LD_LIBRARY_PATH
  29 └           value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
  ..   
────────────────────────────────────────



ollama_gpu_2.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama_gpu_2.yaml:7-14
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 80
  12 │     name: http
  13 │     targetPort: http
  14 └     protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama_gpu_2.yaml:7-14
────────────────────────────────────────
   7 ┌   type: ClusterIP
   8 │   selector:
   9 │     name: ollama
  10 │   ports:
  11 │   - port: 80
  12 │     name: http
  13 │     targetPort: http
  14 └     protocol: TCP
────────────────────────────────────────



ollama_metrics.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollama_metrics.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     name: ollama
   9 │   ports:
  10 │   - name: monitoring
  11 │     port: 8445
  12 └     targetPort: 8445
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollama_metrics.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     name: ollama
   9 │   ports:
  10 │   - name: monitoring
  11 │     port: 8445
  12 └     targetPort: 8445
────────────────────────────────────────



ollamadeployment.yaml (kubernetes)
==================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ollama' of Deployment 'ollama' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ollama' of 'deployment' 'ollama' in 'ollama' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ollama' of Deployment 'ollama' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ollama' of Deployment 'ollama' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'ollama' of Deployment 'ollama' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ollama' of Deployment 'ollama' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ollamadeployment.yaml:7-27
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       name: ollama
  10 │   replicas: 2
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         name: ollama
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ollamadeployment.yaml:7-27
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       name: ollama
  10 │   replicas: 2
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         name: ollama
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ollama" of deployment "ollama" in "ollama" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ollama in ollama namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ollama in ollama namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ollamadeployment.yaml:16-27
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 └           containerPort: 11434
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container ollama in deployment ollama (namespace: ollama) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 ollamadeployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: ollama
  18 │         image: ollama/ollama:latest
  19 │         resources:
  20 │           limits:
  21 │             nvidia.com/gpu: 1
  22 │         ports:
  23 │         - name: http
  24 │           containerPort: 11434
  25 └           protocol: TCP
────────────────────────────────────────



olm1_3.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 olm1_3.yaml:6-11
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - '*'
  10 │   verbs:
  11 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:controller:operator-lifecycle-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 olm1_3.yaml:6-11
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - '*'
  10 │   verbs:
  11 └   - '*'
────────────────────────────────────────



olm1_5.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'olm-operator' of 'deployment' 'olm-operator' in 'olm' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm1_5.yaml:9-58
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm1_5.yaml:9-58
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "olm-operator" of deployment "olm-operator" in "olm" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container olm-operator in olm namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment olm-operator in olm namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm1_5.yaml:20-58
────────────────────────────────────────
  20 ┌       serviceAccountName: olm-operator-serviceaccount
  21 │       containers:
  22 │       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 └         - --writeStatusName
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container olm-operator in deployment olm-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm1_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────



olm1_6.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'catalog-operator' of 'deployment' 'catalog-operator' in 'olm' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm1_6.yaml:9-52
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm1_6.yaml:9-52
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "catalog-operator" of deployment "catalog-operator" in "olm" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container catalog-operator in olm namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment catalog-operator in olm namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm1_6.yaml:20-52
────────────────────────────────────────
  20 ┌       serviceAccountName: olm-operator-serviceaccount
  21 │       containers:
  22 │       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 └         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container catalog-operator in deployment catalog-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm1_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────



olm2_3.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'system:controller:operator-lifecycle-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 olm2_3.yaml:6-20
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - '*'
  10 │   verbs:
  11 │   - watch
  12 │   - list
  13 │   - get
  14 └   - create
  ..   
────────────────────────────────────────



olm2_6.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 108, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm2_6.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm2_6.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm2_6.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm2_6.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm2_6.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm2_6.yaml:9-67
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm2_6.yaml:9-67
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container olm-operator in deployment olm-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm2_6.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────



olm2_7.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 108, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm2_7.yaml:26-60
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm2_7.yaml:26-60
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm2_7.yaml:26-60
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm2_7.yaml:26-60
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm2_7.yaml:26-60
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm2_7.yaml:9-62
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm2_7.yaml:9-62
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container catalog-operator in deployment catalog-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm2_7.yaml:26-60
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────



olm3_1.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0046 (CRITICAL): ClusterRole 'system:controller:operator-lifecycle-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 olm3_1.yaml:6-20
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - '*'
  10 │   verbs:
  11 │   - watch
  12 │   - list
  13 │   - get
  14 └   - create
  ..   
────────────────────────────────────────



olm3_4.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 108, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm3_4.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm3_4.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm3_4.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm3_4.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm3_4.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm3_4.yaml:9-67
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm3_4.yaml:9-67
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container olm-operator in deployment olm-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm3_4.yaml:26-65
────────────────────────────────────────
  26 ┌       - name: olm-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/olm
  34 └         args:
  ..   
────────────────────────────────────────



olm3_5.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 108, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm3_5.yaml:26-61
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm3_5.yaml:26-61
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm3_5.yaml:26-61
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm3_5.yaml:26-61
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm3_5.yaml:26-61
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm3_5.yaml:9-63
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm3_5.yaml:9-63
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: Recreate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container catalog-operator in deployment catalog-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm3_5.yaml:26-61
────────────────────────────────────────
  26 ┌       - name: catalog-operator
  27 │         securityContext:
  28 │           allowPrivilegeEscalation: false
  29 │           capabilities:
  30 │             drop:
  31 │             - ALL
  32 │         command:
  33 │         - /bin/catalog
  34 └         args:
  ..   
────────────────────────────────────────



olm_3.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 olm_3.yaml:6-11
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - '*'
  10 │   verbs:
  11 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'system:controller:operator-lifecycle-manager' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 olm_3.yaml:6-11
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - '*'
   8 │   resources:
   9 │   - '*'
  10 │   verbs:
  11 └   - '*'
────────────────────────────────────────



olm_5.yaml (kubernetes)
=======================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'olm-operator' of 'deployment' 'olm-operator' in 'olm' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'olm-operator' of Deployment 'olm-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm_5.yaml:9-58
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm_5.yaml:9-58
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: olm-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "olm-operator" of deployment "olm-operator" in "olm" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container olm-operator in olm namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment olm-operator in olm namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm_5.yaml:20-58
────────────────────────────────────────
  20 ┌       serviceAccountName: olm-operator-serviceaccount
  21 │       containers:
  22 │       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 └         - --writeStatusName
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container olm-operator in deployment olm-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm_5.yaml:22-56
────────────────────────────────────────
  22 ┌       - name: olm-operator
  23 │         command:
  24 │         - /bin/olm
  25 │         args:
  26 │         - --namespace
  27 │         - $(OPERATOR_NAMESPACE)
  28 │         - --writeStatusName
  29 │         - ''
  30 └         image: quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────



olm_6.yaml (kubernetes)
=======================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'catalog-operator' of 'deployment' 'catalog-operator' in 'olm' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'catalog-operator' of Deployment 'catalog-operator' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 olm_6.yaml:9-52
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 olm_6.yaml:9-52
────────────────────────────────────────
   9 ┌   strategy:
  10 │     type: RollingUpdate
  11 │   replicas: 1
  12 │   selector:
  13 │     matchLabels:
  14 │       app: catalog-operator
  15 │   template:
  16 │     metadata:
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "catalog-operator" of deployment "catalog-operator" in "olm" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container catalog-operator in olm namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment catalog-operator in olm namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 olm_6.yaml:20-52
────────────────────────────────────────
  20 ┌       serviceAccountName: olm-operator-serviceaccount
  21 │       containers:
  22 │       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 └         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container catalog-operator in deployment catalog-operator (namespace: olm) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 olm_6.yaml:22-50
────────────────────────────────────────
  22 ┌       - name: catalog-operator
  23 │         command:
  24 │         - /bin/catalog
  25 │         args:
  26 │         - -namespace
  27 │         - olm
  28 │         - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  29 │         - -util-image
  30 └         - quay.io/operator-framework/olm@sha256:e74b2ac57963c7f3ba19122a8c31c9f2a0deb3c0c5cac9e5323ccffd0ca198ed
  ..   
────────────────────────────────────────



omada-controller2.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 omada-controller2.yaml:8-43
────────────────────────────────────────
   8 ┌   selector:
   9 │     app: omada-controller
  10 │   type: LoadBalancer
  11 │   ports:
  12 │   - name: http
  13 │     port: 80
  14 │     protocol: TCP
  15 │     targetPort: http
  16 └   - name: manage-https
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 omada-controller2.yaml:8-43
────────────────────────────────────────
   8 ┌   selector:
   9 │     app: omada-controller
  10 │   type: LoadBalancer
  11 │   ports:
  12 │   - name: http
  13 │     port: 80
  14 │     protocol: TCP
  15 │     targetPort: http
  16 └   - name: manage-https
  ..   
────────────────────────────────────────



omada-controller2_1.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 95, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 6, HIGH: 2, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'omada-controller' of 'statefulset' 'omada-controller' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'omada-controller' of StatefulSet 'omada-controller' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'omada-controller' of StatefulSet 'omada-controller' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 omada-controller2_1.yaml:8-98
────────────────────────────────────────
   8 ┌   selector:
   9 │     matchLabels:
  10 │       app: omada-controller
  11 │   serviceName: omada-controller
  12 │   replicas: 1
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 └         app: omada-controller
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 omada-controller2_1.yaml:8-98
────────────────────────────────────────
   8 ┌   selector:
   9 │     matchLabels:
  10 │       app: omada-controller
  11 │   serviceName: omada-controller
  12 │   replicas: 1
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 └         app: omada-controller
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "omada-controller" of statefulset "omada-controller" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): statefulset omada-controller in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 omada-controller2_1.yaml:4-6
────────────────────────────────────────
   4 ┌   name: omada-controller
   5 │   labels:
   6 └     app: omada-controller
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): statefulset omada-controller in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container omada-controller in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container omada-controller in statefulset omada-controller (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 omada-controller2_1.yaml:23-73
────────────────────────────────────────
  23 ┌       - name: omada-controller
  24 │         image: mbentley/omada-controller
  25 │         env:
  26 │         - name: MANAGE_HTTP_PORT
  27 │           value: '80'
  28 │         - name: MANAGE_HTTPS_PORT
  29 │           value: '443'
  30 │         - name: PORTAL_HTTP_PORT
  31 └           value: '8088'
  ..   
────────────────────────────────────────



omelete.yaml (kubernetes)
=========================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Pod 'nginx-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'pod' 'nginx-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'nginx' of Pod 'nginx-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 omelete.yaml:9-20
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 │       mountPath: /usr/share/nginx/html
  17 └   volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 omelete.yaml:9-20
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 │       mountPath: /usr/share/nginx/html
  17 └   volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of pod "nginx-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nginx-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 omelete.yaml:5-7
────────────────────────────────────────
   5 ┌   labels:
   6 │     storage: local
   7 └   name: nginx-pod
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod nginx-pod in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omelete.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nginx-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omelete.yaml:9-20
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 │       mountPath: /usr/share/nginx/html
  17 └   volumes:
  ..   
────────────────────────────────────────



omelete1.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Pod 'nginx-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'pod' 'nginx-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'nginx' of Pod 'nginx-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Pod 'nginx-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 omelete1.yaml:9-20
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 │       mountPath: /usr/share/nginx/html
  17 └   volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 omelete1.yaml:9-20
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 │       mountPath: /usr/share/nginx/html
  17 └   volumes:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of pod "nginx-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nginx-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 omelete1.yaml:5-7
────────────────────────────────────────
   5 ┌   labels:
   6 │     storage: local
   7 └   name: nginx-pod
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod nginx-pod in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omelete1.yaml:10-16
────────────────────────────────────────
  10 ┌   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 └       mountPath: /usr/share/nginx/html
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nginx-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omelete1.yaml:9-20
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: nginx
  11 │     image: nginx:latest
  12 │     ports:
  13 │     - containerPort: 80
  14 │     volumeMounts:
  15 │     - name: garcom-pvc
  16 │       mountPath: /usr/share/nginx/html
  17 └   volumes:
  ..   
────────────────────────────────────────



omni-es-deployment.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'fn-cicd-basic-api-elasticsearch' of 'pod' 'fn-cicd-basic-api-elasticsearch' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'fn-cicd-basic-api-elasticsearch' of Pod 'fn-cicd-basic-api-elasticsearch' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 omni-es-deployment.yaml:9-19
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 └     ports:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 omni-es-deployment.yaml:9-19
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 └     ports:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "fn-cicd-basic-api-elasticsearch" of pod "fn-cicd-basic-api-elasticsearch" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod fn-cicd-basic-api-elasticsearch in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 omni-es-deployment.yaml:4-7
────────────────────────────────────────
   4 ┌   labels:
   5 │     run: fn-cicd-basic-api-elasticsearch
   6 │   name: fn-cicd-basic-api-elasticsearch
   7 └   namespace: default
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container fn-cicd-basic-api-elasticsearch in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omni-es-deployment.yaml:10-19
────────────────────────────────────────
  10 ┌   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 │     ports:
  18 │     - containerPort: 9201
  19 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod fn-cicd-basic-api-elasticsearch in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 omni-es-deployment.yaml:9-19
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: fn-cicd-basic-api-elasticsearch
  11 │     image: fn-cicd-basic-api:omni_es
  12 │     resources:
  13 │       requests:
  14 │         memory: 1Gi
  15 │       limits:
  16 │         memory: 1Gi
  17 └     ports:
  ..   
────────────────────────────────────────



omni-es-deployment_1.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 omni-es-deployment_1.yaml:9-15
────────────────────────────────────────
   9 ┌   type: NodePort
  10 │   ports:
  11 │   - port: 9201
  12 │     protocol: TCP
  13 │   selector:
  14 │     run: fn-cicd-basic-api-elasticsearch
  15 └   sessionAffinity: None
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 omni-es-deployment_1.yaml:9-15
────────────────────────────────────────
   9 ┌   type: NodePort
  10 │   ports:
  11 │   - port: 9201
  12 │     protocol: TCP
  13 │   selector:
  14 │     run: fn-cicd-basic-api-elasticsearch
  15 └   sessionAffinity: None
────────────────────────────────────────



oms-client-deployment.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oms-client' of 'deployment' 'oms-client-deployment' in 'oms' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-client-deployment.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 2
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-client
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-client
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-client-deployment.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 2
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-client
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-client
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oms-client" of deployment "oms-client-deployment" in "oms" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment oms-client-deployment in oms namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oms-client-deployment in oms namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oms-client-deployment in oms namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-client-deployment.yaml:16-23
────────────────────────────────────────
  16 ┌       containers:
  17 │         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oms-client in deployment oms-client-deployment (namespace: oms) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oms-client-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────



oms-client-deployment1.yaml (kubernetes)
========================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oms-client' of 'deployment' 'oms-client-deployment' in 'oms' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oms-client' of Deployment 'oms-client-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-client-deployment1.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 2
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-client
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-client
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-client-deployment1.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 2
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-client
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-client
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oms-client" of deployment "oms-client-deployment" in "oms" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment oms-client-deployment in oms namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oms-client-deployment in oms namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oms-client-deployment in oms namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-client-deployment1.yaml:16-23
────────────────────────────────────────
  16 ┌       containers:
  17 │         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oms-client in deployment oms-client-deployment (namespace: oms) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oms-client-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-client
  18 │           image: staticaldust/oms-client:3.0.0
  19 │           ports:
  20 │             - containerPort: 80
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-client-configmap
────────────────────────────────────────



oms-client-service.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-client-service.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-client
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 └       targetPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-client-service.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-client
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 └       targetPort: 80
────────────────────────────────────────



oms-client-service1.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-client-service1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-client
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 └       targetPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-client-service1.yaml:7-12
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-client
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 └       targetPort: 80
────────────────────────────────────────



oms-ingress.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-ingress.yaml:10-27
────────────────────────────────────────
  10 ┌   rules:
  11 │     - host: ae73adf0061dc4d49ab515bf812150d9-1090975536.eu-central-1.elb.amazonaws.com
  12 │       http:
  13 │         paths:
  14 │           - path: /oms(/|$)(.*)
  15 │             pathType: ImplementationSpecific
  16 │             backend:
  17 │               service:
  18 └                 name: oms-client-service
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-ingress.yaml:10-27
────────────────────────────────────────
  10 ┌   rules:
  11 │     - host: ae73adf0061dc4d49ab515bf812150d9-1090975536.eu-central-1.elb.amazonaws.com
  12 │       http:
  13 │         paths:
  14 │           - path: /oms(/|$)(.*)
  15 │             pathType: ImplementationSpecific
  16 │             backend:
  17 │               service:
  18 └                 name: oms-client-service
  ..   
────────────────────────────────────────



oms-ingress1.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-ingress1.yaml:10-27
────────────────────────────────────────
  10 ┌   rules:
  11 │     - host: ae73adf0061dc4d49ab515bf812150d9-1090975536.eu-central-1.elb.amazonaws.com
  12 │       http:
  13 │         paths:
  14 │           - path: /oms(/|$)(.*)
  15 │             pathType: ImplementationSpecific
  16 │             backend:
  17 │               service:
  18 └                 name: oms-client-service
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-ingress1.yaml:10-27
────────────────────────────────────────
  10 ┌   rules:
  11 │     - host: ae73adf0061dc4d49ab515bf812150d9-1090975536.eu-central-1.elb.amazonaws.com
  12 │       http:
  13 │         paths:
  14 │           - path: /oms(/|$)(.*)
  15 │             pathType: ImplementationSpecific
  16 │             backend:
  17 │               service:
  18 └                 name: oms-client-service
  ..   
────────────────────────────────────────



oms-server-configmap.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'oms-server-configmap' in 'oms' namespace stores sensitive contents in key(s) or value(s) '{"PORT", "mongodb", "postgresql"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────


AVD-KSV-0109 (HIGH): ConfigMap 'oms-server-configmap' in 'oms' namespace stores secrets in key(s) or value(s) '{"JWT_SECRET"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



oms-server-configmap1.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'oms-server-configmap' in 'oms' namespace stores sensitive contents in key(s) or value(s) '{"PORT", "mongodb", "postgresql"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────


AVD-KSV-0109 (HIGH): ConfigMap 'oms-server-configmap' in 'oms' namespace stores secrets in key(s) or value(s) '{"JWT_SECRET"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



oms-server-deployment.yaml (kubernetes)
=======================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oms-server' of 'deployment' 'oms-server-deployment' in 'oms' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-server-deployment.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-server
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-server
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-server-deployment.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-server
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-server
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oms-server" of deployment "oms-server-deployment" in "oms" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oms-server-deployment in oms namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oms-server-deployment in oms namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-server-deployment.yaml:16-23
────────────────────────────────────────
  16 ┌       containers:
  17 │         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oms-server in deployment oms-server-deployment (namespace: oms) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oms-server-deployment.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────



oms-server-deployment1.yaml (kubernetes)
========================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'oms-server' of 'deployment' 'oms-server-deployment' in 'oms' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'oms-server' of Deployment 'oms-server-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-server-deployment1.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-server
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-server
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-server-deployment1.yaml:7-23
────────────────────────────────────────
   7 ┌   replicas: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: oms-server
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: oms-server
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "oms-server" of deployment "oms-server-deployment" in "oms" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container oms-server-deployment in oms namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment oms-server-deployment in oms namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 oms-server-deployment1.yaml:16-23
────────────────────────────────────────
  16 ┌       containers:
  17 │         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container oms-server in deployment oms-server-deployment (namespace: oms) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 oms-server-deployment1.yaml:17-23
────────────────────────────────────────
  17 ┌         - name: oms-server
  18 │           image: staticaldust/oms-server:3.0.7
  19 │           ports:
  20 │             - containerPort: 5000
  21 │           envFrom:
  22 │             - configMapRef:
  23 └                 name: oms-server-configmap
────────────────────────────────────────



oms-server-service.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-server-service.yaml:7-13
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-server
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 │       targetPort: 5000
  13 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-server-service.yaml:7-13
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-server
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 │       targetPort: 5000
  13 └   type: ClusterIP
────────────────────────────────────────



oms-server-service1.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 oms-server-service1.yaml:7-13
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-server
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 │       targetPort: 5000
  13 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 oms-server-service1.yaml:7-13
────────────────────────────────────────
   7 ┌   selector:
   8 │     app: oms-server
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 │       targetPort: 5000
  13 └   type: ClusterIP
────────────────────────────────────────



ondemand-sidecar-injector-role.yaml (kubernetes)
================================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'ondemand-sidecar-injector-role' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 ondemand-sidecar-injector-role.yaml:6-8
────────────────────────────────────────
   6 ┌ - apiGroups: ["", "apps"]
   7 │   resources: ["deployments", "pods"]
   8 └   verbs: ["get", "list", "update"]
────────────────────────────────────────



ondevicedeploy.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'tango-ondevice' of 'deployment' 'ondevice' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'tango-ondevice' of Deployment 'ondevice' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ondevicedeploy.yaml:6-36
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: ondevice
   9 │   template:
  10 │     metadata:
  11 │       labels:
  12 │         app: ondevice
  13 │     spec:
  14 └       nodeName: etri-1
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ondevicedeploy.yaml:6-36
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: ondevice
   9 │   template:
  10 │     metadata:
  11 │       labels:
  12 │         app: ondevice
  13 │     spec:
  14 └       nodeName: etri-1
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "tango-ondevice" of deployment "ondevice" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ondevice in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ondevicedeploy.yaml:4
────────────────────────────────────────
   4 [   name: ondevice
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ondevice in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ondevicedeploy.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ondevice in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ondevicedeploy.yaml:14-36
────────────────────────────────────────
  14 ┌       nodeName: etri-1
  15 │       containers:
  16 │       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 └             memory: 2048Mi
  ..   
────────────────────────────────────────



ondevicedeploy1.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'tango-ondevice' of 'deployment' 'ondevice' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'tango-ondevice' of Deployment 'ondevice' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'tango-ondevice' of Deployment 'ondevice' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ondevicedeploy1.yaml:6-36
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: ondevice
   9 │   template:
  10 │     metadata:
  11 │       labels:
  12 │         app: ondevice
  13 │     spec:
  14 └       nodeName: etri-1
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ondevicedeploy1.yaml:6-36
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: ondevice
   9 │   template:
  10 │     metadata:
  11 │       labels:
  12 │         app: ondevice
  13 │     spec:
  14 └       nodeName: etri-1
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "tango-ondevice" of deployment "ondevice" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment ondevice in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 ondevicedeploy1.yaml:4
────────────────────────────────────────
   4 [   name: ondevice
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container ondevice in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ondevicedeploy1.yaml:16-32
────────────────────────────────────────
  16 ┌       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 │             memory: 2048Mi
  23 │         volumeMounts:
  24 └         - mountPath: /tango
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment ondevice in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 ondevicedeploy1.yaml:14-36
────────────────────────────────────────
  14 ┌       nodeName: etri-1
  15 │       containers:
  16 │       - name: tango-ondevice
  17 │         image: tango_ondevice_deploy
  18 │         imagePullPolicy: Never
  19 │         resources:
  20 │           limits:
  21 │             cpu: 500m
  22 └             memory: 2048Mi
  ..   
────────────────────────────────────────



ondevicedeploy1_1.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ondevicedeploy1_1.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ondevice
   8 │   ports:
   9 │   - port: 8891
  10 └     targetPort: 8891
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ondevicedeploy1_1.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ondevice
   8 │   ports:
   9 │   - port: 8891
  10 └     targetPort: 8891
────────────────────────────────────────



ondevicedeploy_1.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 ondevicedeploy_1.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ondevice
   8 │   ports:
   9 │   - port: 8891
  10 └     targetPort: 8891
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 ondevicedeploy_1.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: ondevice
   8 │   ports:
   9 │   - port: 8891
  10 └     targetPort: 8891
────────────────────────────────────────



one-app-deploy.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'mywebapp' of 'deployment' 'frintend-myapp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'mywebapp' of Deployment 'frintend-myapp' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'mywebapp' of Deployment 'frintend-myapp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 one-app-deploy.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 one-app-deploy.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "mywebapp" of deployment "frintend-myapp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment frintend-myapp in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 one-app-deploy.yaml:4-6
────────────────────────────────────────
   4 ┌   name: frintend-myapp
   5 │   labels:
   6 └     type: webapp
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container frintend-myapp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment frintend-myapp in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy.yaml:14-16
────────────────────────────────────────
  14 ┌       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────



one-app-deploy1.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'mywebapp' of Deployment 'myapp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'mywebapp' of 'deployment' 'myapp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'mywebapp' of Deployment 'myapp' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 one-app-deploy1.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 one-app-deploy1.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "mywebapp" of deployment "myapp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment myapp in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 one-app-deploy1.yaml:4-6
────────────────────────────────────────
   4 ┌   name: myapp
   5 │   labels:
   6 └     type: webapp
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container myapp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy1.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment myapp in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy1.yaml:14-16
────────────────────────────────────────
  14 ┌       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────



one-app-deploy2.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'mywebapp' of 'deployment' 'frontend-myapp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'mywebapp' of Deployment 'frontend-myapp' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'mywebapp' of Deployment 'frontend-myapp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 one-app-deploy2.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 one-app-deploy2.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "mywebapp" of deployment "frontend-myapp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment frontend-myapp in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 one-app-deploy2.yaml:4-6
────────────────────────────────────────
   4 ┌   name: frontend-myapp
   5 │   labels:
   6 └     type: webapp
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container frontend-myapp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy2.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment frontend-myapp in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy2.yaml:14-16
────────────────────────────────────────
  14 ┌       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────



one-app-deploy3.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'mywebapp' of Deployment 'myapp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'mywebapp' of 'deployment' 'myapp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'mywebapp' of Deployment 'myapp' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'mywebapp' of Deployment 'myapp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 one-app-deploy3.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 one-app-deploy3.yaml:8-20
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: myapp-pod
  11 │       labels:
  12 │         type: webapp
  13 │     spec:
  14 │       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "mywebapp" of deployment "myapp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment myapp in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 one-app-deploy3.yaml:4-6
────────────────────────────────────────
   4 ┌   name: myapp
   5 │   labels:
   6 └     type: webapp
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container myapp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy3.yaml:15-16
────────────────────────────────────────
  15 ┌         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment myapp in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-app-deploy3.yaml:14-16
────────────────────────────────────────
  14 ┌       containers:
  15 │         - name: mywebapp
  16 └           image: nginx
────────────────────────────────────────



one-container-pod.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'redis-pod' of Pod 'redis-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'redis-pod' of Pod 'redis-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'redis-pod' of 'pod' 'redis-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'redis-pod' of Pod 'redis-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'redis-pod' of Pod 'redis-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'redis-pod' of Pod 'redis-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'redis-pod' of Pod 'redis-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'redis-pod' of Pod 'redis-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'redis-pod' of Pod 'redis-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'redis-pod' of Pod 'redis-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis-pod' of Pod 'redis-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis-pod' of Pod 'redis-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 one-container-pod.yaml:7-11
────────────────────────────────────────
   7 ┌   containers:
   8 │     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 one-container-pod.yaml:7-11
────────────────────────────────────────
   7 ┌   containers:
   8 │     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "redis-pod" of pod "redis-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod redis-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 one-container-pod.yaml:4
────────────────────────────────────────
   4 [   name: redis-pod
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod redis-pod in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container redis-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-container-pod.yaml:8-11
────────────────────────────────────────
   8 ┌     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod redis-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 one-container-pod.yaml:7-11
────────────────────────────────────────
   7 ┌   containers:
   8 │     - name: redis-pod
   9 │       image: redis:latest
  10 │       ports:
  11 └         - containerPort: 80
────────────────────────────────────────



onepage-initial.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 97, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 2, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'onepage' of Deployment 'onepage' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'onepage' of Deployment 'onepage' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'onepage' of 'deployment' 'onepage' in 'onepage' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'onepage' of Deployment 'onepage' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'onepage' of Deployment 'onepage' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'onepage' of Deployment 'onepage' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'onepage' of Deployment 'onepage' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'onepage' of Deployment 'onepage' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onepage-initial.yaml:98-131
────────────────────────────────────────
  98 ┌   replicas: 1
  99 │   selector:
 100 │     matchLabels:
 101 │       k8s-app: onepage
 102 │   template:
 103 │     metadata:
 104 │       name: onepage
 105 │       creationTimestamp: null
 106 └       labels:
 ...   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onepage-initial.yaml:98-131
────────────────────────────────────────
  98 ┌   replicas: 1
  99 │   selector:
 100 │     matchLabels:
 101 │       k8s-app: onepage
 102 │   template:
 103 │     metadata:
 104 │       name: onepage
 105 │       creationTimestamp: null
 106 └       labels:
 ...   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "onepage" of deployment "onepage" in "onepage" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment onepage in onepage namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 onepage-initial.yaml:109-124
────────────────────────────────────────
 109 ┌       containers:
 110 │         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
 ...   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container onepage in deployment onepage (namespace: onepage) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 onepage-initial.yaml:110-117
────────────────────────────────────────
 110 ┌         - name: onepage
 111 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
 112 │           resources: {}
 113 │           terminationMessagePath: /dev/termination-log
 114 │           terminationMessagePolicy: File
 115 │           imagePullPolicy: Always
 116 │           securityContext:
 117 └             privileged: false
────────────────────────────────────────



onepage-service.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onepage-service.yaml:9-13
────────────────────────────────────────
   9 ┌   selector:
  10 │     k8s-app: onepage
  11 │   ports:
  12 │   - protocol: TCP
  13 └     port: 9000
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onepage-service.yaml:9-13
────────────────────────────────────────
   9 ┌   selector:
  10 │     k8s-app: onepage
  11 │   ports:
  12 │   - protocol: TCP
  13 └     port: 9000
────────────────────────────────────────



onepage.yaml (kubernetes)
=========================
Tests: 133 (SUCCESSES: 96, FAILURES: 37)
Failures: 37 (UNKNOWN: 0, LOW: 22, MEDIUM: 10, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'install' of Deployment 'onepage' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'onepage' of Deployment 'onepage' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'install' of Deployment 'onepage' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'onepage' of Deployment 'onepage' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'install' of 'deployment' 'onepage' in 'onepage' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'onepage' of 'deployment' 'onepage' in 'onepage' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'install' of Deployment 'onepage' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'install' of Deployment 'onepage' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'onepage' of Deployment 'onepage' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'install' of Deployment 'onepage' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'onepage' of Deployment 'onepage' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'install' of Deployment 'onepage' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'onepage' of Deployment 'onepage' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'install' of Deployment 'onepage' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'install' of Deployment 'onepage' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'install' of Deployment 'onepage' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'onepage' of Deployment 'onepage' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'install' of Deployment 'onepage' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'onepage' of Deployment 'onepage' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'install' of Deployment 'onepage' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'onepage' of Deployment 'onepage' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'onepage' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 onepage.yaml:9-43
────────────────────────────────────────
   9 ┌   replicas: 2
  10 │   selector:
  11 │     matchLabels:
  12 │       app: onepage
  13 │       k8s-app: onepage
  14 │   template:
  15 │     metadata:
  16 │       name: onepage
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onepage.yaml:9-43
────────────────────────────────────────
   9 ┌   replicas: 2
  10 │   selector:
  11 │     matchLabels:
  12 │       app: onepage
  13 │       k8s-app: onepage
  14 │   template:
  15 │     metadata:
  16 │       name: onepage
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onepage.yaml:9-43
────────────────────────────────────────
   9 ┌   replicas: 2
  10 │   selector:
  11 │     matchLabels:
  12 │       app: onepage
  13 │       k8s-app: onepage
  14 │   template:
  15 │     metadata:
  16 │       name: onepage
  17 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "install" of deployment "onepage" in "onepage" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "onepage" of deployment "onepage" in "onepage" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container onepage in onepage namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container onepage in onepage namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 onepage.yaml:34-43
────────────────────────────────────────
  34 ┌         - name: install
  35 │           image: busybox
  36 │           volumeMounts:
  37 │             - name: dir
  38 │               mountPath: /code
  39 │           command:
  40 │             - cp
  41 │             - "-r"
  42 │             - "/var/www/."
  43 └             - "/code"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment onepage in onepage namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 onepage.yaml:21-43
────────────────────────────────────────
  21 ┌       volumes:
  22 │         - name: dir
  23 │           hostPath:
  24 │             path: /code
  25 │       containers:
  26 │         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 └             - name: dir
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container onepage in deployment onepage (namespace: onepage) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 onepage.yaml:26-32
────────────────────────────────────────
  26 ┌         - name: onepage
  27 │           image: registry.digitalocean.com/unmutedtech/onepage:latest
  28 │           volumeMounts:
  29 │             - name: dir
  30 │               mountPath: /code
  31 │           terminationMessagePolicy: File
  32 └           imagePullPolicy: Always
────────────────────────────────────────



onepage_service-initial.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onepage_service-initial.yaml:66-84
────────────────────────────────────────
  66 ┌   ports:
  67 │     - name: tcp-80-8080-m6sbs
  68 │       protocol: TCP
  69 │       port: 80
  70 │       targetPort: 80
  71 │       nodePort: 31737
  72 │   selector:
  73 │     k8s-app: onepage
  74 └   clusterIP: 10.245.172.201
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onepage_service-initial.yaml:66-84
────────────────────────────────────────
  66 ┌   ports:
  67 │     - name: tcp-80-8080-m6sbs
  68 │       protocol: TCP
  69 │       port: 80
  70 │       targetPort: 80
  71 │       nodePort: 31737
  72 │   selector:
  73 │     k8s-app: onepage
  74 └   clusterIP: 10.245.172.201
  ..   
────────────────────────────────────────



onepage_service-old.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onepage_service-old.yaml:9-16
────────────────────────────────────────
   9 ┌   ports:
  10 │     - protocol: TCP 
  11 │       port: 80
  12 │       targetPort: 80
  13 │   selector:
  14 │     app: onepage
  15 │     k8s-app: onepage
  16 └   type: LoadBalancer
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onepage_service-old.yaml:9-16
────────────────────────────────────────
   9 ┌   ports:
  10 │     - protocol: TCP 
  11 │       port: 80
  12 │       targetPort: 80
  13 │   selector:
  14 │     app: onepage
  15 │     k8s-app: onepage
  16 └   type: LoadBalancer
────────────────────────────────────────



onetimesecret.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'onetimesecret' of 'deployment' 'onetimesecret' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'onetimesecret' of Deployment 'onetimesecret' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'onetimesecret' of Deployment 'onetimesecret' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onetimesecret.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 2
   7 │   selector:
   8 │     matchLabels:
   9 │       app: onetimesecret
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: onetimesecret
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onetimesecret.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 2
   7 │   selector:
   8 │     matchLabels:
   9 │       app: onetimesecret
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: onetimesecret
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "onetimesecret" of deployment "onetimesecret" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment onetimesecret in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 onetimesecret.yaml:4
────────────────────────────────────────
   4 [   name: onetimesecret
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container onetimesecret in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment onetimesecret in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 onetimesecret.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container onetimesecret in deployment onetimesecret (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 onetimesecret.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: onetimesecret
  17 │         image: dismantl/onetimesecret:latest
  18 │         ports:
  19 └         - containerPort: 7143
────────────────────────────────────────



onetimesecret_1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 onetimesecret_1.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: onetimesecret
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 80
  11 └     targetPort: 7143
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 onetimesecret_1.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: onetimesecret
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 80
  11 └     targetPort: 7143
────────────────────────────────────────


