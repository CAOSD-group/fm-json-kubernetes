
Report Summary

┌────────────────────────────────────────────────────┬────────────┬───────────────────┐
│                       Target                       │    Type    │ Misconfigurations │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ Limit Cache Volume Size.yaml                       │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ LimitRange-cpu.yaml                                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ LimitRange-memory.yaml                             │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ LimitRange6.yaml                                   │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ LimitResources-quotas.yaml                         │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role121.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role122.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role123.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role125.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role126.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role127.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role128.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role129.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role13.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role130.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role131.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role132.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role133.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role134.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role135.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role136.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role137.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role139.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role14.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role140.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role141.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role142.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role143.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role144.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role145.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role146.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role147.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role148.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role149.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role15.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role150.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role151.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role152.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role153.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role154.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role155.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role156.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role157.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role158.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role159.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role16.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role160.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role161.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role162.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role163.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role164.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role165.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role166.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role167.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role168.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role169.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role170.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role171.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role172.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role173.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role174.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role175.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role177.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role179.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role18.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role180.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role181.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role182.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role183.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role184.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role185.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role186.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role187.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role188.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role189.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role19.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role190.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role192.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role194.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role195.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role196.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role197.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role198.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role199.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role2.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role20.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role200.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role201.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role202.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role204.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role205.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role206.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role207.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role209.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role21.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role210.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role211.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role212.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role213.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role214.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role215.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role216.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role217.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role218.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role219.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role22.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role220.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role221.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role222.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role223.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role224.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role225.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role226.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role227.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role228.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role229.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role23.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role230.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role231.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role232.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role233.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role234.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role235.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role236.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role237.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role238.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role239.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role24.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role240.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role241.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role242.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role243.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role244.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role245.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role25.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role26.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role264.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role265.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role266.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role267.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role268.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role269.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role27.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role270.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role271.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role272.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role273.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role274.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role275.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role276.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role277.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role278.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role279.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role28.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role280.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role281.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role282.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role283.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role284.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role285.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role286.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role287.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role288.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role289.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role29.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role290.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role291.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role292.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role293.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role294.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role295.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role296.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role297.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role298.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role299.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role30.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role300.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role301.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role302.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role303.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role304.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role305.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role306.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role31.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role32.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role33.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role34.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role35.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role36.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role37.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role38.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role39.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role4.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role40.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role41.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role42.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role43.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role44.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role45.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role46.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role47.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role48.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role49.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role5.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role50.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role51.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role52.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role53.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role54.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role55.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role56.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role57.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role58.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role59.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role6.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role60.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role61.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role62.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role63.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role64.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role65.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role66.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role67.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role68.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role69.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role7.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role70.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role71.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role72.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role73.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role74.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role75.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role76.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role77.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role78.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role79.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role8.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role80.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role81.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role82.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role83.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role84.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role85.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role86.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role87.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role88.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role89.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role9.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role90.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role91.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role92.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role93.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role94.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role95.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role96.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role97.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role99.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding.yaml                  │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding1.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding10.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding100.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding101.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding102.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding103.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding104.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding105.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding106.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding107.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding108.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding109.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding11.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding110.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding111.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding112.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding113.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding114.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding115.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding116.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding117.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding118.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding119.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding12.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding120.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding121.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding122.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding123.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding125.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding126.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding127.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding128.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding129.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding13.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding130.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding131.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding132.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding133.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding134.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding135.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding136.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding137.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding139.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding14.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding140.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding141.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding142.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding143.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding144.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding145.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding146.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding147.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding148.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding149.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding15.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding150.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding151.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding152.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding153.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding154.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding155.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding156.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding157.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding158.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding159.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding16.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding160.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding161.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding162.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding163.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding164.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding165.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding166.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding167.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding168.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding169.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding170.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding171.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding172.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding173.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding174.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding175.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding177.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding179.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding18.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding180.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding181.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding182.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding183.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding184.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding185.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding186.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding187.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding188.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding189.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding19.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding190.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding192.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding194.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding195.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding196.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding197.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding198.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding199.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding2.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding20.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding200.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding201.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding202.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding204.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding205.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding206.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding207.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding209.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding21.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding210.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding211.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding212.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding213.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding214.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding215.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding216.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding217.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding218.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding219.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding22.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding220.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding221.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding222.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding223.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding224.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding225.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding226.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding227.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding228.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding229.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding23.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding230.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding231.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding232.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding233.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding234.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding235.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding236.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding237.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding238.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding239.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding24.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding240.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding241.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding242.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding243.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding244.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding245.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding25.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding26.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding264.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding265.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding266.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding267.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding268.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding269.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding27.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding270.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding271.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding272.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding273.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding274.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding275.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding276.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding277.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding278.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding279.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding28.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding280.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding281.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding282.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding283.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding284.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding285.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding286.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding287.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding288.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding289.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding29.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding290.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding291.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding292.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding293.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding294.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding295.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding296.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding297.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding298.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding299.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding30.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding300.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding301.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding302.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding303.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding304.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding305.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding306.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding31.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding32.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding33.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding34.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding35.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding36.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding37.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding38.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding39.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding4.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding40.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding41.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding42.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding43.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding44.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding45.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding46.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding47.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding48.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding49.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding5.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding50.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding51.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding52.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding53.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding54.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding55.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding56.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding57.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding58.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding59.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding6.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding60.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding61.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding62.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding63.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding64.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding65.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding66.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding67.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding68.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding69.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding7.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding70.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding71.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding72.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding73.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding74.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding75.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding76.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding77.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding78.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding79.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding8.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding80.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding81.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding82.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding83.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding84.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding85.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding86.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding87.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding88.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding89.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding9.yaml                 │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding90.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding91.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding92.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding93.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding94.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding95.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding96.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding97.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leader_election_role_binding99.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leaky-deploy.yaml                                  │ kubernetes │        51         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ learner-service-deployment.yaml                    │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ learner-service-service.yaml                       │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ learning-resources-api.yaml                        │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ learning-resources-api1.yaml                       │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ learning-resources-api1_1.yaml                     │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ learning-resources-api_1.yaml                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leases-service.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ leases.yaml                                        │ kubernetes │        16         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lecturer-service-claim0-persistentvolumeclaim.yaml │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lecturer-service-deployment.yaml                   │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lecturer-service-service.yaml                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ left.yaml                                          │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ left_1.yaml                                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ left_2.yaml                                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ left_3.yaml                                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legacy-pod.yaml                                    │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legacy-pod1.yaml                                   │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legacy-pod_1.yaml                                  │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legacy-redirects.deployment.yaml                   │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legacy-redirects.ingress.yaml                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legacy-redirects.service.yaml                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legocarla-deployment.yaml                          │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legocarla-deployment_2.yaml                        │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ legocarla-service.yaml                             │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lemon.yaml                                         │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lemon1.yaml                                        │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lemp-service.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lemp-wp-deployment.yaml                            │ kubernetes │        36         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones_1.yaml                                │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones_2.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones_3.yaml                                │ kubernetes │        22         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones_4.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones_5.yaml                                │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ les-gorgones_6.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lesson_1_6.yaml                                    │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ letschat-srv.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ letsencrypt-job.yaml                               │ kubernetes │        22         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ letsencrypt.yaml                                   │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ letsencrypt1.yaml                                  │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1.yaml                                         │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_1.yaml                                       │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_10.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_11.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_12.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_13.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_14.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_15.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_16.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_17.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_18.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_19.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_2.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_20.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_21.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_22.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_23.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_24.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_25.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_26.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_27.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_28.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_3.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_30.yaml                                      │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_31.yaml                                      │ kubernetes │         6         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_32.yaml                                      │ kubernetes │        14         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_33.yaml                                      │ kubernetes │        15         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_34.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_35.yaml                                      │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_36.yaml                                      │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_37.yaml                                      │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_4.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_5.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_6.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_7.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_8.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lgtm1_9.yaml                                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ liam-nginx-deployment.yaml                         │ kubernetes │         4         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ libenessprobe.yaml                                 │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ libenessprobe_1.yaml                               │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-configMap.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-configMap1.yaml                          │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-configMap1_1.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-configMap_1.yaml                         │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-deployment.yaml                          │ kubernetes │        31         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-deployment1.yaml                         │ kubernetes │        31         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-deployment1_1.yaml                       │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-deployment_1.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-ingress.yaml                             │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-ingress1.yaml                            │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-pvc.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-pvc1.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-svc.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ librechat-svc1.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ licaodecasa.yaml                                   │ kubernetes │        52         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ licaodecasa1.yaml                                  │ kubernetes │        52         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ license-key-secret.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ license-key-secret1.yaml                           │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle-demo.yaml                                │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle-demo1.yaml                               │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle-demo2.yaml                               │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle-events.yaml                              │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle.yaml                                     │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle1.yaml                                    │ kubernetes │        36         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle2.yaml                                    │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle3.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle4.yaml                                    │ kubernetes │        36         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle5.yaml                                    │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lifecycle6.yaml                                    │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lightdeploy.yaml                                   │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lightdeploy1.yaml                                  │ kubernetes │        21         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-external-plugins-cm.yaml                │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-foghorn-rb.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-foghorn-role.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-foghorn-sa.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-gc-jobs-cronjob.yaml                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-gc-jobs-rb.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-gc-jobs-role.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-gc-jobs-sa.yaml                         │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-keeper-deploy.yaml                      │ kubernetes │        15         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-keeper-rb.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-keeper-role.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-keeper-sa.yaml                          │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-keeper-svc.yaml                         │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-pvc.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-pvc1.yaml                               │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-tekton-controller-rb.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-tekton-controller-role.yaml             │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-tekton-controller-sa.yaml               │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-tekton-controller-svc.yaml              │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-webhooks-rb.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-webhooks-role.yaml                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lighthouse-webhooks-sa.yaml                        │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lights.yaml                                        │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lights1.yaml                                       │ kubernetes │        20         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lights1_1.yaml                                     │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lights_1.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lightservice.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ lightservice1.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-cpu.yaml                                     │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-cpu1.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-cpu2.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-cpu21.yaml                                   │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-object.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-only.yaml                                    │ kubernetes │        32         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-only1.yaml                                   │ kubernetes │        32         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram.yaml                                     │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram1.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram2.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram21.yaml                                   │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram22.yaml                                   │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram3.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram31.yaml                                   │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ram4.yaml                                    │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-1.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-2.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu.yaml                               │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu1.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu2.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu3.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu4.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu5.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-cpu6.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-definiation.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-definiation1.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-definition.yaml                        │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-definition1.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-demo.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-demo1.yaml                             │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-demo2.yaml                             │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-demo3.yaml                             │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-memory.yaml                            │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-memory1.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-memory2.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-memory3.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-memory4.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range-memory5.yaml                           │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range.yaml                                   │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range1.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range10.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range11.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range12.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range13.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range15.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range16.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range17.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range18.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range19.yaml                                 │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range2.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range20.yaml                                 │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range21.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range22.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range23.yaml                                 │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range3.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range4.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range5.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range6.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range7.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-range8.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ranges-default-min-max.yaml                  │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ranges-default-min-max1.yaml                 │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ranges-default-min-max2.yaml                 │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ranges.yaml                                  │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ranges1.yaml                                 │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-ranges2.yaml                                 │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit-resource.yaml                                │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit.yaml                                         │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limit1.yaml                                        │ kubernetes │        16         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitRange10.yaml                                  │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitRange11.yaml                                  │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitRange13.yaml                                  │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitRange5.yaml                                   │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitando_recurcso.yaml                            │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitando_recurcso1.yaml                           │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limite de cpu.yaml                                 │ kubernetes │        16         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limite-cpu.yaml                                    │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limite.yaml                                        │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited-memory-pod.yaml                            │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited-memory-pod_1.yaml                          │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited-pod.yaml                                   │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited-pod1.yaml                                  │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited_namespaceaccess.yaml                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited_namespaceaccess1.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited_namespaceaccess1_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited_namespaceaccess2.yaml                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited_namespaceaccess2_1.yaml                    │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limited_namespaceaccess_1.yaml                     │ kubernetes │         0         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitex-resource.yaml                              │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitex.yaml                                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitex1.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitr-1-pod-1.yaml                                │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitr-1-pod-11.yaml                               │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitr-1-pod-2.yaml                                │ kubernetes │        17         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitr-1.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitr-11.yaml                                     │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitr-2.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange-1.yaml                                  │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange-def.yaml                                │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange.yaml                                    │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange1.yaml                                   │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange15.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange16.yaml                                  │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange2.yaml                                   │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange3.yaml                                   │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange7.yaml                                   │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange8.yaml                                   │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limitrange9.yaml                                   │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits-no-restart.yaml                             │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits-no-restart1.yaml                            │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits-pod-too-big.yaml                            │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits-pod-too-big1.yaml                           │ kubernetes │        19         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits.yaml                                        │ kubernetes │        18         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits1.yaml                                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits10.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits11.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits12.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits13.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits14.yaml                                      │ kubernetes │        28         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits16.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits17.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits18.yaml                                      │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits19.yaml                                      │ kubernetes │         2         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits2.yaml                                       │ kubernetes │         1         │
├────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ limits20.yaml                                      │ kubernetes │         1         │
└────────────────────────────────────────────────────┴────────────┴───────────────────┘
Legend:
- '-': Not scanned
- '0': Clean (no security findings detected)


Limit Cache Volume Size.yaml (kubernetes)
=========================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'tomcat-container' of 'pod' 'tomcat-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'tomcat-container' of Pod 'tomcat-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 Limit Cache Volume Size.yaml:9-19
────────────────────────────────────────
   9 ┌   containers :
  10 │    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 │         mountPath : /oracle-volume
  15 │   
  16 │   volumes :
  17 └    - name : cache-volume
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 Limit Cache Volume Size.yaml:9-19
────────────────────────────────────────
   9 ┌   containers :
  10 │    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 │         mountPath : /oracle-volume
  15 │   
  16 │   volumes :
  17 └    - name : cache-volume
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "tomcat-container" of pod "tomcat-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod tomcat-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 Limit Cache Volume Size.yaml:6
────────────────────────────────────────
   6 [   name : tomcat-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container tomcat-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod tomcat-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 Limit Cache Volume Size.yaml:9-19
────────────────────────────────────────
   9 ┌   containers :
  10 │    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 │         mountPath : /oracle-volume
  15 │   
  16 │   volumes :
  17 └    - name : cache-volume
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container tomcat-container in pod tomcat-pod (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 Limit Cache Volume Size.yaml:10-14
────────────────────────────────────────
  10 ┌    - image : vishymails/tomcatimage:1.0
  11 │      name : tomcat-container
  12 │      volumeMounts :
  13 │       - name : cache-volume
  14 └         mountPath : /oracle-volume
────────────────────────────────────────



LimitRange6.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 LimitRange6.yaml:6-19
────────────────────────────────────────
   6 ┌     limits:
   7 │         - default: # Límites por defecto asignados a los recursos (pods)
   8 │               memory: '512Mi'
   9 │               cpu: '1'
  10 │           defautl-request: # Límites para solicitudes (request) de recursos
  11 │               memory: '256Mi'
  12 │               cpu: '500m'
  13 │           max: # Límite máximo de recursos consumidos por el clúster (namespace)
  14 └               memory: '1Gi'
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 LimitRange6.yaml:6-19
────────────────────────────────────────
   6 ┌     limits:
   7 │         - default: # Límites por defecto asignados a los recursos (pods)
   8 │               memory: '512Mi'
   9 │               cpu: '1'
  10 │           defautl-request: # Límites para solicitudes (request) de recursos
  11 │               memory: '256Mi'
  12 │               cpu: '500m'
  13 │           max: # Límite máximo de recursos consumidos por el clúster (namespace)
  14 └               memory: '1Gi'
  ..   
────────────────────────────────────────



LimitResources-quotas.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 LimitResources-quotas.yaml:7-11
────────────────────────────────────────
   7 ┌    hard: 
   8 │      requests.cpu: 4
   9 │      requests.memory: 4Gi
  10 │      limits.cpu: 10
  11 └      limits:memory: 10Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 LimitResources-quotas.yaml:8-11
────────────────────────────────────────
   8 ┌      requests.cpu: 4
   9 │      requests.memory: 4Gi
  10 │      limits.cpu: 10
  11 └      limits:memory: 10Gi
────────────────────────────────────────



leader_election_role121.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role121.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role122.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role122.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role123.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role123.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role125.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role125.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role126.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role126.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role127.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role127.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role128.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role128.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role129.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role129.yaml:14-25
────────────────────────────────────────
  14 ┌   - apiGroups:
  15 │       - ""
  16 │     resources:
  17 │       - configmaps
  18 │     verbs:
  19 │       - get
  20 │       - list
  21 │       - watch
  22 └       - create
  ..   
────────────────────────────────────────



leader_election_role13.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role13.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role130.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role130.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role131.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role131.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role132.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role132.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role133.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role133.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role134.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role134.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role135.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role135.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role136.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role136.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role137.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role137.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role139.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role139.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role14.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role14.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role140.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role140.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role141.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role141.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role142.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role142.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role143.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role143.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role144.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role144.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role145.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role145.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role146.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role146.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role147.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role147.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role148.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role148.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role149.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role149.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role15.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role15.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role150.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role150.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role151.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role151.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role152.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role152.yaml:14-25
────────────────────────────────────────
  14 ┌   - apiGroups:
  15 │       - ""
  16 │     resources:
  17 │       - configmaps
  18 │     verbs:
  19 │       - get
  20 │       - list
  21 │       - watch
  22 └       - create
  ..   
────────────────────────────────────────



leader_election_role153.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role153.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role154.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role154.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role155.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role155.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role156.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role156.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role157.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role157.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role158.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role158.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role159.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role159.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role16.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role16.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role160.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role160.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role161.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role161.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role162.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role162.yaml:14-25
────────────────────────────────────────
  14 ┌   - apiGroups:
  15 │       - ""
  16 │     resources:
  17 │       - configmaps
  18 │     verbs:
  19 │       - get
  20 │       - list
  21 │       - watch
  22 └       - create
  ..   
────────────────────────────────────────



leader_election_role163.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role163.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role164.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role164.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role165.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role165.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role166.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role166.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role167.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role167.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role168.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role168.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role169.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role169.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role170.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role170.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role171.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role171.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role172.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role172.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role173.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role173.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role174.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role174.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role175.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role175.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role177.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role177.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role179.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role179.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role18.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role18.yaml:15-26
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ""
  17 │   resources:
  18 │   - configmaps
  19 │   verbs:
  20 │   - get
  21 │   - list
  22 │   - watch
  23 └   - create
  ..   
────────────────────────────────────────



leader_election_role180.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role180.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role181.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role181.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role182.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role182.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role183.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role183.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role184.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role184.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role185.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role185.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role186.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role186.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role187.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role187.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role188.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role188.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role189.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role189.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role19.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role19.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role190.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role190.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role192.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role192.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role194.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role194.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role195.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role195.yaml:8-19
────────────────────────────────────────
   8 ┌ - apiGroups:
   9 │   - ""
  10 │   resources:
  11 │   - configmaps
  12 │   verbs:
  13 │   - get
  14 │   - list
  15 │   - watch
  16 └   - create
  ..   
────────────────────────────────────────



leader_election_role196.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role196.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role197.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role197.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role198.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role198.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role199.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role199.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role2.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role2.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role20.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role20.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role200.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role200.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role201.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role201.yaml:14-25
────────────────────────────────────────
  14 ┌   - apiGroups:
  15 │       - ""
  16 │     resources:
  17 │       - configmaps
  18 │     verbs:
  19 │       - get
  20 │       - list
  21 │       - watch
  22 └       - create
  ..   
────────────────────────────────────────



leader_election_role202.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role202.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role204.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role204.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role205.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role205.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role206.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role206.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role207.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role207.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role209.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role209.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role21.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role21.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role210.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role210.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role211.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role211.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role212.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role212.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role213.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role213.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role214.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role214.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role215.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role215.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role216.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role216.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role217.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role217.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role218.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role218.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role219.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role219.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role22.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role22.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role220.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role220.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role221.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role221.yaml:14-25
────────────────────────────────────────
  14 ┌   - apiGroups:
  15 │       - ""
  16 │     resources:
  17 │       - configmaps
  18 │     verbs:
  19 │       - get
  20 │       - list
  21 │       - watch
  22 └       - create
  ..   
────────────────────────────────────────



leader_election_role222.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role222.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role223.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role223.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role224.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role224.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role225.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'telegraf-sidecar-operator' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role225.yaml:12-23
────────────────────────────────────────
  12 ┌   - apiGroups:
  13 │       - ""
  14 │     resources:
  15 │       - configmaps
  16 │     verbs:
  17 │       - get
  18 │       - list
  19 │       - watch
  20 └       - create
  ..   
────────────────────────────────────────



leader_election_role226.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role226.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role227.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role227.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role228.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role228.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role229.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role229.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role23.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role23.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role230.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role230.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role231.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role231.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role232.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role232.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role233.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role233.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role234.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role234.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role235.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role235.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role236.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role236.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role237.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role237.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role238.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role238.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role239.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role239.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role24.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role24.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role240.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role240.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role241.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role241.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role242.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role242.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role243.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role243.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role244.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role244.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role245.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role245.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role25.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role25.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role26.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role26.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role264.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role264.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role265.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role265.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role266.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role266.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role267.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role267.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role268.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role268.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role269.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role269.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role27.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role27.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role270.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role270.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role271.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role271.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role272.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role272.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role273.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role273.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role274.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role274.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role275.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role275.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role276.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role276.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role277.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role277.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role278.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role278.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role279.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role279.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role28.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role28.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role280.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role280.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role281.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role281.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role282.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role282.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role283.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role283.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role284.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role284.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role285.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role285.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role286.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role286.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role287.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role287.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role288.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role288.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role289.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role289.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role29.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role29.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role290.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role290.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role291.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role291.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role292.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role292.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role293.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role293.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role294.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role294.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role295.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role295.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role296.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role296.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role297.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role297.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role298.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role298.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role299.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role299.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role30.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role30.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role300.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role300.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role301.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role301.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role302.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role302.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role303.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role303.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role304.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role304.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role305.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role305.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role306.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role306.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role31.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role31.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role32.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role32.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role33.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role33.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role34.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role34.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role35.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role35.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role36.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role36.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role37.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role37.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role38.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role38.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role39.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role39.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role4.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role4.yaml:15-26
────────────────────────────────────────
  15 ┌ - apiGroups:
  16 │   - ""
  17 │   resources:
  18 │   - configmaps
  19 │   verbs:
  20 │   - get
  21 │   - list
  22 │   - watch
  23 └   - create
  ..   
────────────────────────────────────────



leader_election_role40.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role40.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role41.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role41.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role42.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role42.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role43.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role43.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role44.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role44.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role45.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role45.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role46.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role46.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role47.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role47.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role48.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role48.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role49.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role49.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role5.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role5.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role50.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role50.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role51.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role51.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role52.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role52.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role53.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role53.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role54.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role54.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role55.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role55.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role56.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role56.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role57.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role57.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role58.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role58.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role59.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role59.yaml:13-24
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ""
  15 │   resources:
  16 │   - configmaps
  17 │   verbs:
  18 │   - get
  19 │   - list
  20 │   - watch
  21 └   - create
  ..   
────────────────────────────────────────



leader_election_role6.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role6.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role60.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role60.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role61.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role61.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role62.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role62.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role63.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role63.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role64.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role64.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role65.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role65.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role66.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role66.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role67.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role67.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role68.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role68.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role69.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role69.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role7.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role7.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role70.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role70.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role71.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role71.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role72.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role72.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role73.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role73.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role74.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role74.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role75.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role75.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role76.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role76.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role77.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role77.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role78.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role78.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role79.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role79.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role8.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role8.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role80.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role80.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role81.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role81.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role82.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role82.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role83.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role83.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role84.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role84.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role85.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role85.yaml:10-21
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - configmaps
  14 │   verbs:
  15 │   - get
  16 │   - list
  17 │   - watch
  18 └   - create
  ..   
────────────────────────────────────────



leader_election_role86.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role86.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role87.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role87.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role88.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role88.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role89.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role89.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role9.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role9.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role90.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role90.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role91.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role91.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role92.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role92.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role93.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role93.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role94.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role94.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role95.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role95.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role96.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role96.yaml:7-18
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ""
   9 │   resources:
  10 │   - configmaps
  11 │   verbs:
  12 │   - get
  13 │   - list
  14 │   - watch
  15 └   - create
  ..   
────────────────────────────────────────



leader_election_role97.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role97.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leader_election_role99.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'leader-election-role' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 leader_election_role99.yaml:14-25
────────────────────────────────────────
  14 ┌ - apiGroups:
  15 │   - ""
  16 │   resources:
  17 │   - configmaps
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 │   - watch
  22 └   - create
  ..   
────────────────────────────────────────



leaky-deploy.yaml (kubernetes)
==============================
Tests: 148 (SUCCESSES: 97, FAILURES: 51)
Failures: 51 (UNKNOWN: 0, LOW: 32, MEDIUM: 12, HIGH: 7, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'poc7' of Deployment 'poc-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'poc8' of Deployment 'poc-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'poc9' of Deployment 'poc-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'poc7' of Deployment 'poc-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'poc8' of Deployment 'poc-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'poc9' of Deployment 'poc-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'poc7' of 'deployment' 'poc-deployment' in 'poc' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'poc8' of 'deployment' 'poc-deployment' in 'poc' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'poc9' of 'deployment' 'poc-deployment' in 'poc' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'poc7' of Deployment 'poc-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'poc8' of Deployment 'poc-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'poc9' of Deployment 'poc-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'poc7' of Deployment 'poc-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'poc8' of Deployment 'poc-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'poc9' of Deployment 'poc-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'poc7' of Deployment 'poc-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'poc8' of Deployment 'poc-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'poc9' of Deployment 'poc-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'poc7' of Deployment 'poc-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'poc8' of Deployment 'poc-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'poc9' of Deployment 'poc-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'poc7' of Deployment 'poc-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'poc8' of Deployment 'poc-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'poc9' of Deployment 'poc-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'poc7' of Deployment 'poc-deployment' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'poc8' of Deployment 'poc-deployment' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'poc9' of Deployment 'poc-deployment' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'poc7' of Deployment 'poc-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'poc8' of Deployment 'poc-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'poc9' of Deployment 'poc-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'poc7' of Deployment 'poc-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'poc8' of Deployment 'poc-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'poc9' of Deployment 'poc-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'poc7' of Deployment 'poc-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'poc8' of Deployment 'poc-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'poc9' of Deployment 'poc-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 leaky-deploy.yaml:7-31
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: poc
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: poc
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 leaky-deploy.yaml:7-31
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       app: poc
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: poc
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "poc7" of deployment "poc-deployment" in "poc" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "poc8" of deployment "poc-deployment" in "poc" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "poc9" of deployment "poc-deployment" in "poc" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment poc-deployment in poc namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 leaky-deploy.yaml:16-31
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 │           privileged: true
  22 │       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 └         imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container poc7 in deployment poc-deployment (namespace: poc) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 leaky-deploy.yaml:17-21
────────────────────────────────────────
  17 ┌       - name: poc7
  18 │         image: entlein/poc:0.0.2
  19 │         imagePullPolicy: Always
  20 │         securityContext:
  21 └           privileged: true
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container poc8 in deployment poc-deployment (namespace: poc) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 leaky-deploy.yaml:22-26
────────────────────────────────────────
  22 ┌       - name: poc8
  23 │         image: entlein/poc:0.0.8
  24 │         imagePullPolicy: Always
  25 │         securityContext:
  26 └           privileged: true
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container poc9 in deployment poc-deployment (namespace: poc) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 leaky-deploy.yaml:27-31
────────────────────────────────────────
  27 ┌       - name: poc9
  28 │         image: entlein/poc:0.0.9
  29 │         imagePullPolicy: Always
  30 │         securityContext:
  31 └           privileged: true
────────────────────────────────────────



learner-service-deployment.yaml (kubernetes)
============================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'learner-service' of Deployment 'learner-service' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'learner-service' of Deployment 'learner-service' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'learner-service' of 'deployment' 'learner-service' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'learner-service' of Deployment 'learner-service' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'learner-service' of Deployment 'learner-service' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'learner-service' of Deployment 'learner-service' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'learner-service' of Deployment 'learner-service' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'learner-service' of Deployment 'learner-service' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'learner-service' of Deployment 'learner-service' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'learner-service' of Deployment 'learner-service' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'learner-service' of Deployment 'learner-service' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'learner-service' of Deployment 'learner-service' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 learner-service-deployment.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 3
   7 │   selector:
   8 │     matchLabels:
   9 │       app: learner-service
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: learner-service
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 learner-service-deployment.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 3
   7 │   selector:
   8 │     matchLabels:
   9 │       app: learner-service
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: learner-service
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "learner-service" of deployment "learner-service" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment learner-service in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 learner-service-deployment.yaml:4
────────────────────────────────────────
   4 [   name: learner-service
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container learner-service in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment learner-service in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 learner-service-deployment.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container learner-service in deployment learner-service (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 learner-service-deployment.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: learner-service
  17 │         image: sanjananilanka/learner-service:latest
  18 │         ports:
  19 └         - containerPort: 5001
────────────────────────────────────────



learner-service-service.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 learner-service-service.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: learner-service
   8 │   ports:
   9 │     - protocol: TCP
  10 │       port: 84
  11 └       targetPort: 5001
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 learner-service-service.yaml:6-11
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: learner-service
   8 │   ports:
   9 │     - protocol: TCP
  10 │       port: 84
  11 └       targetPort: 5001
────────────────────────────────────────



learning-resources-api.yaml (kubernetes)
========================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'learning-resources-container' of 'deployment' 'learning-resources' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'learning-resources-container' of Deployment 'learning-resources' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 learning-resources-api.yaml:8-35
────────────────────────────────────────
   8 ┌   replicas: 3
   9 │   selector:
  10 │     matchLabels:
  11 │       app: learning-resources
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: learning-resources
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 learning-resources-api.yaml:8-35
────────────────────────────────────────
   8 ┌   replicas: 3
   9 │   selector:
  10 │     matchLabels:
  11 │       app: learning-resources
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: learning-resources
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "learning-resources-container" of deployment "learning-resources" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment learning-resources in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 learning-resources-api.yaml:4-6
────────────────────────────────────────
   4 ┌   name: learning-resources
   5 │   labels:
   6 └     app: learning-resources
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container learning-resources in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment learning-resources in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 learning-resources-api.yaml:17-35
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container learning-resources-container in deployment learning-resources (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 learning-resources-api.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:latest
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────



learning-resources-api1.yaml (kubernetes)
=========================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'learning-resources-container' of 'deployment' 'learning-resources' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'learning-resources-container' of Deployment 'learning-resources' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 learning-resources-api1.yaml:8-35
────────────────────────────────────────
   8 ┌   replicas: 2
   9 │   selector:
  10 │     matchLabels:
  11 │       app: learning-resources
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: learning-resources
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 learning-resources-api1.yaml:8-35
────────────────────────────────────────
   8 ┌   replicas: 2
   9 │   selector:
  10 │     matchLabels:
  11 │       app: learning-resources
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: learning-resources
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "learning-resources-container" of deployment "learning-resources" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment learning-resources in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 learning-resources-api1.yaml:4-6
────────────────────────────────────────
   4 ┌   name: learning-resources
   5 │   labels:
   6 └     app: learning-resources
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container learning-resources in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment learning-resources in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 learning-resources-api1.yaml:17-35
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container learning-resources-container in deployment learning-resources (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 learning-resources-api1.yaml:18-35
────────────────────────────────────────
  18 ┌       - name: learning-resources-container
  19 │         image: kimschles/learning-resources:0.0.8
  20 │         imagePullPolicy: Always
  21 │         ports:
  22 │         - containerPort: 3000
  23 │         env:
  24 │         - name: POD_NAME
  25 │           valueFrom:
  26 └             fieldRef:
  ..   
────────────────────────────────────────



learning-resources-api1_1.yaml (kubernetes)
===========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 learning-resources-api1_1.yaml:8-14
────────────────────────────────────────
   8 ┌   selector:
   9 │     app: learning-resources
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 80
  13 │     targetPort: 3000
  14 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 learning-resources-api1_1.yaml:8-14
────────────────────────────────────────
   8 ┌   selector:
   9 │     app: learning-resources
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 80
  13 │     targetPort: 3000
  14 └   type: ClusterIP
────────────────────────────────────────



learning-resources-api_1.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 learning-resources-api_1.yaml:8-14
────────────────────────────────────────
   8 ┌   selector:
   9 │     app: learning-resources
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 80
  13 │     targetPort: 3000
  14 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 learning-resources-api_1.yaml:8-14
────────────────────────────────────────
   8 ┌   selector:
   9 │     app: learning-resources
  10 │   ports:
  11 │   - protocol: TCP
  12 │     port: 80
  13 │     targetPort: 3000
  14 └   type: ClusterIP
────────────────────────────────────────



leases-service.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 leases-service.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: leases
   8 │   ports:
   9 │     - protocol: TCP
  10 └       port: 9000
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 leases-service.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: leases
   8 │   ports:
   9 │     - protocol: TCP
  10 └       port: 9000
────────────────────────────────────────



leases.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 101, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 10, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'leases' of Deployment 'leases' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'leases' of Deployment 'leases' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'leases' of 'deployment' 'leases' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'leases' of Deployment 'leases' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'leases' of Deployment 'leases' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'leases' of Deployment 'leases' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'leases' of Deployment 'leases' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'leases' of Deployment 'leases' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 leases.yaml:8-63
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: leases
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: leases
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 leases.yaml:8-63
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: leases
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: leases
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "leases" of deployment "leases" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment leases in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 leases.yaml:4-6
────────────────────────────────────────
   4 ┌   name: leases
   5 │   labels:
   6 └     app: leases
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container leases in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 leases.yaml:18-56
────────────────────────────────────────
  18 ┌       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 │           mountPath: /etc/grid
  26 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment leases in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 leases.yaml:17-63
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: leases
  19 │         image: eu.gcr.io/grid-301122/leases:containerised
  20 │         imagePullPolicy: Always
  21 │         volumeMounts:
  22 │         - name: gu-config
  23 │           mountPath: /etc/gu
  24 │         - name: grid-config
  25 └           mountPath: /etc/grid
  ..   
────────────────────────────────────────



lecturer-service-claim0-persistentvolumeclaim.yaml (kubernetes)
===============================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lecturer-service-claim0-persistentvolumeclaim.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lecturer-service-claim0-persistentvolumeclaim.yaml:9-13
────────────────────────────────────────
   9 ┌   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 100Mi
────────────────────────────────────────



lecturer-service-deployment.yaml (kubernetes)
=============================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lecurers' of Deployment 'lecturer-service' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lecurers' of 'deployment' 'lecturer-service' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lecurers' of Deployment 'lecturer-service' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lecurers' of Deployment 'lecturer-service' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lecurers' of Deployment 'lecturer-service' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lecurers' of Deployment 'lecturer-service' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0024 (HIGH): Container 'lecurers' of Deployment 'lecturer-service' should not set host ports, 'ports[*].hostPort'
════════════════════════════════════════
According to pod security standard 'Host Ports', hostPorts should be disallowed, or at minimum restricted to a known list.

See https://avd.aquasec.com/misconfig/ksv024
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lecturer-service-deployment.yaml:12-43
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       io.kompose.service: lecturer-service
  16 │   strategy:
  17 │     type: Recreate
  18 │   template:
  19 │     metadata:
  20 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lecturer-service-deployment.yaml:12-43
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       io.kompose.service: lecturer-service
  16 │   strategy:
  17 │     type: Recreate
  18 │   template:
  19 │     metadata:
  20 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lecurers" of deployment "lecturer-service" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lecturer-service in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lecturer-service-deployment.yaml:4-10
────────────────────────────────────────
   4 ┌   annotations:
   5 │     kompose.cmd: C:\Users\nadun\AppData\Local\Microsoft\WinGet\Links\kompose.exe convert
   6 │     kompose.version: 1.31.2 (a92241f79)
   7 │   creationTimestamp: null
   8 │   labels:
   9 │     io.kompose.service: lecturer-service
  10 └   name: lecturer-service
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lecturer-service in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lecturer-service-deployment.yaml:29-38
────────────────────────────────────────
  29 ┌         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 │           volumeMounts:
  37 │             - mountPath: /app
  38 └               name: lecturer-service-claim0
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lecturer-service in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lecturer-service-deployment.yaml:28-43
────────────────────────────────────────
  28 ┌       containers:
  29 │         - image: lecturer-service
  30 │           name: lecurers
  31 │           ports:
  32 │             - containerPort: 1113
  33 │               hostPort: 1113
  34 │               protocol: TCP
  35 │           resources: {}
  36 └           volumeMounts:
  ..   
────────────────────────────────────────



lecturer-service-service.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lecturer-service-service.yaml:12-17
────────────────────────────────────────
  12 ┌   ports:
  13 │     - name: "1113"
  14 │       port: 1113
  15 │       targetPort: 1113
  16 │   selector:
  17 └     io.kompose.service: lecturer-service
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lecturer-service-service.yaml:12-17
────────────────────────────────────────
  12 ┌   ports:
  13 │     - name: "1113"
  14 │       port: 1113
  15 │       targetPort: 1113
  16 │   selector:
  17 └     io.kompose.service: lecturer-service
────────────────────────────────────────



left.yaml (kubernetes)
======================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-deployment' in 'test' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 left.yaml:9-22
────────────────────────────────────────
   9 ┌   replicas: 3
  10 │   selector:
  11 │     matchLabels:
  12 │       app: nginx
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: nginx
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 left.yaml:9-22
────────────────────────────────────────
   9 ┌   replicas: 3
  10 │   selector:
  11 │     matchLabels:
  12 │       app: nginx
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         app: nginx
  17 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-deployment" in "test" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-deployment in test namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-deployment in test namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 left.yaml:19-22
────────────────────────────────────────
  19 ┌       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-deployment in test namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 left.yaml:18-22
────────────────────────────────────────
  18 ┌       containers:
  19 │       - name: nginx
  20 │         image: nginx:1.14.2
  21 │         ports:
  22 └         - containerPort: 80
────────────────────────────────────────



legacy-pod1.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'legacy-pod' of 'statefulset' 'legacy-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'legacy-pod' of StatefulSet 'legacy-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legacy-pod1.yaml:6-18
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: legacy-pod
   9 │   serviceName: legacy-pod
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: legacy-pod
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legacy-pod1.yaml:6-18
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: legacy-pod
   9 │   serviceName: legacy-pod
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: legacy-pod
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "legacy-pod" of statefulset "legacy-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): statefulset legacy-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 legacy-pod1.yaml:4
────────────────────────────────────────
   4 [   name: legacy-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container legacy-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): statefulset legacy-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legacy-pod1.yaml:15-18
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container legacy-pod in statefulset legacy-pod (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 legacy-pod1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name: legacy-pod
  17 │         image: nicolaka/netshoot
  18 └         command: ["tail",  "-f", "/dev/null"]
────────────────────────────────────────



legacy-pod_1.yaml (kubernetes)
==============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'legacy-pod' of 'statefulset' 'legacy-pod' in 'ns-tanpa-istio' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'legacy-pod' of StatefulSet 'legacy-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'legacy-pod' of StatefulSet 'legacy-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legacy-pod_1.yaml:7-22
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       app: legacy-pod
  10 │   serviceName: legacy-pod
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: legacy-pod
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legacy-pod_1.yaml:7-22
────────────────────────────────────────
   7 ┌   selector:
   8 │     matchLabels:
   9 │       app: legacy-pod
  10 │   serviceName: legacy-pod
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         app: legacy-pod
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "legacy-pod" of statefulset "legacy-pod" in "ns-tanpa-istio" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container legacy-pod in ns-tanpa-istio namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0118 (HIGH): statefulset legacy-pod in ns-tanpa-istio namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legacy-pod_1.yaml:16-22
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container legacy-pod in statefulset legacy-pod (namespace: ns-tanpa-istio) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 legacy-pod_1.yaml:17-22
────────────────────────────────────────
  17 ┌       - name: legacy-pod
  18 │         image: nicolaka/netshoot
  19 │         command:
  20 │         - tail
  21 │         - -f
  22 └         - /dev/null
────────────────────────────────────────



legacy-redirects.deployment.yaml (kubernetes)
=============================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 9, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'web' of Deployment 'legacy-redirects' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'web' of Deployment 'legacy-redirects' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'web' of 'deployment' 'legacy-redirects' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'web' of Deployment 'legacy-redirects' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'web' of Deployment 'legacy-redirects' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'web' of Deployment 'legacy-redirects' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'web' of Deployment 'legacy-redirects' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'web' of Deployment 'legacy-redirects' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legacy-redirects.deployment.yaml:7-42
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       name: legacy-redirects
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         name: legacy-redirects
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legacy-redirects.deployment.yaml:7-42
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       name: legacy-redirects
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 │         name: legacy-redirects
  15 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "web" of deployment "legacy-redirects" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment legacy-redirects in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 legacy-redirects.deployment.yaml:5
────────────────────────────────────────
   5 [   name: legacy-redirects
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment legacy-redirects in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────
 legacy-redirects.deployment.yaml:1
────────────────────────────────────────
   1 [ ---
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container legacy-redirects in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment legacy-redirects in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legacy-redirects.deployment.yaml:16-42
────────────────────────────────────────
  16 ┌       containers:
  17 │       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 └               key: ip
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container web in deployment legacy-redirects (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 legacy-redirects.deployment.yaml:17-40
────────────────────────────────────────
  17 ┌       - name: web
  18 │         image: us-docker.pkg.dev/exac-gnomad/gnomad/legacy-redirects:latest
  19 │         env:
  20 │         - name: INGRESS_IP
  21 │           valueFrom:
  22 │             configMapKeyRef:
  23 │               name: legacy-redirects-settings
  24 │               key: ip
  25 └         ports:
  ..   
────────────────────────────────────────



legacy-redirects.ingress.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legacy-redirects.ingress.yaml:7-16
────────────────────────────────────────
   7 ┌     rules:
   8 │     - http:
   9 │         paths:
  10 │         - backend:
  11 │             service:
  12 │               name: legacy-redirects-nodeport
  13 │               port:
  14 │                 number: 80
  15 │           path: /*
  16 └           pathType: ImplementationSpecific
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legacy-redirects.ingress.yaml:7-16
────────────────────────────────────────
   7 ┌     rules:
   8 │     - http:
   9 │         paths:
  10 │         - backend:
  11 │             service:
  12 │               name: legacy-redirects-nodeport
  13 │               port:
  14 │                 number: 80
  15 │           path: /*
  16 └           pathType: ImplementationSpecific
────────────────────────────────────────



legacy-redirects.service.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legacy-redirects.service.yaml:7-12
────────────────────────────────────────
   7 ┌     type: NodePort
   8 │     selector:
   9 │       name: legacy-redirects
  10 │     ports:
  11 │     - port: 80
  12 └       targetPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legacy-redirects.service.yaml:7-12
────────────────────────────────────────
   7 ┌     type: NodePort
   8 │     selector:
   9 │       name: legacy-redirects
  10 │     ports:
  11 │     - port: 80
  12 └       targetPort: 80
────────────────────────────────────────



legocarla-deployment.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 98, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 9, MEDIUM: 5, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'carla-server' of Deployment 'legocarla' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'carla-server' of Deployment 'legocarla' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'carla-server' of 'deployment' 'legocarla' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0009 (HIGH): Deployment 'legocarla' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 legocarla-deployment.yaml:12-90
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       io.kompose.service: legocarla
  16 │   strategy: {}
  17 │   template:
  18 │     metadata:
  19 │       annotations:
  20 └         kompose.cmd: kompose convert
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'carla-server' of Deployment 'legocarla' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'carla-server' of Deployment 'legocarla' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'carla-server' of Deployment 'legocarla' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'carla-server' of Deployment 'legocarla' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'carla-server' of Deployment 'legocarla' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Deployment 'legocarla' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 legocarla-deployment.yaml:12-90
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       io.kompose.service: legocarla
  16 │   strategy: {}
  17 │   template:
  18 │     metadata:
  19 │       annotations:
  20 └         kompose.cmd: kompose convert
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legocarla-deployment.yaml:12-90
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       io.kompose.service: legocarla
  16 │   strategy: {}
  17 │   template:
  18 │     metadata:
  19 │       annotations:
  20 └         kompose.cmd: kompose convert
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legocarla-deployment.yaml:12-90
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       io.kompose.service: legocarla
  16 │   strategy: {}
  17 │   template:
  18 │     metadata:
  19 │       annotations:
  20 └         kompose.cmd: kompose convert
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "carla-server" of deployment "legocarla" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment legocarla in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 legocarla-deployment.yaml:4-10
────────────────────────────────────────
   4 ┌   annotations:
   5 │     kompose.cmd: kompose convert
   6 │     kompose.version: 1.26.0 (40646f47)
   7 │   creationTimestamp: null
   8 │   labels:
   9 │     io.kompose.service: legocarla
  10 └   name: legocarla
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment legocarla in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legocarla-deployment.yaml:26-90
────────────────────────────────────────
  26 ┌       volumes:
  27 │         - name: x11-socket
  28 │           hostPath:
  29 │             path: /tmp/.X11-unix
  30 │       hostNetwork: true
  31 │       containers:
  32 │         - command:
  33 │             - /bin/bash
  34 └           # args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container carla-server in deployment legocarla (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 legocarla-deployment.yaml:32-89
────────────────────────────────────────
  32 ┌         - command:
  33 │             - /bin/bash
  34 │           # args:
  35 │           #   - /home/CARLA_0.9.15/CarlaUE4.sh
  36 │           #   - -RenderOffScreen
  37 │           volumeMounts:
  38 │             - name: x11-socket
  39 │               mountPath: /tmp/.X11-unix
  40 └           livenessProbe:
  ..   
────────────────────────────────────────



legocarla-deployment_2.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 95, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'carla-server' of Pod 'legocarla' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'carla-server' of Pod 'legocarla' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'carla-server' of 'pod' 'legocarla' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0009 (HIGH): Pod 'legocarla' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 legocarla-deployment_2.yaml:8-38
────────────────────────────────────────
   8 ┌   hostNetwork: true
   9 │   containers:
  10 │     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 └       # ports:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'carla-server' of Pod 'legocarla' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'carla-server' of Pod 'legocarla' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'carla-server' of Pod 'legocarla' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'carla-server' of Pod 'legocarla' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'carla-server' of Pod 'legocarla' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'carla-server' of Pod 'legocarla' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'carla-server' of Pod 'legocarla' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'carla-server' of Pod 'legocarla' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'carla-server' of Pod 'legocarla' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legocarla-deployment_2.yaml:8-38
────────────────────────────────────────
   8 ┌   hostNetwork: true
   9 │   containers:
  10 │     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 └       # ports:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legocarla-deployment_2.yaml:8-38
────────────────────────────────────────
   8 ┌   hostNetwork: true
   9 │   containers:
  10 │     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 └       # ports:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "carla-server" of pod "legocarla" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod legocarla in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 legocarla-deployment_2.yaml:4-6
────────────────────────────────────────
   4 ┌   name: legocarla
   5 │   labels:
   6 └     app: carla
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod legocarla in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 legocarla-deployment_2.yaml:8-38
────────────────────────────────────────
   8 ┌   hostNetwork: true
   9 │   containers:
  10 │     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 └       # ports:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container carla-server in pod legocarla (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 legocarla-deployment_2.yaml:10-38
────────────────────────────────────────
  10 ┌     - image: jiapengzhao/lego_carla_server:CMDtest
  11 │       name: carla-server
  12 │       # command: [/bin/bash] 
  13 │       # args: ["-c", "nvidia-smi"]
  14 │       stdin: true
  15 │       tty: true
  16 │       # ports:
  17 │       #   - containerPort: 80
  18 └       #     containerPort: 2000
  ..   
────────────────────────────────────────



legocarla-service.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 legocarla-service.yaml:12-23
────────────────────────────────────────
  12 ┌   ports:
  13 │     - name: "2000"
  14 │       port: 2000
  15 │       targetPort: 2000
  16 │     - name: "2001"
  17 │       port: 2001
  18 │       targetPort: 2001
  19 │     - name: "2002"
  20 └       port: 2002
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 legocarla-service.yaml:12-23
────────────────────────────────────────
  12 ┌   ports:
  13 │     - name: "2000"
  14 │       port: 2000
  15 │       targetPort: 2000
  16 │     - name: "2001"
  17 │       port: 2001
  18 │       targetPort: 2001
  19 │     - name: "2002"
  20 └       port: 2002
  ..   
────────────────────────────────────────



lemon.yaml (kubernetes)
=======================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lemon' of Pod 'lemon' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lemon' of Pod 'lemon' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lemon' of 'pod' 'lemon' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lemon' of Pod 'lemon' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lemon' of Pod 'lemon' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lemon' of Pod 'lemon' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lemon' of Pod 'lemon' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lemon' of Pod 'lemon' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0036 (MEDIUM): Container of Pod 'lemon' should set 'spec.automountServiceAccountToken' to false
════════════════════════════════════════
ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false

See https://avd.aquasec.com/misconfig/ksv036
────────────────────────────────────────
 lemon.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lemon.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lemon.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lemon" of pod "lemon" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lemon in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lemon.yaml:4-10
────────────────────────────────────────
   4 ┌   creationTimestamp: "2024-02-20T23:16:16Z"
   5 │   labels:
   6 │     run: lemon
   7 │   name: lemon
   8 │   namespace: default
   9 │   resourceVersion: "6586"
  10 └   uid: 825f077a-7e10-43b1-84d6-bd910b4262aa
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lemon in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemon.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lemon in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemon.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────



lemon1.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lemon' of Pod 'lemon' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lemon' of Pod 'lemon' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lemon' of 'pod' 'lemon' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lemon' of Pod 'lemon' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lemon' of Pod 'lemon' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lemon' of Pod 'lemon' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lemon' of Pod 'lemon' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lemon' of Pod 'lemon' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lemon' of Pod 'lemon' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0036 (MEDIUM): Container of Pod 'lemon' should set 'spec.automountServiceAccountToken' to false
════════════════════════════════════════
ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false

See https://avd.aquasec.com/misconfig/ksv036
────────────────────────────────────────
 lemon1.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lemon1.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lemon1.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lemon" of pod "lemon" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lemon in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lemon1.yaml:4-10
────────────────────────────────────────
   4 ┌   creationTimestamp: "2024-02-20T23:16:16Z"
   5 │   labels:
   6 │     run: lemon
   7 │   name: lemon
   8 │   namespace: default
   9 │   resourceVersion: "156446"
  10 └   uid: 825f077a-7e10-43b1-84d6-bd910b4262aa
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lemon in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemon1.yaml:13-22
────────────────────────────────────────
  13 ┌   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 │     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  21 │       name: kube-api-access-qmh2h
  22 └       readOnly: true
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lemon in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemon1.yaml:12-61
────────────────────────────────────────
  12 ┌   containers:
  13 │   - image: nginx
  14 │     imagePullPolicy: Always
  15 │     name: lemon
  16 │     resources: {}
  17 │     terminationMessagePath: /dev/termination-log
  18 │     terminationMessagePolicy: File
  19 │     volumeMounts:
  20 └     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  ..   
────────────────────────────────────────



lemp-service.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lemp-service.yaml:6-13
────────────────────────────────────────
   6 ┌   type: NodePort
   7 │   selector:
   8 │     app: lemp-wp
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 │       targetPort: 80
  13 └       nodePort: 30008
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lemp-service.yaml:6-13
────────────────────────────────────────
   6 ┌   type: NodePort
   7 │   selector:
   8 │     app: lemp-wp
   9 │   ports:
  10 │     - protocol: TCP
  11 │       port: 80
  12 │       targetPort: 80
  13 └       nodePort: 30008
────────────────────────────────────────



lemp-wp-deployment.yaml (kubernetes)
====================================
Tests: 132 (SUCCESSES: 96, FAILURES: 36)
Failures: 36 (UNKNOWN: 0, LOW: 23, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'mysql-container' of Deployment 'lemp-wp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'mysql-container' of 'deployment' 'lemp-wp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx-php-container' of 'deployment' 'lemp-wp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'mysql-container' of Deployment 'lemp-wp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'mysql-container' of Deployment 'lemp-wp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'mysql-container' of Deployment 'lemp-wp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx-php-container' of Deployment 'lemp-wp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lemp-wp-deployment.yaml:6-94
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lemp-wp
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lemp-wp
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lemp-wp-deployment.yaml:6-94
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lemp-wp
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lemp-wp
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "mysql-container" of deployment "lemp-wp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx-php-container" of deployment "lemp-wp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lemp-wp in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lemp-wp-deployment.yaml:4
────────────────────────────────────────
   4 [   name: lemp-wp
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment lemp-wp in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lemp-wp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemp-wp-deployment.yaml:52-84
────────────────────────────────────────
  52 ┌         - name: mysql-container
  53 │           image: mysql:8.0
  54 │           ports:
  55 │             - containerPort: 3306
  56 │           volumeMounts:
  57 │             - name: mysql-config-volume
  58 │               mountPath: /etc/mysql/conf.d
  59 │           env:
  60 └             - name: MYSQL_ROOT_PASSWORD
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lemp-wp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lemp-wp in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lemp-wp-deployment.yaml:15-94
────────────────────────────────────────
  15 ┌       containers:
  16 │         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 └               subPath: php.ini
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container nginx-php-container in deployment lemp-wp (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lemp-wp-deployment.yaml:16-51
────────────────────────────────────────
  16 ┌         - name: nginx-php-container
  17 │           image: webdevops/php-nginx:alpine-3-php7
  18 │           ports:
  19 │             - containerPort: 80
  20 │           volumeMounts:
  21 │             - name: php-config-volume
  22 │               mountPath: /opt/docker/etc/php/php.ini
  23 │               subPath: php.ini
  24 └             - name: app-config
  ..   
────────────────────────────────────────



les-gorgones.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │   - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │   - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────



les-gorgones_1.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'database' of Deployment 'pm-lg-database' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'database' of Deployment 'pm-lg-database' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'database' of 'deployment' 'pm-lg-database' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'database' of Deployment 'pm-lg-database' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'database' of Deployment 'pm-lg-database' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'database' of Deployment 'pm-lg-database' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'database' of Deployment 'pm-lg-database' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'database' of Deployment 'pm-lg-database' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'database' of Deployment 'pm-lg-database' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'database' of Deployment 'pm-lg-database' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'database' of Deployment 'pm-lg-database' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'database' of Deployment 'pm-lg-database' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones_1.yaml:6-28
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: pm-lg
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: pm-lg
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones_1.yaml:6-28
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: pm-lg
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: pm-lg
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "database" of deployment "pm-lg-database" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment pm-lg-database in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 les-gorgones_1.yaml:4
────────────────────────────────────────
   4 [   name: pm-lg-database
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pm-lg-database in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment pm-lg-database in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 les-gorgones_1.yaml:15-28
────────────────────────────────────────
  15 ┌       volumes:
  16 │       - name: pm-lg-db-persistent-storage
  17 │         persistentVolumeClaim:
  18 │           claimName: pm-lg-db-pvc
  19 │       containers:
  20 │       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 └         volumeMounts:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container database in deployment pm-lg-database (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 les-gorgones_1.yaml:20-28
────────────────────────────────────────
  20 ┌       - name: database
  21 │         image: paulinemdt/microservice-database-image
  22 │         imagePullPolicy: Always
  23 │         volumeMounts:
  24 │         - name: pm-lg-db-persistent-storage
  25 │           mountPath: /var/lib/mysql
  26 │         ports:
  27 │         - containerPort: 3306
  28 └           name: pm-lg-web-db
────────────────────────────────────────



les-gorgones_2.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones_2.yaml:6-13
────────────────────────────────────────
   6 ┌   type: ClusterIP
   7 │   selector:
   8 │     app: pm-lg
   9 │   ports:
  10 │   - name: pm-lg-database-port
  11 │     protocol: TCP
  12 │     port: 3306
  13 └     targetPort: pm-lg-web-db
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones_2.yaml:6-13
────────────────────────────────────────
   6 ┌   type: ClusterIP
   7 │   selector:
   8 │     app: pm-lg
   9 │   ports:
  10 │   - name: pm-lg-database-port
  11 │     protocol: TCP
  12 │     port: 3306
  13 └     targetPort: pm-lg-web-db
────────────────────────────────────────



les-gorgones_3.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 95, FAILURES: 22)
Failures: 22 (UNKNOWN: 0, LOW: 13, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pm-lg-laravel-api-container' of 'deployment' 'pm-lg-backend' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pm-lg-laravel-api-container' of Deployment 'pm-lg-backend' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones_3.yaml:6-24
────────────────────────────────────────
   6 ┌   replicas: 3
   7 │   selector:
   8 │     matchLabels:
   9 │       app: pm-lg
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: pm-lg
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones_3.yaml:6-24
────────────────────────────────────────
   6 ┌   replicas: 3
   7 │   selector:
   8 │     matchLabels:
   9 │       app: pm-lg
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: pm-lg
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pm-lg-laravel-api-container" of deployment "pm-lg-backend" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment pm-lg-backend in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 les-gorgones_3.yaml:4
────────────────────────────────────────
   4 [   name: pm-lg-backend
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment pm-lg-backend in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pm-lg-backend in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment pm-lg-backend in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 les-gorgones_3.yaml:15-24
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pm-lg-laravel-api-container in deployment pm-lg-backend (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 les-gorgones_3.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-laravel-api-container
  17 │         image: paulinemdt/microservice-laravel-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: DB_HOST
  21 │           value: pm-lg-database-service
  22 │         ports:
  23 │         - containerPort: 80
  24 └           name: pm-lg-web-api
────────────────────────────────────────



les-gorgones_4.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones_4.yaml:6-13
────────────────────────────────────────
   6 ┌   type: LoadBalancer
   7 │   selector:
   8 │     app: pm-lg
   9 │   ports:
  10 │   - name: pm-lg-laravel-api-port
  11 │     protocol: TCP
  12 │     port: 80
  13 └     targetPort: pm-lg-web-api
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones_4.yaml:6-13
────────────────────────────────────────
   6 ┌   type: LoadBalancer
   7 │   selector:
   8 │     app: pm-lg
   9 │   ports:
  10 │   - name: pm-lg-laravel-api-port
  11 │     protocol: TCP
  12 │     port: 80
  13 └     targetPort: pm-lg-web-api
────────────────────────────────────────



les-gorgones_5.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'pm-lg-react-app-container' of 'deployment' 'pm-lg-frontend' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'pm-lg-react-app-container' of Deployment 'pm-lg-frontend' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones_5.yaml:6-24
────────────────────────────────────────
   6 ┌   replicas: 3
   7 │   selector:
   8 │     matchLabels:
   9 │       app: pm-lg
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: pm-lg
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones_5.yaml:6-24
────────────────────────────────────────
   6 ┌   replicas: 3
   7 │   selector:
   8 │     matchLabels:
   9 │       app: pm-lg
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: pm-lg
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "pm-lg-react-app-container" of deployment "pm-lg-frontend" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment pm-lg-frontend in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 les-gorgones_5.yaml:4
────────────────────────────────────────
   4 [   name: pm-lg-frontend
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pm-lg-frontend in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment pm-lg-frontend in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 les-gorgones_5.yaml:15-24
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container pm-lg-react-app-container in deployment pm-lg-frontend (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 les-gorgones_5.yaml:16-24
────────────────────────────────────────
  16 ┌       - name: pm-lg-react-app-container
  17 │         image: paulinemdt/microservice-react-image
  18 │         imagePullPolicy: Always
  19 │         env:
  20 │         - name: REACT_APP_API_URL
  21 │           value: http://4.225.73.105:80/
  22 │         ports:
  23 │         - containerPort: 3000
  24 └           name: pm-lg-web-app
────────────────────────────────────────



les-gorgones_6.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 les-gorgones_6.yaml:6-13
────────────────────────────────────────
   6 ┌   type: LoadBalancer
   7 │   selector:
   8 │     app: pm-lg
   9 │   ports:
  10 │   - name: pm-lg-react-app-port
  11 │     protocol: TCP
  12 │     port: 3000
  13 └     targetPort: pm-lg-web-app
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 les-gorgones_6.yaml:6-13
────────────────────────────────────────
   6 ┌   type: LoadBalancer
   7 │   selector:
   8 │     app: pm-lg
   9 │   ports:
  10 │   - name: pm-lg-react-app-port
  11 │     protocol: TCP
  12 │     port: 3000
  13 └     targetPort: pm-lg-web-app
────────────────────────────────────────



letschat-srv.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 letschat-srv.yaml:6-12
────────────────────────────────────────
   6 ┌   type: NodePort
   7 │   ports:
   8 │   - name: http
   9 │     port: 8080
  10 │     targetPort: http-server
  11 │   selector:
  12 └     name: letschat
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 letschat-srv.yaml:6-12
────────────────────────────────────────
   6 ┌   type: NodePort
   7 │   ports:
   8 │   - name: http
   9 │     port: 8080
  10 │     targetPort: http-server
  11 │   selector:
  12 └     name: letschat
────────────────────────────────────────



letsencrypt-job.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 95, FAILURES: 22)
Failures: 22 (UNKNOWN: 0, LOW: 13, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'letsencrypt' of 'job' 'letsencrypt-job' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'letsencrypt' of Job 'letsencrypt-job' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'letsencrypt' of Job 'letsencrypt-job' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 letsencrypt-job.yaml:8-28
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: letsencrypt
  11 │       labels:
  12 │         app: letsencrypt
  13 │     spec:
  14 │       containers:
  15 │       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 └         name: letsencrypt
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 letsencrypt-job.yaml:8-28
────────────────────────────────────────
   8 ┌   template:
   9 │     metadata:
  10 │       name: letsencrypt
  11 │       labels:
  12 │         app: letsencrypt
  13 │     spec:
  14 │       containers:
  15 │       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 └         name: letsencrypt
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "letsencrypt" of job "letsencrypt-job" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): job letsencrypt-job in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 letsencrypt-job.yaml:4-6
────────────────────────────────────────
   4 ┌   name: letsencrypt-job
   5 │   labels:
   6 └     app: letsencrypt
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): job letsencrypt-job in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container letsencrypt-job in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): job letsencrypt-job in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 letsencrypt-job.yaml:14-28
────────────────────────────────────────
  14 ┌       containers:
  15 │       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 └         - name: DOMAINS
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container letsencrypt in job letsencrypt-job (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 letsencrypt-job.yaml:15-27
────────────────────────────────────────
  15 ┌       - image: quay.io/hiphipjorge/kube-nginx-letsencrypt:latest
  16 │         name: letsencrypt
  17 │         imagePullPolicy: Always
  18 │         ports:
  19 │         - name: letsencrypt
  20 │           containerPort: 80
  21 │         env:
  22 │         - name: DOMAINS
  23 └           value: cloud08.core.wits.ac.za
  ..   
────────────────────────────────────────



lgtm1.yaml (kubernetes)
=======================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1.yaml:13-25
────────────────────────────────────────
  13 ┌   podSelector:
  14 │     matchLabels:
  15 │       app.kubernetes.io/instance: lgtm
  16 │       app.kubernetes.io/name: minio
  17 │   policyTypes:
  18 │   - Ingress
  19 │   - Egress
  20 │   egress:
  21 └   - {}
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1.yaml:13-25
────────────────────────────────────────
  13 ┌   podSelector:
  14 │     matchLabels:
  15 │       app.kubernetes.io/instance: lgtm
  16 │       app.kubernetes.io/name: minio
  17 │   policyTypes:
  18 │   - Ingress
  19 │   - Egress
  20 │   egress:
  21 └   - {}
  ..   
────────────────────────────────────────



lgtm1_1.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_1.yaml:13-17
────────────────────────────────────────
  13 ┌   maxUnavailable: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/instance: lgtm
  17 └       app.kubernetes.io/name: minio
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_1.yaml:13-17
────────────────────────────────────────
  13 ┌   maxUnavailable: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/instance: lgtm
  17 └       app.kubernetes.io/name: minio
────────────────────────────────────────



lgtm1_10.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'lgtm-grafana' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"      - key"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



lgtm1_13.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'mimir-cm' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"      secret_access_key"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



lgtm1_15.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_15.yaml:13-17
────────────────────────────────────────
  13 ┌   accessModes:
  14 │   - ReadWriteOnce
  15 │   resources:
  16 │     requests:
  17 └       storage: 8Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_15.yaml:13-17
────────────────────────────────────────
  13 ┌   accessModes:
  14 │   - ReadWriteOnce
  15 │   resources:
  16 │     requests:
  17 └       storage: 8Gi
────────────────────────────────────────



lgtm1_16.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'lgtm-grafana-clusterrole' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 lgtm1_16.yaml:12-20
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ''
  14 │   resources:
  15 │   - configmaps
  16 │   - secrets
  17 │   verbs:
  18 │   - get
  19 │   - watch
  20 └   - list
────────────────────────────────────────



lgtm1_22.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_22.yaml:13-21
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: service
  16 │     port: 80
  17 │     protocol: TCP
  18 │     targetPort: 3000
  19 │   selector:
  20 │     app.kubernetes.io/name: grafana
  21 └     app.kubernetes.io/instance: lgtm
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_22.yaml:13-21
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: service
  16 │     port: 80
  17 │     protocol: TCP
  18 │     targetPort: 3000
  19 │   selector:
  20 │     app.kubernetes.io/name: grafana
  21 └     app.kubernetes.io/instance: lgtm
────────────────────────────────────────



lgtm1_23.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_23.yaml:13-21
────────────────────────────────────────
  13 ┌   clusterIP: None
  14 │   ports:
  15 │   - port: 3100
  16 │     protocol: TCP
  17 │     name: http-metrics
  18 │     targetPort: http-metrics
  19 │   selector:
  20 │     app: loki
  21 └     release: lgtm
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_23.yaml:13-21
────────────────────────────────────────
  13 ┌   clusterIP: None
  14 │   ports:
  15 │   - port: 3100
  16 │     protocol: TCP
  17 │     name: http-metrics
  18 │     targetPort: http-metrics
  19 │   selector:
  20 │     app: loki
  21 └     release: lgtm
────────────────────────────────────────



lgtm1_24.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_24.yaml:12-22
────────────────────────────────────────
  12 ┌   type: ClusterIP
  13 │   clusterIP: None
  14 │   publishNotReadyAddresses: true
  15 │   ports:
  16 │   - name: http
  17 │     port: 7946
  18 │     targetPort: memberlist-port
  19 │     protocol: TCP
  20 └   selector:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_24.yaml:12-22
────────────────────────────────────────
  12 ┌   type: ClusterIP
  13 │   clusterIP: None
  14 │   publishNotReadyAddresses: true
  15 │   ports:
  16 │   - name: http
  17 │     port: 7946
  18 │     targetPort: memberlist-port
  19 │     protocol: TCP
  20 └   selector:
  ..   
────────────────────────────────────────



lgtm1_25.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_25.yaml:13-21
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - port: 3100
  16 │     protocol: TCP
  17 │     name: http-metrics
  18 │     targetPort: http-metrics
  19 │   selector:
  20 │     app: loki
  21 └     release: lgtm
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_25.yaml:13-21
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - port: 3100
  16 │     protocol: TCP
  17 │     name: http-metrics
  18 │     targetPort: http-metrics
  19 │   selector:
  20 │     app: loki
  21 └     release: lgtm
────────────────────────────────────────



lgtm1_26.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_26.yaml:12-20
────────────────────────────────────────
  12 ┌   type: ClusterIP
  13 │   ports:
  14 │   - port: 9009
  15 │     targetPort: http
  16 │     protocol: TCP
  17 │     name: http
  18 │   selector:
  19 │     app.kubernetes.io/name: mimir
  20 └     app.kubernetes.io/instance: lgtm
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_26.yaml:12-20
────────────────────────────────────────
  12 ┌   type: ClusterIP
  13 │   ports:
  14 │   - port: 9009
  15 │     targetPort: http
  16 │     protocol: TCP
  17 │     name: http
  18 │   selector:
  19 │     app.kubernetes.io/name: mimir
  20 └     app.kubernetes.io/instance: lgtm
────────────────────────────────────────



lgtm1_27.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_27.yaml:13-25
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: minio-api
  16 │     port: 9000
  17 │     targetPort: minio-api
  18 │     nodePort: null
  19 │   - name: minio-console
  20 │     port: 9001
  21 └     targetPort: minio-console
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_27.yaml:13-25
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: minio-api
  16 │     port: 9000
  17 │     targetPort: minio-api
  18 │     nodePort: null
  19 │   - name: minio-console
  20 │     port: 9001
  21 └     targetPort: minio-console
  ..   
────────────────────────────────────────



lgtm1_28.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_28.yaml:13-60
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: tempo-prom-metrics
  16 │     port: 3100
  17 │     targetPort: 3100
  18 │   - name: tempo-jaeger-thrift-compact
  19 │     port: 6831
  20 │     protocol: UDP
  21 └     targetPort: 6831
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_28.yaml:13-60
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: tempo-prom-metrics
  16 │     port: 3100
  17 │     targetPort: 3100
  18 │   - name: tempo-jaeger-thrift-compact
  19 │     port: 6831
  20 │     protocol: UDP
  21 └     targetPort: 6831
  ..   
────────────────────────────────────────



lgtm1_30.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'mimir' of Deployment 'lgtm-mimir' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'mimir' of 'deployment' 'lgtm-mimir' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'mimir' of Deployment 'lgtm-mimir' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'mimir' of Deployment 'lgtm-mimir' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'mimir' of Deployment 'lgtm-mimir' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_30.yaml:12-48
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app.kubernetes.io/name: mimir
  16 │       app.kubernetes.io/instance: lgtm
  17 │   template:
  18 │     metadata:
  19 │       labels:
  20 └         helm.sh/chart: mimir-0.1.0
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_30.yaml:12-48
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app.kubernetes.io/name: mimir
  16 │       app.kubernetes.io/instance: lgtm
  17 │   template:
  18 │     metadata:
  19 │       labels:
  20 └         helm.sh/chart: mimir-0.1.0
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "mimir" of deployment "lgtm-mimir" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lgtm-mimir in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lgtm1_30.yaml:4-10
────────────────────────────────────────
   4 ┌   name: lgtm-mimir
   5 │   labels:
   6 │     helm.sh/chart: mimir-0.1.0
   7 │     app.kubernetes.io/name: mimir
   8 │     app.kubernetes.io/instance: lgtm
   9 │     app.kubernetes.io/version: 2.13.0
  10 └     app.kubernetes.io/managed-by: Helm
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lgtm-mimir in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lgtm-mimir in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lgtm1_30.yaml:26-48
────────────────────────────────────────
  26 ┌       serviceAccountName: default
  27 │       securityContext: {}
  28 │       containers:
  29 │       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 └         - -target=all
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container mimir in deployment lgtm-mimir (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lgtm1_30.yaml:29-44
────────────────────────────────────────
  29 ┌       - name: mimir
  30 │         securityContext: {}
  31 │         image: grafana/mimir:2.13.0
  32 │         imagePullPolicy: IfNotPresent
  33 │         args:
  34 │         - -target=all
  35 │         - -config.expand-env=true
  36 │         - -config.file=/etc/mimir/mimir.yaml
  37 └         ports:
  ..   
────────────────────────────────────────



lgtm1_31.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 110, FAILURES: 6)
Failures: 6 (UNKNOWN: 0, LOW: 5, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0020 (LOW): Container 'minio' of Deployment 'lgtm-minio' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lgtm1_31.yaml:50-141
────────────────────────────────────────
  50 ┌       - name: minio
  51 │         image: docker.io/bitnami/minio:2024.3.15-debian-12-r0
  52 │         imagePullPolicy: IfNotPresent
  53 │         securityContext:
  54 │           allowPrivilegeEscalation: false
  55 │           capabilities:
  56 │             drop:
  57 │             - ALL
  58 └           privileged: false
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'minio' of Deployment 'lgtm-minio' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lgtm1_31.yaml:50-141
────────────────────────────────────────
  50 ┌       - name: minio
  51 │         image: docker.io/bitnami/minio:2024.3.15-debian-12-r0
  52 │         imagePullPolicy: IfNotPresent
  53 │         securityContext:
  54 │           allowPrivilegeEscalation: false
  55 │           capabilities:
  56 │             drop:
  57 │             - ALL
  58 └           privileged: false
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_31.yaml:13-147
────────────────────────────────────────
  13 ┌   selector:
  14 │     matchLabels:
  15 │       app.kubernetes.io/instance: lgtm
  16 │       app.kubernetes.io/name: minio
  17 │   strategy:
  18 │     type: Recreate
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_31.yaml:13-147
────────────────────────────────────────
  13 ┌   selector:
  14 │     matchLabels:
  15 │       app.kubernetes.io/instance: lgtm
  16 │       app.kubernetes.io/name: minio
  17 │   strategy:
  18 │     type: Recreate
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lgtm-minio in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lgtm1_31.yaml:4-11
────────────────────────────────────────
   4 ┌   name: lgtm-minio
   5 │   namespace: default
   6 │   labels:
   7 │     app.kubernetes.io/instance: lgtm
   8 │     app.kubernetes.io/managed-by: Helm
   9 │     app.kubernetes.io/name: minio
  10 │     app.kubernetes.io/version: 2024.9.22
  11 └     helm.sh/chart: minio-14.7.11
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container minio in deployment lgtm-minio (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lgtm1_31.yaml:50-141
────────────────────────────────────────
  50 ┌       - name: minio
  51 │         image: docker.io/bitnami/minio:2024.3.15-debian-12-r0
  52 │         imagePullPolicy: IfNotPresent
  53 │         securityContext:
  54 │           allowPrivilegeEscalation: false
  55 │           capabilities:
  56 │             drop:
  57 │             - ALL
  58 └           privileged: false
  ..   
────────────────────────────────────────



lgtm1_32.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 102, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 0, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'loki' of StatefulSet 'lgtm-loki' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'loki' of StatefulSet 'lgtm-loki' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'loki' of 'statefulset' 'lgtm-loki' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'loki' of StatefulSet 'lgtm-loki' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'loki' of StatefulSet 'lgtm-loki' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'loki' of StatefulSet 'lgtm-loki' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'loki' of StatefulSet 'lgtm-loki' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_32.yaml:13-89
────────────────────────────────────────
  13 ┌   podManagementPolicy: OrderedReady
  14 │   replicas: 1
  15 │   selector:
  16 │     matchLabels:
  17 │       app: loki
  18 │       release: lgtm
  19 │   serviceName: lgtm-loki-headless
  20 │   updateStrategy:
  21 └     type: RollingUpdate
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_32.yaml:13-89
────────────────────────────────────────
  13 ┌   podManagementPolicy: OrderedReady
  14 │   replicas: 1
  15 │   selector:
  16 │     matchLabels:
  17 │       app: loki
  18 │       release: lgtm
  19 │   serviceName: lgtm-loki-headless
  20 │   updateStrategy:
  21 └     type: RollingUpdate
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "loki" of statefulset "lgtm-loki" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): statefulset lgtm-loki in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lgtm1_32.yaml:4-11
────────────────────────────────────────
   4 ┌   name: lgtm-loki
   5 │   namespace: default
   6 │   labels:
   7 │     app: loki
   8 │     chart: loki-2.16.0
   9 │     release: lgtm
  10 │     heritage: Helm
  11 └   annotations: {}
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container loki in statefulset lgtm-loki (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lgtm1_32.yaml:41-76
────────────────────────────────────────
  41 ┌       - name: loki
  42 │         image: grafana/loki:2.8.1
  43 │         imagePullPolicy: IfNotPresent
  44 │         args:
  45 │         - -config.file=/etc/loki/loki.yaml
  46 │         volumeMounts:
  47 │         - name: tmp
  48 │           mountPath: /tmp
  49 └         - name: config
  ..   
────────────────────────────────────────



lgtm1_33.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 101, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'tempo' of StatefulSet 'lgtm-tempo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'tempo' of StatefulSet 'lgtm-tempo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'tempo' of 'statefulset' 'lgtm-tempo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'tempo' of StatefulSet 'lgtm-tempo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'tempo' of StatefulSet 'lgtm-tempo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'tempo' of StatefulSet 'lgtm-tempo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'tempo' of StatefulSet 'lgtm-tempo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'tempo' of StatefulSet 'lgtm-tempo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_33.yaml:13-76
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: tempo
  17 │       app.kubernetes.io/instance: lgtm
  18 │   serviceName: lgtm-tempo-headless
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_33.yaml:13-76
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: tempo
  17 │       app.kubernetes.io/instance: lgtm
  18 │   serviceName: lgtm-tempo-headless
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "tempo" of statefulset "lgtm-tempo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): statefulset lgtm-tempo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lgtm1_33.yaml:4-11
────────────────────────────────────────
   4 ┌   name: lgtm-tempo
   5 │   namespace: default
   6 │   labels:
   7 │     helm.sh/chart: tempo-1.10.3
   8 │     app.kubernetes.io/name: tempo
   9 │     app.kubernetes.io/instance: lgtm
  10 │     app.kubernetes.io/version: 2.5.0
  11 └     app.kubernetes.io/managed-by: Helm
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container tempo in statefulset lgtm-tempo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lgtm1_33.yaml:30-65
────────────────────────────────────────
  30 ┌       - args:
  31 │         - -config.file=/conf/tempo.yaml
  32 │         - -mem-ballast-size-mbs=1024
  33 │         image: grafana/tempo:2.5.0
  34 │         imagePullPolicy: IfNotPresent
  35 │         name: tempo
  36 │         ports:
  37 │         - containerPort: 3100
  38 └           name: prom-metrics
  ..   
────────────────────────────────────────



lgtm1_36.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lgtm-test' of 'pod' 'lgtm-grafana-test' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lgtm-test' of Pod 'lgtm-grafana-test' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_36.yaml:16-33
────────────────────────────────────────
  16 ┌   serviceAccountName: lgtm-grafana-test
  17 │   containers:
  18 │   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 └     - /tests/run.sh
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_36.yaml:16-33
────────────────────────────────────────
  16 ┌   serviceAccountName: lgtm-grafana-test
  17 │   containers:
  18 │   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 └     - /tests/run.sh
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lgtm-test" of pod "lgtm-grafana-test" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lgtm-grafana-test in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lgtm1_36.yaml:4-14
────────────────────────────────────────
   4 ┌   name: lgtm-grafana-test
   5 │   labels:
   6 │     helm.sh/chart: grafana-8.5.1
   7 │     app.kubernetes.io/name: grafana
   8 │     app.kubernetes.io/instance: lgtm
   9 │     app.kubernetes.io/version: 11.2.0
  10 │     app.kubernetes.io/managed-by: Helm
  11 │   annotations:
  12 └     helm.sh/hook: test
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lgtm-grafana-test in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lgtm-grafana-test in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lgtm1_36.yaml:16-33
────────────────────────────────────────
  16 ┌   serviceAccountName: lgtm-grafana-test
  17 │   containers:
  18 │   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 └     - /tests/run.sh
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lgtm-test in pod lgtm-grafana-test (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lgtm1_36.yaml:18-28
────────────────────────────────────────
  18 ┌   - name: lgtm-test
  19 │     image: docker.io/bats/bats:v1.4.1
  20 │     imagePullPolicy: IfNotPresent
  21 │     command:
  22 │     - /opt/bats/bin/bats
  23 │     - -t
  24 │     - /tests/run.sh
  25 │     volumeMounts:
  26 └     - mountPath: /tests
  ..   
────────────────────────────────────────



lgtm1_37.yaml (kubernetes)
==========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'test' of 'pod' 'lgtm-loki-stack-test' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'test' of Pod 'lgtm-loki-stack-test' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lgtm1_37.yaml:13-31
────────────────────────────────────────
  13 ┌   containers:
  14 │   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 └       value: lgtm-loki
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lgtm1_37.yaml:13-31
────────────────────────────────────────
  13 ┌   containers:
  14 │   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 └       value: lgtm-loki
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "test" of pod "lgtm-loki-stack-test" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lgtm-loki-stack-test in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lgtm1_37.yaml:4-11
────────────────────────────────────────
   4 ┌   annotations:
   5 │     helm.sh/hook: test-success
   6 │   labels:
   7 │     app: loki-stack
   8 │     chart: loki-stack-2.10.2
   9 │     release: lgtm
  10 │     heritage: Helm
  11 └   name: lgtm-loki-stack-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lgtm-loki-stack-test in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lgtm-loki-stack-test in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lgtm1_37.yaml:13-31
────────────────────────────────────────
  13 ┌   containers:
  14 │   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 └       value: lgtm-loki
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container test in pod lgtm-loki-stack-test (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lgtm1_37.yaml:14-26
────────────────────────────────────────
  14 ┌   - name: test
  15 │     image: bats/bats:1.8.2
  16 │     imagePullPolicy: ''
  17 │     args:
  18 │     - /var/lib/loki/test.sh
  19 │     env:
  20 │     - name: LOKI_SERVICE
  21 │       value: lgtm-loki
  22 └     - name: LOKI_PORT
  ..   
────────────────────────────────────────



liam-nginx-deployment.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 112, FAILURES: 4)
Failures: 4 (UNKNOWN: 0, LOW: 3, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 liam-nginx-deployment.yaml:6-19
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: nginx
   9 │   replicas: 2
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: nginx
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 liam-nginx-deployment.yaml:6-19
────────────────────────────────────────
   6 ┌   selector:
   7 │     matchLabels:
   8 │       app: nginx
   9 │   replicas: 2
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: nginx
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 liam-nginx-deployment.yaml:4
────────────────────────────────────────
   4 [   name: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 liam-nginx-deployment.yaml:15-19
────────────────────────────────────────
  15 ┌       container:
  16 │       - name: nginx-container
  17 │         image: nginx:latest
  18 │         ports:
  19 └         - containerPort: 80
────────────────────────────────────────



libenessprobe.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'app' of Deployment 'myapp' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'app' of Deployment 'myapp' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'app' of 'deployment' 'myapp' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'app' of Deployment 'myapp' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'app' of Deployment 'myapp' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'app' of Deployment 'myapp' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'app' of Deployment 'myapp' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'app' of Deployment 'myapp' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'app' of Deployment 'myapp' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'app' of Deployment 'myapp' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'app' of Deployment 'myapp' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'app' of Deployment 'myapp' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 libenessprobe.yaml:8-29
────────────────────────────────────────
   8 ┌   selector:
   9 │     matchLabels:
  10 │       type: webapp
  11 │   replicas: 3
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         type: webapp
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 libenessprobe.yaml:8-29
────────────────────────────────────────
   8 ┌   selector:
   9 │     matchLabels:
  10 │       type: webapp
  11 │   replicas: 3
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         type: webapp
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "app" of deployment "myapp" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment myapp in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 libenessprobe.yaml:4-6
────────────────────────────────────────
   4 ┌   name: myapp
   5 │   labels:
   6 └     type: webapp
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment myapp in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container myapp in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 libenessprobe.yaml:18-29
────────────────────────────────────────
  18 ┌       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 │             port: 80
  26 └           initialDelaySeconds: 3
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment myapp in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 libenessprobe.yaml:17-29
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: app
  19 │         image: nginx
  20 │         ports:
  21 │         - containerPort: 80
  22 │         livenessProbe:
  23 │           httpGet:
  24 │             path: /test
  25 └             port: 80
  ..   
────────────────────────────────────────



libenessprobe_1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 libenessprobe_1.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     type: webapp
   8 │   ports:
   9 │   - port: 80
  10 └     targetPort: 80
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 libenessprobe_1.yaml:6-10
────────────────────────────────────────
   6 ┌   selector:
   7 │     type: webapp
   8 │   ports:
   9 │   - port: 80
  10 └     targetPort: 80
────────────────────────────────────────



librechat-configMap.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'librechat-configenv' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"ALLOW_EMAIL_LOGIN", "ALLOW_UNVERIFIED_EMAIL_LOGIN", "BAN_DURATION", "CREDS_KEY", "PORT", "RAG_PORT", "REFRESH_TOKEN_EXPIRY", "SESSION_EXPIRY"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────


AVD-KSV-0109 (HIGH): ConfigMap 'librechat-configenv' in 'default' namespace stores secrets in key(s) or value(s) '{"REFRESH_TOKEN_EXPIRY"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



librechat-configMap1.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'librechat-configenv' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"ALLOW_EMAIL_LOGIN", "ALLOW_UNVERIFIED_EMAIL_LOGIN", "BAN_DURATION", "CREDS_KEY", "PORT", "RAG_PORT", "REFRESH_TOKEN_EXPIRY", "SESSION_EXPIRY"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────


AVD-KSV-0109 (HIGH): ConfigMap 'librechat-configenv' in 'default' namespace stores secrets in key(s) or value(s) '{"REFRESH_TOKEN_EXPIRY"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



librechat-configMap1_1.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'librechat-config' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"      apiKey"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



librechat-configMap_1.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'librechat-config' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"      apiKey"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



librechat-deployment.yaml (kubernetes)
======================================
Tests: 127 (SUCCESSES: 96, FAILURES: 31)
Failures: 31 (UNKNOWN: 0, LOW: 19, MEDIUM: 8, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'librechat' of Deployment 'librechat' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'librechat' of Deployment 'librechat' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'librechat' of 'deployment' 'librechat' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'take-data-dir-ownership' of 'deployment' 'librechat' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'librechat' of Deployment 'librechat' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'librechat' of Deployment 'librechat' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'librechat' of Deployment 'librechat' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'librechat' of Deployment 'librechat' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'librechat' of Deployment 'librechat' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-deployment.yaml:6-68
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: librechat
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         app: librechat
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-deployment.yaml:6-68
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: librechat
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         app: librechat
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "librechat" of deployment "librechat" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "take-data-dir-ownership" of deployment "librechat" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment librechat in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 librechat-deployment.yaml:4
────────────────────────────────────────
   4 [   name: librechat
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container librechat in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 librechat-deployment.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment librechat in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 librechat-deployment.yaml:16-68
────────────────────────────────────────
  16 ┌       initContainers:
  17 │       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 └         - name: image-volume
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container librechat in deployment librechat (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 librechat-deployment.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────



librechat-deployment1.yaml (kubernetes)
=======================================
Tests: 127 (SUCCESSES: 96, FAILURES: 31)
Failures: 31 (UNKNOWN: 0, LOW: 19, MEDIUM: 8, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'librechat' of Deployment 'librechat' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'librechat' of Deployment 'librechat' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'librechat' of 'deployment' 'librechat' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'take-data-dir-ownership' of 'deployment' 'librechat' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'librechat' of Deployment 'librechat' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'librechat' of Deployment 'librechat' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'librechat' of Deployment 'librechat' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'librechat' of Deployment 'librechat' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'librechat' of Deployment 'librechat' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'take-data-dir-ownership' of Deployment 'librechat' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-deployment1.yaml:6-68
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: librechat
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         app: librechat
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-deployment1.yaml:6-68
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   revisionHistoryLimit: 3
   8 │   selector:
   9 │     matchLabels:
  10 │       app: librechat
  11 │   template:
  12 │     metadata:
  13 │       labels:
  14 └         app: librechat
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "librechat" of deployment "librechat" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "take-data-dir-ownership" of deployment "librechat" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment librechat in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 librechat-deployment1.yaml:4
────────────────────────────────────────
   4 [   name: librechat
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container librechat in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 librechat-deployment1.yaml:17-25
────────────────────────────────────────
  17 ┌       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 │         - name: image-volume
  25 └           mountPath: /app/client/public/images
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment librechat in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 librechat-deployment1.yaml:16-68
────────────────────────────────────────
  16 ┌       initContainers:
  17 │       - name: take-data-dir-ownership
  18 │         image: alpine:3
  19 │         command:
  20 │         - sh
  21 │         - -c
  22 │         - chown -R 1000:3000 /app/client/public/images
  23 │         volumeMounts:
  24 └         - name: image-volume
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container librechat in deployment librechat (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 librechat-deployment1.yaml:36-61
────────────────────────────────────────
  36 ┌       - name: librechat
  37 │         image: ghcr.io/danny-avila/librechat:latest
  38 │         ports:
  39 │         - containerPort: 3080
  40 │         resources:
  41 │           requests:
  42 │             cpu: '2'
  43 │             memory: 4Gi
  44 └           limits:
  ..   
────────────────────────────────────────



librechat-ingress.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-ingress.yaml:11-26
────────────────────────────────────────
  11 ┌   ingressClassName: nginx
  12 │   tls:
  13 │     - hosts:
  14 │         - librechat.inspection.alpha.canada.ca
  15 │       secretName: aciacfia-tls
  16 │   rules:
  17 │     - host: librechat.inspection.alpha.canada.ca
  18 │       http:
  19 └         paths:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-ingress.yaml:11-26
────────────────────────────────────────
  11 ┌   ingressClassName: nginx
  12 │   tls:
  13 │     - hosts:
  14 │         - librechat.inspection.alpha.canada.ca
  15 │       secretName: aciacfia-tls
  16 │   rules:
  17 │     - host: librechat.inspection.alpha.canada.ca
  18 │       http:
  19 └         paths:
  ..   
────────────────────────────────────────



librechat-ingress1.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-ingress1.yaml:11-26
────────────────────────────────────────
  11 ┌   ingressClassName: nginx
  12 │   tls:
  13 │     - hosts:
  14 │         - librechat.inspection.alpha.canada.ca
  15 │       secretName: aciacfia-tls
  16 │   rules:
  17 │     - host: librechat.inspection.alpha.canada.ca
  18 │       http:
  19 └         paths:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-ingress1.yaml:11-26
────────────────────────────────────────
  11 ┌   ingressClassName: nginx
  12 │   tls:
  13 │     - hosts:
  14 │         - librechat.inspection.alpha.canada.ca
  15 │       secretName: aciacfia-tls
  16 │   rules:
  17 │     - host: librechat.inspection.alpha.canada.ca
  18 │       http:
  19 └         paths:
  ..   
────────────────────────────────────────



librechat-pvc.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-pvc.yaml:8-13
────────────────────────────────────────
   8 ┌   storageClassName: default
   9 │   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 5Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-pvc.yaml:8-13
────────────────────────────────────────
   8 ┌   storageClassName: default
   9 │   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 5Gi
────────────────────────────────────────



librechat-pvc1.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-pvc1.yaml:8-13
────────────────────────────────────────
   8 ┌   storageClassName: default
   9 │   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 5Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-pvc1.yaml:8-13
────────────────────────────────────────
   8 ┌   storageClassName: default
   9 │   accessModes:
  10 │     - ReadWriteOnce
  11 │   resources:
  12 │     requests:
  13 └       storage: 5Gi
────────────────────────────────────────



librechat-svc.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-svc.yaml:6-11
────────────────────────────────────────
   6 ┌   clusterIP: None
   7 │   selector:
   8 │     app: librechat
   9 │   ports:
  10 │     - port: 3080
  11 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-svc.yaml:6-11
────────────────────────────────────────
   6 ┌   clusterIP: None
   7 │   selector:
   8 │     app: librechat
   9 │   ports:
  10 │     - port: 3080
  11 └       protocol: TCP
────────────────────────────────────────



librechat-svc1.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 librechat-svc1.yaml:6-11
────────────────────────────────────────
   6 ┌   clusterIP: None
   7 │   selector:
   8 │     app: librechat
   9 │   ports:
  10 │     - port: 3080
  11 └       protocol: TCP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 librechat-svc1.yaml:6-11
────────────────────────────────────────
   6 ┌   clusterIP: None
   7 │   selector:
   8 │     app: librechat
   9 │   ports:
  10 │     - port: 3080
  11 └       protocol: TCP
────────────────────────────────────────



licaodecasa.yaml (kubernetes)
=============================
Tests: 153 (SUCCESSES: 101, FAILURES: 52)
Failures: 52 (UNKNOWN: 0, LOW: 27, MEDIUM: 16, HIGH: 9, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'alpine-container' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'redis-container' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ubuntu-container' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'webserver' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'alpine-container' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'redis-container' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ubuntu-container' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'webserver' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'alpine-container' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'redis-container' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ubuntu-container' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'webserver' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 licaodecasa.yaml:8-73
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 licaodecasa.yaml:8-73
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "alpine-container" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "redis-container" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ubuntu-container" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "webserver" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod licaodecasa in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 licaodecasa.yaml:4-6
────────────────────────────────────────
   4 ┌   labels:
   5 │     run: licaodecasa
   6 └   name: licaodecasa
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod licaodecasa in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa.yaml:8-73
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────



licaodecasa1.yaml (kubernetes)
==============================
Tests: 153 (SUCCESSES: 101, FAILURES: 52)
Failures: 52 (UNKNOWN: 0, LOW: 27, MEDIUM: 16, HIGH: 9, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'alpine-container' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'redis-container' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ubuntu-container' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'webserver' of Pod 'licaodecasa' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'alpine-container' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'redis-container' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ubuntu-container' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'webserver' of 'pod' 'licaodecasa' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'alpine-container' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'redis-container' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ubuntu-container' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'webserver' of Pod 'licaodecasa' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'alpine-container' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'redis-container' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ubuntu-container' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'webserver' of Pod 'licaodecasa' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 licaodecasa1.yaml:8-73
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 licaodecasa1.yaml:8-73
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "alpine-container" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "redis-container" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ubuntu-container" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "webserver" of pod "licaodecasa" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod licaodecasa in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 licaodecasa1.yaml:4-6
────────────────────────────────────────
   4 ┌   labels:
   5 │     run: licaodecasa
   6 └   name: licaodecasa
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa1.yaml:47-58
────────────────────────────────────────
  47 ┌   - image: redis
  48 │     name: redis-container
  49 │     volumeMounts:
  50 │     - mountPath: /tmp/redis-volume
  51 │       name: redis-volume
  52 │     resources:
  53 │       limits:
  54 │         cpu: "0.3"
  55 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa1.yaml:34-46
────────────────────────────────────────
  34 ┌   - image: ubuntu
  35 │     name: ubuntu-container
  36 │     volumeMounts:
  37 │     - mountPath: /tmp/ubuntu-volume
  38 │       name: ubuntu-volume
  39 │     command: ["sleep", "3600"]
  40 │     resources:
  41 │       limits:
  42 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa1.yaml:21-33
────────────────────────────────────────
  21 ┌   - image: alpine
  22 │     name: alpine-container
  23 │     volumeMounts:
  24 │     - mountPath: /tmp/alpine-volume
  25 │       name: alpine-volume
  26 │     command: ["sleep", "3600"]
  27 │     resources:
  28 │       limits:
  29 └         cpu: "0.2"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container licaodecasa in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod licaodecasa in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 licaodecasa1.yaml:8-73
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: nginx
  10 │     name: webserver
  11 │     volumeMounts:
  12 │     - mountPath: /tmp/nginx-volume
  13 │       name: nginx-volume
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────



lifecycle-demo.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-demo-container' of 'pod' 'lifecycle-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle-demo.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle-demo.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-demo-container" of pod "lifecycle-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle-demo.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-demo.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-demo.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────



lifecycle-demo1.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-demo-container' of 'pod' 'lifecycle-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle-demo1.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle-demo1.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-demo-container" of pod "lifecycle-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle-demo1.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-demo1.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-demo1.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────



lifecycle-demo2.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-demo-container' of 'pod' 'lifecycle-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle-demo2.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle-demo2.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-demo-container" of pod "lifecycle-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle-demo2.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-demo2.yaml:7-18
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         httpGet:
  15 └           path: /
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-demo2.yaml:6-18
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 └         httpGet:
  ..   
────────────────────────────────────────



lifecycle-events.yaml (kubernetes)
==================================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-demo-container' of 'pod' 'lifecycle-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle-events.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle-events.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-demo-container" of pod "lifecycle-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle-events.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle-events.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lifecycle-demo-container in pod lifecycle-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lifecycle-events.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────



lifecycle.yaml (kubernetes)
===========================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'random-generator' of 'pod' 'pod-with-liveness-check' in 'deploy-test' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'random-generator' of Pod 'pod-with-liveness-check' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle.yaml:7-20
────────────────────────────────────────
   7 ┌   containers:
   8 │     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 └         - containerPort: 8080
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle.yaml:7-20
────────────────────────────────────────
   7 ┌   containers:
   8 │     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 └         - containerPort: 8080
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "random-generator" of pod "pod-with-liveness-check" in "deploy-test" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pod-with-liveness-check in deploy-test namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod pod-with-liveness-check in deploy-test namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle.yaml:7-20
────────────────────────────────────────
   7 ┌   containers:
   8 │     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 └         - containerPort: 8080
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container random-generator in pod pod-with-liveness-check (namespace: deploy-test) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lifecycle.yaml:8-20
────────────────────────────────────────
   8 ┌     - image: k8spatterns/random-generator:1.0
   9 │       name: random-generator
  10 │       env:
  11 │         - name: DELAY_STARTUP
  12 │           value: "20"
  13 │       resources: {}
  14 │       ports:
  15 │         - containerPort: 8080
  16 └       livenessProbe: 
  ..   
────────────────────────────────────────



lifecycle1.yaml (kubernetes)
============================
Tests: 133 (SUCCESSES: 97, FAILURES: 36)
Failures: 36 (UNKNOWN: 0, LOW: 23, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'init' of Deployment 'lifecycle' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'init' of Deployment 'lifecycle' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'init' of 'deployment' 'lifecycle' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-container' of 'deployment' 'lifecycle' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'init' of Deployment 'lifecycle' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'init' of Deployment 'lifecycle' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-container' of Deployment 'lifecycle' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'init' of Deployment 'lifecycle' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'init' of Deployment 'lifecycle' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'init' of Deployment 'lifecycle' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle1.yaml:6-38
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lifecycle
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lifecycle
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle1.yaml:6-38
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lifecycle
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lifecycle
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "init" of deployment "lifecycle" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-container" of deployment "lifecycle" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lifecycle in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle1.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle1.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle1.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lifecycle in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle1.yaml:15-38
────────────────────────────────────────
  15 ┌       initContainers:
  16 │       - name:           init
  17 │         image:          busybox
  18 │         command:       ['sh', '-c', 'sleep 10']
  19 │       containers:
  20 │       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 └         readinessProbe:
  ..   
────────────────────────────────────────



lifecycle2.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 11, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle' of Pod 'lifecycle' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle' of 'pod' 'lifecycle' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle' of Pod 'lifecycle' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle2.yaml:8-20
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 └       limits:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle2.yaml:8-20
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 └       limits:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle" of pod "lifecycle" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle2.yaml:4-6
────────────────────────────────────────
   4 ┌   name: lifecycle
   5 │   labels:
   6 └     name: lifecycle
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod lifecycle in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle2.yaml:8-20
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 └       limits:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lifecycle in pod lifecycle (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lifecycle2.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: registry.cn-beijing.aliyuncs.com/qingfeng666/nginx:latest
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c","echo lifecycle from postStart handle > /usr/share/message "]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────



lifecycle3.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle' of Pod 'lifecycle' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle' of 'pod' 'lifecycle' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle' of Pod 'lifecycle' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle' of Pod 'lifecycle' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle3.yaml:8-20
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 └       limits:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle3.yaml:8-20
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 └       limits:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle" of pod "lifecycle" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle3.yaml:4-6
────────────────────────────────────────
   4 ┌   name: lifecycle
   5 │   labels:
   6 └     name: lifecycle
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): pod lifecycle in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle3.yaml:9-20
────────────────────────────────────────
   9 ┌   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 │       limits:
  17 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle3.yaml:8-20
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: lifecycle
  10 │     image: nginx
  11 │     lifecycle:
  12 │       postStart:
  13 │         exec:
  14 │           command: ["sh", "-c", "echo lifecycle from postStart handle > /usr/share/message"]
  15 │     resources:
  16 └       limits:
  ..   
────────────────────────────────────────



lifecycle4.yaml (kubernetes)
============================
Tests: 133 (SUCCESSES: 97, FAILURES: 36)
Failures: 36 (UNKNOWN: 0, LOW: 23, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'init' of Deployment 'lifecycle' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'init' of Deployment 'lifecycle' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'init' of 'deployment' 'lifecycle' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-container' of 'deployment' 'lifecycle' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'init' of Deployment 'lifecycle' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'init' of Deployment 'lifecycle' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-container' of Deployment 'lifecycle' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'init' of Deployment 'lifecycle' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'init' of Deployment 'lifecycle' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'init' of Deployment 'lifecycle' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'init' of Deployment 'lifecycle' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-container' of Deployment 'lifecycle' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle4.yaml:6-38
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lifecycle
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lifecycle
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle4.yaml:6-38
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lifecycle
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lifecycle
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "init" of deployment "lifecycle" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-container" of deployment "lifecycle" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lifecycle in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle4.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle4.yaml:20-38
────────────────────────────────────────
  20 ┌       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 │         readinessProbe:
  24 │           exec:
  25 │             command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
  26 │           initialDelaySeconds: 35
  27 │         livenessProbe:
  28 └           exec:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle4.yaml:16-18
────────────────────────────────────────
  16 ┌       - name:           init
  17 │         image:          busybox
  18 └         command:       ['sh', '-c', 'sleep 10']
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lifecycle in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle4.yaml:15-38
────────────────────────────────────────
  15 ┌       initContainers:
  16 │       - name:           init
  17 │         image:          busybox
  18 │         command:       ['sh', '-c', 'sleep 10']
  19 │       containers:
  20 │       - name: lifecycle-container
  21 │         image: busybox
  22 │         command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
  23 └         readinessProbe:
  ..   
────────────────────────────────────────



lifecycle5.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-demo-container' of 'pod' 'lifecycle-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-demo-container' of Pod 'lifecycle-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle5.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle5.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-demo-container" of pod "lifecycle-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle5.yaml:4
────────────────────────────────────────
   4 [   name: lifecycle-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle5.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle5.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: lifecycle-demo-container
   8 │     image: nginx
   9 │     lifecycle:
  10 │       postStart:
  11 │         exec:
  12 │           command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │       preStop:
  14 │         exec:
  15 └           command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
────────────────────────────────────────



lifecycle6.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lifecycle-container' of 'pod' 'lifecycle-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'lifecycle-container' of Pod 'lifecycle-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lifecycle-container' of Pod 'lifecycle-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lifecycle6.yaml:7-16
────────────────────────────────────────
   7 ┌   containers:
   8 │     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lifecycle6.yaml:7-16
────────────────────────────────────────
   7 ┌   containers:
   8 │     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lifecycle-container" of pod "lifecycle-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod lifecycle-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lifecycle6.yaml:5
────────────────────────────────────────
   5 [   name: lifecycle-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lifecycle-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle6.yaml:8-16
────────────────────────────────────────
   8 ┌     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod lifecycle-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lifecycle6.yaml:7-16
────────────────────────────────────────
   7 ┌   containers:
   8 │     - image: nginx
   9 │       lifecycle:
  10 │         postStart:
  11 │           exec:
  12 │             command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  13 │         preStop:
  14 │           exec:
  15 │             command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]
  16 └       name: lifecycle-container
────────────────────────────────────────



lightdeploy.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'light' of Deployment 'light' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'light' of Deployment 'light' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'light' of 'deployment' 'light' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'light' of Deployment 'light' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'light' of Deployment 'light' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'light' of Deployment 'light' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'light' of Deployment 'light' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'light' of Deployment 'light' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'light' of Deployment 'light' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'light' of Deployment 'light' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'light' of Deployment 'light' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lightdeploy.yaml:8-24
────────────────────────────────────────
   8 ┌   replicas: 2
   9 │   selector:
  10 │     matchLabels:
  11 │       app: light
  12 │   template:
  13 │     metadata:
  14 │       name: light
  15 │       labels:
  16 └         app: light
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lightdeploy.yaml:8-24
────────────────────────────────────────
   8 ┌   replicas: 2
   9 │   selector:
  10 │     matchLabels:
  11 │       app: light
  12 │   template:
  13 │     metadata:
  14 │       name: light
  15 │       labels:
  16 └         app: light
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "light" of deployment "light" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment light in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lightdeploy.yaml:4-6
────────────────────────────────────────
   4 ┌   name: light
   5 │   labels:
   6 └     app: light
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment light in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container light in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment light in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lightdeploy.yaml:18-24
────────────────────────────────────────
  18 ┌       containers:
  19 │         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 │             - containerPort: 80
  24 └       restartPolicy: Always
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container light in deployment light (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lightdeploy.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────



lightdeploy1.yaml (kubernetes)
==============================
Tests: 117 (SUCCESSES: 96, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'light' of Deployment 'light' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'light' of Deployment 'light' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'light' of 'deployment' 'light' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'light' of Deployment 'light' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'light' of Deployment 'light' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'light' of Deployment 'light' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'light' of Deployment 'light' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'light' of Deployment 'light' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'light' of Deployment 'light' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'light' of Deployment 'light' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'light' of Deployment 'light' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lightdeploy1.yaml:8-24
────────────────────────────────────────
   8 ┌   replicas: 2
   9 │   selector:
  10 │     matchLabels:
  11 │       app: light
  12 │   template:
  13 │     metadata:
  14 │       name: light
  15 │       labels:
  16 └         app: light
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lightdeploy1.yaml:8-24
────────────────────────────────────────
   8 ┌   replicas: 2
   9 │   selector:
  10 │     matchLabels:
  11 │       app: light
  12 │   template:
  13 │     metadata:
  14 │       name: light
  15 │       labels:
  16 └         app: light
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "light" of deployment "light" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment light in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lightdeploy1.yaml:4-6
────────────────────────────────────────
   4 ┌   name: light
   5 │   labels:
   6 └     app: light
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment light in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container light in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment light in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lightdeploy1.yaml:18-24
────────────────────────────────────────
  18 ┌       containers:
  19 │         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 │             - containerPort: 80
  24 └       restartPolicy: Always
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container light in deployment light (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lightdeploy1.yaml:19-23
────────────────────────────────────────
  19 ┌         - name: light
  20 │           image: german29/gn-lighttpd:1.0.0
  21 │           imagePullPolicy: IfNotPresent
  22 │           ports:
  23 └             - containerPort: 80
────────────────────────────────────────



lighthouse-foghorn-role.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'lighthouse-foghorn' shouldn't have access to manage secrets in namespace 'jx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 lighthouse-foghorn-role.yaml:12-21
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ""
  14 │   resources:
  15 │   - namespaces
  16 │   - configmaps
  17 │   - secrets
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 └   - watch
────────────────────────────────────────



lighthouse-gc-jobs-cronjob.yaml (kubernetes)
============================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lighthouse-gc-jobs' of 'cronjob' 'lighthouse-gc-jobs' in 'jx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lighthouse-gc-jobs' of CronJob 'lighthouse-gc-jobs' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:13-48
────────────────────────────────────────
  13 ┌   concurrencyPolicy: Forbid
  14 │   failedJobsHistoryLimit: 1
  15 │   jobTemplate:
  16 │     spec:
  17 │       backoffLimit: 6
  18 │       template:
  19 │         metadata:
  20 │           labels:
  21 └             app: lighthouse-gc-jobs
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:13-48
────────────────────────────────────────
  13 ┌   concurrencyPolicy: Forbid
  14 │   failedJobsHistoryLimit: 1
  15 │   jobTemplate:
  16 │     spec:
  17 │       backoffLimit: 6
  18 │       template:
  19 │         metadata:
  20 │           labels:
  21 └             app: lighthouse-gc-jobs
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lighthouse-gc-jobs" of cronjob "lighthouse-gc-jobs" in "jx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lighthouse-gc-jobs in jx namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): cronjob lighthouse-gc-jobs in jx namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:24-44
────────────────────────────────────────
  24 ┌           containers:
  25 │           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 └             env:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lighthouse-gc-jobs in cronjob lighthouse-gc-jobs (namespace: jx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lighthouse-gc-jobs-cronjob.yaml:25-38
────────────────────────────────────────
  25 ┌           - command:
  26 │             - /home/jx/gc-jobs
  27 │             image: ghcr.io/jenkins-x/lighthouse-gc-jobs:1.14.4
  28 │             imagePullPolicy: IfNotPresent
  29 │             args:
  30 │             - "--namespace=jx"
  31 │             - "--max-age=168h"
  32 │             env:
  33 └             - name: LOG_LEVEL
  ..   
────────────────────────────────────────



lighthouse-gc-jobs-role.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'lighthouse-gc-jobs' shouldn't have access to manage secrets in namespace 'jx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 lighthouse-gc-jobs-role.yaml:12-21
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ""
  14 │   resources:
  15 │   - namespaces
  16 │   - configmaps
  17 │   - secrets
  18 │   verbs:
  19 │   - get
  20 │   - list
  21 └   - watch
────────────────────────────────────────



lighthouse-keeper-deploy.yaml (kubernetes)
==========================================
Tests: 117 (SUCCESSES: 102, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 8, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lighthouse-keeper' of Deployment 'lighthouse-keeper' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lighthouse-keeper' of Deployment 'lighthouse-keeper' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lighthouse-keeper' of 'deployment' 'lighthouse-keeper' in 'jx' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lighthouse-keeper' of Deployment 'lighthouse-keeper' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lighthouse-keeper' of Deployment 'lighthouse-keeper' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lighthouse-keeper' of Deployment 'lighthouse-keeper' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lighthouse-keeper' of Deployment 'lighthouse-keeper' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:15-99
────────────────────────────────────────
  15 ┌   replicas: 1
  16 │   strategy:
  17 │     type: RollingUpdate
  18 │     rollingUpdate:
  19 │       maxSurge: 1
  20 │       maxUnavailable: 1
  21 │   selector:
  22 │     matchLabels:
  23 └       app: lighthouse-keeper
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:15-99
────────────────────────────────────────
  15 ┌   replicas: 1
  16 │   strategy:
  17 │     type: RollingUpdate
  18 │     rollingUpdate:
  19 │       maxSurge: 1
  20 │       maxUnavailable: 1
  21 │   selector:
  22 │     matchLabels:
  23 └       app: lighthouse-keeper
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lighthouse-keeper" of deployment "lighthouse-keeper" in "jx" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lighthouse-keeper in jx namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lighthouse-keeper in jx namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:34-99
────────────────────────────────────────
  34 ┌       serviceAccountName: lighthouse-keeper
  35 │       terminationGracePeriodSeconds: 30
  36 │       containers:
  37 │       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lighthouse-keeper in deployment lighthouse-keeper (namespace: jx) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lighthouse-keeper-deploy.yaml:37-99
────────────────────────────────────────
  37 ┌       - name: lighthouse-keeper
  38 │         image: ghcr.io/jenkins-x/lighthouse-keeper:1.14.4
  39 │         imagePullPolicy: IfNotPresent
  40 │         args:
  41 │         - "--namespace=jx"
  42 │         ports:
  43 │         - name: http
  44 │           containerPort: 8888
  45 └           protocol: TCP
  ..   
────────────────────────────────────────



lighthouse-keeper-svc.yaml (kubernetes)
=======================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lighthouse-keeper-svc.yaml:12-19
────────────────────────────────────────
  12 ┌   type: ClusterIP
  13 │   selector:
  14 │     app: lighthouse-keeper
  15 │   ports:
  16 │   - port: 80
  17 │     targetPort: 8888
  18 │     protocol: TCP
  19 └     name: http
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lighthouse-keeper-svc.yaml:12-19
────────────────────────────────────────
  12 ┌   type: ClusterIP
  13 │   selector:
  14 │     app: lighthouse-keeper
  15 │   ports:
  16 │   - port: 80
  17 │     targetPort: 8888
  18 │     protocol: TCP
  19 └     name: http
────────────────────────────────────────



lighthouse-pvc.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lighthouse-pvc.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │     - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lighthouse-pvc.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │     - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────



lighthouse-pvc1.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lighthouse-pvc1.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │     - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lighthouse-pvc1.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │     - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────



lighthouse-tekton-controller-svc.yaml (kubernetes)
==================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lighthouse-tekton-controller-svc.yaml:13-20
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - port: 8080
  16 │     targetPort: metrics
  17 │     protocol: TCP
  18 │     name: metrics
  19 │   selector:
  20 └     app: lighthouse-tekton-controller
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lighthouse-tekton-controller-svc.yaml:13-20
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - port: 8080
  16 │     targetPort: metrics
  17 │     protocol: TCP
  18 │     name: metrics
  19 │   selector:
  20 └     app: lighthouse-tekton-controller
────────────────────────────────────────



lighthouse-webhooks-role.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'lighthouse-webhooks' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 lighthouse-webhooks-role.yaml:12-23
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ""
  14 │   resources:
  15 │   - namespaces
  16 │   - configmaps
  17 │   - secrets
  18 │   verbs:
  19 │   - get
  20 └   - update
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'lighthouse-webhooks' shouldn't have access to manage secrets in namespace 'jx'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 lighthouse-webhooks-role.yaml:12-23
────────────────────────────────────────
  12 ┌ - apiGroups:
  13 │   - ""
  14 │   resources:
  15 │   - namespaces
  16 │   - configmaps
  17 │   - secrets
  18 │   verbs:
  19 │   - get
  20 └   - update
  ..   
────────────────────────────────────────



lights.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lights-k8s' of 'deployment' 'lights-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lights.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lights-deployment
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lights-deployment
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lights.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lights-deployment
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lights-deployment
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lights-k8s" of deployment "lights-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lights-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lights.yaml:4
────────────────────────────────────────
   4 [   name: lights-deployment
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lights-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lights-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lights.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lights-k8s in deployment lights-deployment (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lights.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────



lights1.yaml (kubernetes)
=========================
Tests: 117 (SUCCESSES: 97, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 13, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'lights-k8s' of 'deployment' 'lights-deployment' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'lights-k8s' of Deployment 'lights-deployment' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lights1.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lights-deployment
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lights-deployment
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lights1.yaml:6-19
────────────────────────────────────────
   6 ┌   replicas: 1
   7 │   selector:
   8 │     matchLabels:
   9 │       app: lights-deployment
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: lights-deployment
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "lights-k8s" of deployment "lights-deployment" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment lights-deployment in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 lights1.yaml:4
────────────────────────────────────────
   4 [   name: lights-deployment
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container lights-deployment in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment lights-deployment in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 lights1.yaml:15-19
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container lights-k8s in deployment lights-deployment (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 lights1.yaml:16-19
────────────────────────────────────────
  16 ┌       - name: lights-k8s
  17 │         image: paddymcbreen/lights-img:1.0
  18 │         ports:
  19 └         - containerPort: 3000
────────────────────────────────────────



lights1_1.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lights1_1.yaml:6-12
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: lights-deployment
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 3000
  11 │     targetPort: 3000
  12 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lights1_1.yaml:6-12
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: lights-deployment
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 3000
  11 │     targetPort: 3000
  12 └   type: ClusterIP
────────────────────────────────────────



lights_1.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lights_1.yaml:6-12
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: lights-deployment
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 3000
  11 │     targetPort: 3000
  12 └   type: ClusterIP
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lights_1.yaml:6-12
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: lights-deployment
   8 │   ports:
   9 │   - protocol: TCP
  10 │     port: 3000
  11 │     targetPort: 3000
  12 └   type: ClusterIP
────────────────────────────────────────



lightservice.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lightservice.yaml:6-13
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: light
   8 │   ports:
   9 │     - protocol: TCP
  10 │       port: 80
  11 │       targetPort: 80
  12 │       nodePort: 30009
  13 └   type: NodePort
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lightservice.yaml:6-13
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: light
   8 │   ports:
   9 │     - protocol: TCP
  10 │       port: 80
  11 │       targetPort: 80
  12 │       nodePort: 30009
  13 └   type: NodePort
────────────────────────────────────────



lightservice1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 lightservice1.yaml:6-13
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: light
   8 │   ports:
   9 │     - protocol: TCP
  10 │       port: 80
  11 │       targetPort: 80
  12 │       nodePort: 30009
  13 └   type: NodePort
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 lightservice1.yaml:6-13
────────────────────────────────────────
   6 ┌   selector:
   7 │     app: light
   8 │   ports:
   9 │     - protocol: TCP
  10 │       port: 80
  11 │       targetPort: 80
  12 │       nodePort: 30009
  13 └   type: NodePort
────────────────────────────────────────



limit-cpu.yaml (kubernetes)
===========================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cpu-demo-ctr' of 'pod' 'cpu-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-cpu.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-cpu.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cpu-demo-ctr" of pod "cpu-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod cpu-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-cpu.yaml:4
────────────────────────────────────────
   4 [   name: cpu-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cpu-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod cpu-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cpu-demo-ctr in pod cpu-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-cpu.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────



limit-cpu1.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cpu-demo-ctr' of 'pod' 'cpu-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-cpu1.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-cpu1.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cpu-demo-ctr" of pod "cpu-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod cpu-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-cpu1.yaml:4
────────────────────────────────────────
   4 [   name: cpu-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cpu-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod cpu-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu1.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cpu-demo-ctr in pod cpu-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-cpu1.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "1"
  12 │       requests:
  13 │         cpu: "0.5"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────



limit-cpu2.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cpu-demo-ctr' of 'pod' 'cpu-demo2' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-cpu2.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-cpu2.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cpu-demo-ctr" of pod "cpu-demo2" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod cpu-demo2 in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-cpu2.yaml:4
────────────────────────────────────────
   4 [   name: cpu-demo2
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cpu-demo2 in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod cpu-demo2 in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu2.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cpu-demo-ctr in pod cpu-demo2 (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-cpu2.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────



limit-cpu21.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cpu-demo-ctr' of 'pod' 'cpu-demo2' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cpu-demo-ctr' of Pod 'cpu-demo2' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-cpu21.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-cpu21.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cpu-demo-ctr" of pod "cpu-demo2" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod cpu-demo2 in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-cpu21.yaml:4
────────────────────────────────────────
   4 [   name: cpu-demo2
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container cpu-demo2 in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod cpu-demo2 in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-cpu21.yaml:6-16
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 └     args:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cpu-demo-ctr in pod cpu-demo2 (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-cpu21.yaml:7-16
────────────────────────────────────────
   7 ┌   - name: cpu-demo-ctr
   8 │     image: vish/stress
   9 │     resources:
  10 │       limits:
  11 │         cpu: "100"
  12 │       requests:
  13 │         cpu: "100"
  14 │     args:
  15 │     - -cpus
  16 └     - "2"
────────────────────────────────────────



limit-object.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-object.yaml:7-10
────────────────────────────────────────
   7 ┌   hard:
   8 │     pods: "2"
   9 │     services: "5"
  10 └     configmaps: "10"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-object.yaml:8-10
────────────────────────────────────────
   8 ┌     pods: "2"
   9 │     services: "5"
  10 └     configmaps: "10"
────────────────────────────────────────



limit-only.yaml (kubernetes)
============================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 19, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'larger-container' of 'pod' 'limit-only-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'limited-container' of 'pod' 'limit-only-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'larger-container' of Pod 'limit-only-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'limited-container' of Pod 'limit-only-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-only.yaml:6-22
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-only.yaml:6-22
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "larger-container" of pod "limit-only-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "limited-container" of pod "limit-only-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod limit-only-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-only.yaml:4
────────────────────────────────────────
   4 [   name: limit-only-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limit-only-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-only.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limit-only-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-only.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limit-only-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-only.yaml:6-22
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
  ..   
────────────────────────────────────────



limit-only1.yaml (kubernetes)
=============================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 19, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'larger-container' of 'pod' 'limit-only-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'limited-container' of 'pod' 'limit-only-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'larger-container' of Pod 'limit-only-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'limited-container' of Pod 'limit-only-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'larger-container' of Pod 'limit-only-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'limited-container' of Pod 'limit-only-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-only1.yaml:6-22
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-only1.yaml:6-22
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "larger-container" of pod "limit-only-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "limited-container" of pod "limit-only-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod limit-only-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-only1.yaml:4
────────────────────────────────────────
   4 [   name: limit-only-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limit-only-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-only1.yaml:15-22
────────────────────────────────────────
  15 ┌     - name: larger-container
  16 │       image: nginx
  17 │       ports:
  18 │         - containerPort: 8081
  19 │       resources:
  20 │         limits:
  21 │           cpu: 2
  22 └           memory: "4Gi"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limit-only-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-only1.yaml:7-14
────────────────────────────────────────
   7 ┌     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limit-only-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-only1.yaml:6-22
────────────────────────────────────────
   6 ┌   containers:
   7 │     - name: limited-container
   8 │       image: nginx
   9 │       ports:
  10 │         - containerPort: 8080
  11 │       resources:
  12 │         limits:
  13 │           cpu: 1
  14 └           memory: "245Mi"
  ..   
────────────────────────────────────────



limit-ram.yaml (kubernetes)
===========================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram1.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram1.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram1.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram1.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram1.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram1.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram2.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram2.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram2.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram2.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram2.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram2.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram21.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram21.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram21.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram21.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram21.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram21.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "200Mi"
  12 │       requests:
  13 │         memory: "100Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram22.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram22.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram22.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram22.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram22.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram22.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram3.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram3.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram3.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram3.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram3.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram3.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram31.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram31.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram31.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram31.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram31.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram31.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       limits:
  11 │         memory: "1000Gi"
  12 │       requests:
  13 │         memory: "1000Gi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"]
────────────────────────────────────────



limit-ram4.yaml (kubernetes)
============================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'memory-demo-ctr' of 'pod' 'memory-demo' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'memory-demo-ctr' of Pod 'memory-demo' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'memory-demo-ctr' of Pod 'memory-demo' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-ram4.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ram4.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "memory-demo-ctr" of pod "memory-demo" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod memory-demo in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit-ram4.yaml:4
────────────────────────────────────────
   4 [   name: memory-demo
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container memory-demo in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod memory-demo in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit-ram4.yaml:6-15
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container memory-demo-ctr in pod memory-demo (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limit-ram4.yaml:7-15
────────────────────────────────────────
   7 ┌   - name: memory-demo-ctr
   8 │     image: polinux/stress
   9 │     resources:
  10 │       requests:
  11 │         memory: "100Mi"
  12 │       limits:
  13 │         memory: "200Mi"
  14 │     command: ["stress"]
  15 └     args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
────────────────────────────────────────



limit-range-1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range-1.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       memory: 256Mi
  10 │       cpu: 0.5
  11 │     defaultRequest:
  12 │       memory: 100Mi
  13 │       cpu: 0.2
  14 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-1.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       memory: 256Mi
  10 │       cpu: 0.5
  11 │     defaultRequest:
  12 │       memory: 100Mi
  13 │       cpu: 0.2
  14 └     type: Container
────────────────────────────────────────



limit-range-2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range-2.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - max:
   9 │       memory: 512Mi
  10 │       cpu: 0.8
  11 │     min:
  12 │       memory: 256Mi
  13 │       cpu: 0.3
  14 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-2.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - max:
   9 │       memory: 512Mi
  10 │       cpu: 0.8
  11 │     min:
  12 │       memory: 256Mi
  13 │       cpu: 0.3
  14 └     type: Container
────────────────────────────────────────



limit-range-cpu.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │       defaultRequest:
  10 │         cpu: 500m
  11 │       max:
  12 │         cpu: "1"
  13 │       min:
  14 │         cpu: 100m
  15 └       type: Container
────────────────────────────────────────



limit-range-cpu1.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu1.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │       defaultRequest:
  10 │         cpu: 500m
  11 │       max:
  12 │         cpu: "1"
  13 │       min:
  14 │         cpu: 100m
  15 └       type: Container
────────────────────────────────────────



limit-range-cpu2.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu2.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │       defaultRequest:
  10 │         cpu: 500m
  11 │       max:
  12 │         cpu: "1"
  13 │       min:
  14 │         cpu: 100m
  15 └       type: Container
────────────────────────────────────────



limit-range-cpu3.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu3.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │       defaultRequest:
  10 │         cpu: 500m
  11 │       max:
  12 │         cpu: "1"
  13 │       min:
  14 │         cpu: 100m
  15 └       type: Container
────────────────────────────────────────



limit-range-cpu4.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu4.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │       defaultRequest:
  10 │         cpu: 500m
  11 │       max: 
  12 │         cpu: "1"
  13 │       min:
  14 │         cpu: 100m
  15 └       type: Container
────────────────────────────────────────



limit-range-cpu5.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu5.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │         memory: 1Gi
  10 │       defaultRequest:
  11 │         cpu: 500m
  12 │         memory: 1Gi
  13 │       max: # Maximum limit for the resource type in this case its "Container"
  14 └         cpu: "1"
  ..   
────────────────────────────────────────



limit-range-cpu6.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-cpu6.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 500m
   9 │     defaultRequest:
  10 │       cpu: 500m 
  11 │     max:
  12 │       cpu: "1"
  13 │     min:
  14 │       cpu: 100m
  15 └     type: Container
────────────────────────────────────────



limit-range-definiation.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-definiation.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: containers
   8 │       default:
   9 │         cpu: 500m # equals to 0.5
  10 │         memory: 1Gi
  11 │       defaultRequest:
  12 │         cpu: 500m
  13 │         memory: 1Gi
  14 └       max:
  ..   
────────────────────────────────────────



limit-range-definiation1.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-definiation1.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: containers
   8 │       default:
   9 │         cpu: 500m # equals to 0.5
  10 │         memory: 1Gi
  11 │       defaultRequest:
  12 │         cpu: 500m
  13 │         memory: 1Gi
  14 └       max:
  ..   
────────────────────────────────────────



limit-range-definition.yaml (kubernetes)
========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-definition.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 500m
   9 │     defaultRequest:
  10 │       cpu: 500m
  11 │     max:
  12 │       cpu: "1"
  13 │     min:
  14 │       cpu: 100m
  15 └     type: Container
────────────────────────────────────────



limit-range-definition1.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-definition1.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         cpu: 500m
   9 │         memory: 1Gi
  10 │       defaultRequest:
  11 │         cpu: 500m
  12 │         memory: 1Gi
  13 │       max:
  14 └         cpu: "1"
  ..   
────────────────────────────────────────



limit-range-demo.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-demo.yaml:6-34
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 50m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 6Gi
  14 └     - type: Container
  ..   
────────────────────────────────────────



limit-range-demo1.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-demo1.yaml:6-34
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 50m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 6Gi
  14 └     - type: Container
  ..   
────────────────────────────────────────



limit-range-demo2.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-demo2.yaml:6-36
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 50m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 6Gi
  14 └ 
  ..   
────────────────────────────────────────



limit-range-demo3.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-demo3.yaml:6-36
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 50m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 6Gi
  14 └ 
  ..   
────────────────────────────────────────



limit-range-memory.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-memory.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         memory: 1Gi
   9 │       defaultRequest:
  10 │         memory: 1Gi
  11 │       max:
  12 │         memory: 1Gi
  13 │       min:
  14 │         memory: 500Mi
  15 └       type: Container
────────────────────────────────────────



limit-range-memory1.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-memory1.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         memory: 1Gi
   9 │       defaultRequest:
  10 │         memory: 1Gi
  11 │       max:
  12 │         memory: 1Gi
  13 │       min:
  14 │         memory: 500Mi
  15 └       type: Container
────────────────────────────────────────



limit-range-memory2.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-memory2.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         memory: 1Gi
   9 │       defaultRequest:
  10 │         memory: 1Gi
  11 │       max:
  12 │         memory: 1Gi
  13 │       min:
  14 │         memory: 500Mi
  15 └       type: Container
────────────────────────────────────────



limit-range-memory3.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-memory3.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - default:
   8 │         memory: 1Gi
   9 │       defaultRequest:
  10 │         memory: 1Gi
  11 │       max: 
  12 │         memory: 1Gi
  13 │       min:
  14 │         memory: 1Gi
  15 └       type: Container
────────────────────────────────────────



limit-range-memory4.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-memory4.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Container
   8 │       default:
   9 │         cpu: "1Gi"
  10 │       defaultRequest:
  11 │         cpu: "1Gi"
  12 │       max:
  13 │         cpu: "1Gi"
  14 │       min:
  15 └         cpu: "500Mi"
────────────────────────────────────────



limit-range-memory5.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range-memory5.yaml:6-15
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 1Gi
   9 │     defaultRequest:
  10 │       cpu: 1Gi 
  11 │     max:
  12 │       cpu: 1Gi
  13 │     min:
  14 │       cpu: 500mi
  15 └     type: Container
────────────────────────────────────────



limit-range.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range1.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range1.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range1.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range10.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range10.yaml:7-10
────────────────────────────────────────
   7 ┌   limits:
   8 │     - type: "Container"
   9 │       defaultRequest:
  10 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range10.yaml:7-10
────────────────────────────────────────
   7 ┌   limits:
   8 │     - type: "Container"
   9 │       defaultRequest:
  10 └         cpu: "100m"
────────────────────────────────────────



limit-range11.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range11.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range11.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range12.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range12.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range12.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range13.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range13.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range13.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range15.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range15.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range15.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range16.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range16.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range16.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range17.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range17.yaml:7-18
────────────────────────────────────────
   7 ┌     limits:
   8 │         -   type: Pod
   9 │             max:
  10 │                 cpu: "1"
  11 │                 memory: 1Gi
  12 │         -   type: Container
  13 │             default:
  14 │                 cpu: "500m"
  15 └                 memory: "512Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range17.yaml:7-18
────────────────────────────────────────
   7 ┌     limits:
   8 │         -   type: Pod
   9 │             max:
  10 │                 cpu: "1"
  11 │                 memory: 1Gi
  12 │         -   type: Container
  13 │             default:
  14 │                 cpu: "500m"
  15 └                 memory: "512Mi"
  ..   
────────────────────────────────────────



limit-range18.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range18.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range18.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range19.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range19.yaml:7-44
────────────────────────────────────────
   7 ┌   limits:
   8 │   - type: Pod
   9 │     max:
  10 │       cpu: "2"
  11 │       memory: "4Gi"
  12 │     min:
  13 │       cpu: "200m"
  14 │       memory: "256Mi"
  15 └     maxLimitRequestRatio:
  ..   
────────────────────────────────────────



limit-range2.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range2.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range2.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range20.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range20.yaml:6-34
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 50m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 6Gi
  14 └     - type: Container
  ..   
────────────────────────────────────────



limit-range21.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range21.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range21.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range22.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range22.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range22.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range23.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range23.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range23.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range3.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range3.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range3.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range4.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range4.yaml:7-10
────────────────────────────────────────
   7 ┌   limits:
   8 │     - type: "Container"
   9 │       defaultRequest:
  10 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range4.yaml:7-10
────────────────────────────────────────
   7 ┌   limits:
   8 │     - type: "Container"
   9 │       defaultRequest:
  10 └         cpu: "100m"
────────────────────────────────────────



limit-range5.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range5.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 300m
  10 │       memory: 512Mi
  11 │     defaultRequest:
  12 │       cpu: 100m
  13 │       memory: 256Mi
  14 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range5.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 300m
  10 │       memory: 512Mi
  11 │     defaultRequest:
  12 │       cpu: 100m
  13 │       memory: 256Mi
  14 └     type: Container
────────────────────────────────────────



limit-range6.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range6.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range6.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range7.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range7.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range7.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-range8.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-range8.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-range8.yaml:9-12
────────────────────────────────────────
   9 ┌   limits:
  10 │     - type: "Container"
  11 │       defaultRequest:
  12 └         cpu: "100m"
────────────────────────────────────────



limit-ranges-default-min-max.yaml (kubernetes)
==============================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ranges-default-min-max.yaml:7-20
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 100m
  10 │       memory: 200Mi
  11 │     defaultRequest:
  12 │       cpu: 40m
  13 │       memory: 100Mi
  14 │     max:
  15 └       cpu: "200m"
  ..   
────────────────────────────────────────



limit-ranges-default-min-max1.yaml (kubernetes)
===============================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ranges-default-min-max1.yaml:7-20
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 100m
  10 │       memory: 200Mi
  11 │     defaultRequest:
  12 │       cpu: 40m
  13 │       memory: 100Mi
  14 │     max:
  15 └       cpu: "200m"
  ..   
────────────────────────────────────────



limit-ranges-default-min-max2.yaml (kubernetes)
===============================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ranges-default-min-max2.yaml:7-20
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 100m
  10 │       memory: 200Mi
  11 │     defaultRequest:
  12 │       cpu: 40m
  13 │       memory: 100Mi
  14 │     max:
  15 └       cpu: "200m"
  ..   
────────────────────────────────────────



limit-ranges.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ranges.yaml:7-20
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 111m
  10 │       memory: 99Mi
  11 │     defaultRequest:
  12 │       cpu: 101m
  13 │       memory: 91Mi
  14 │     max:
  15 └       cpu: 200m
  ..   
────────────────────────────────────────



limit-ranges1.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ranges1.yaml:7-20
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 111m
  10 │       memory: 99Mi
  11 │     defaultRequest:
  12 │       cpu: 101m
  13 │       memory: 91Mi
  14 │     max:
  15 └       cpu: 200m
  ..   
────────────────────────────────────────



limit-ranges2.yaml (kubernetes)
===============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit-ranges2.yaml:7-20
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 111m
  10 │       memory: 99Mi
  11 │     defaultRequest:
  12 │       cpu: 101m
  13 │       memory: 91Mi
  14 │     max:
  15 └       cpu: 200m
  ..   
────────────────────────────────────────



limit-resource.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit-resource.yaml:7-11
────────────────────────────────────────
   7 ┌   hard:
   8 │     requests.cpu: "1"
   9 │     requests.memory: 1Gi
  10 │     limits.cpu: "2"
  11 └     limits.memory: 2Gi
────────────────────────────────────────



limit.yaml (kubernetes)
=======================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'httpd' of Deployment 'limit-test' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'httpd' of Deployment 'limit-test' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'httpd' of 'deployment' 'limit-test' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'httpd' of Deployment 'limit-test' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'httpd' of Deployment 'limit-test' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'httpd' of Deployment 'limit-test' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'httpd' of Deployment 'limit-test' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'httpd' of Deployment 'limit-test' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'httpd' of Deployment 'limit-test' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'httpd' of Deployment 'limit-test' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit.yaml:7-25
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       name: limit-test
  11 │   template:
  12 │     metadata:
  13 │       name: test
  14 │       labels:
  15 └         name: limit-test
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit.yaml:7-25
────────────────────────────────────────
   7 ┌   replicas: 1
   8 │   selector:
   9 │     matchLabels:
  10 │       name: limit-test
  11 │   template:
  12 │     metadata:
  13 │       name: test
  14 │       labels:
  15 └         name: limit-test
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "httpd" of deployment "limit-test" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment limit-test in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit.yaml:5
────────────────────────────────────────
   5 [   name: limit-test
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment limit-test in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limit-test in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit.yaml:18-25
────────────────────────────────────────
  18 ┌         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment limit-test in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit.yaml:17-25
────────────────────────────────────────
  17 ┌       containers:
  18 │         - name: httpd
  19 │           image: httpd
  20 │           ports:
  21 │             - containerPort: 80
  22 │           resources:
  23 │             requests:
  24 │               cpu: 1000Mi
  25 └               memory: 1Gi
────────────────────────────────────────



limit1.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 101, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 9, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'ubuntu' of Pod 'giropops' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'ubuntu' of Pod 'giropops' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'ubuntu' of 'pod' 'giropops' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'ubuntu' of Pod 'giropops' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'ubuntu' of Pod 'giropops' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'ubuntu' of Pod 'giropops' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'ubuntu' of Pod 'giropops' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'ubuntu' of Pod 'giropops' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limit1.yaml:8-22
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limit1.yaml:8-22
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "ubuntu" of pod "giropops" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod giropops in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limit1.yaml:4-6
────────────────────────────────────────
   4 ┌   labels:
   5 │     run: giropops
   6 └   name: giropops
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container giropops in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit1.yaml:9-20
────────────────────────────────────────
   9 ┌   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 │         cpu: "0.5"
  17 └         memory: "128Mi"  
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod giropops in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limit1.yaml:8-22
────────────────────────────────────────
   8 ┌   containers:
   9 │   - image: ubuntu
  10 │     name: ubuntu
  11 │     args:
  12 │     - sleep
  13 │     - "1800" 
  14 │     resources:
  15 │       limits:
  16 └         cpu: "0.5"
  ..   
────────────────────────────────────────



limitRange10.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitRange10.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 100Mi
   9 │       cpu: 100m
  10 │     defaultRequest:
  11 │       memory: 100Mi
  12 │       cpu: 100m
  13 │     max:
  14 └       memory: 1Gi
  ..   
────────────────────────────────────────



limitRange11.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitRange11.yaml:6-31
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       max:
   9 │         cpu: "2"
  10 │         memory: 1Gi
  11 │       min:
  12 │         cpu: 200m
  13 │         memory: 6Mi
  14 └     - type: Container
  ..   
────────────────────────────────────────



limitRange13.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitRange13.yaml:6-29
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 50m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 5Gi
  14 └     - type: Container
  ..   
────────────────────────────────────────



limitRange5.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitRange5.yaml:6-34
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Pod
   8 │       min:
   9 │         cpu: 500m
  10 │         memory: 5Mi
  11 │       max:
  12 │         cpu: "2"
  13 │         memory: 2Gi
  14 └     - type: PersistentVolumeClaim
  ..   
────────────────────────────────────────



limitando_recurcso.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitando_recurcso.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 1
   9 │       memory: 256Mi
  10 │     defaultRequest:
  11 │       cpu: 0.5
  12 │       memory: 128Mi
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitando_recurcso.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 1
   9 │       memory: 256Mi
  10 │     defaultRequest:
  11 │       cpu: 0.5
  12 │       memory: 128Mi
  13 └     type: Container
────────────────────────────────────────



limitando_recurcso1.yaml (kubernetes)
=====================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitando_recurcso1.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 1
   9 │       memory: 256Mi
  10 │     defaultRequest:
  11 │       cpu: 0.5
  12 │       memory: 128Mi
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitando_recurcso1.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 1
   9 │       memory: 256Mi
  10 │     defaultRequest:
  11 │       cpu: 0.5
  12 │       memory: 128Mi
  13 └     type: Container
────────────────────────────────────────



limite de cpu.yaml (kubernetes)
===============================
Tests: 117 (SUCCESSES: 101, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 9, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'nginx' of Deployment 'nginx-d' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'nginx' of Deployment 'nginx-d' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'nginx' of 'deployment' 'nginx-d' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'nginx' of Deployment 'nginx-d' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'nginx' of Deployment 'nginx-d' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'nginx' of Deployment 'nginx-d' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'nginx' of Deployment 'nginx-d' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limite de cpu.yaml:8-28
────────────────────────────────────────
   8 ┌   selector:   #permite seleccionar un conjunto de objetos que cumplan las condicione
   9 │     matchLabels:
  10 │       app: nginx
  11 │   replicas: 2 # indica al controlador que ejecute 2 pods
  12 │   template:   # Plantilla que define los containers
  13 │     metadata:
  14 │       labels:
  15 │         app: nginx
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limite de cpu.yaml:8-28
────────────────────────────────────────
   8 ┌   selector:   #permite seleccionar un conjunto de objetos que cumplan las condicione
   9 │     matchLabels:
  10 │       app: nginx
  11 │   replicas: 2 # indica al controlador que ejecute 2 pods
  12 │   template:   # Plantilla que define los containers
  13 │     metadata:
  14 │       labels:
  15 │         app: nginx
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "nginx" of deployment "nginx-d" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment nginx-d in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limite de cpu.yaml:4-6
────────────────────────────────────────
   4 ┌   name: nginx-d
   5 │   labels:
   6 └     estado: "1"
────────────────────────────────────────


AVD-KSV-0117 (MEDIUM): deployment nginx-d in default namespace should not set spec.template.spec.containers.ports.containerPort to less than 1024
════════════════════════════════════════
The ports which are lower than 1024 receive and transmit various sensitive and privileged data. Allowing containers to use them can bring serious implications.

See https://avd.aquasec.com/misconfig/ksv117
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-d in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limite de cpu.yaml:18-28
────────────────────────────────────────
  18 ┌       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 │               cpu: "2"
  26 └           requests:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment nginx-d in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limite de cpu.yaml:17-28
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: nginx
  19 │         image: nginx:1.25.3
  20 │         ports:
  21 │         - containerPort: 80
  22 │         resources:
  23 │           limits:
  24 │               memory: "200Mi"
  25 └               cpu: "2"
  ..   
────────────────────────────────────────



limite-cpu.yaml (kubernetes)
============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limite-cpu.yaml:7-10
────────────────────────────────────────
   7 ┌   limits:
   8 │   - defaultRequest:
   9 │       cpu: 2
  10 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limite-cpu.yaml:7-10
────────────────────────────────────────
   7 ┌   limits:
   8 │   - defaultRequest:
   9 │       cpu: 2
  10 └     type: Container
────────────────────────────────────────



limite.yaml (kubernetes)
========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limite.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory  : 1Gi
   9 │       cpu: 1
  10 │     defaultRequest:
  11 │        memory: "512Mi"
  12 │        cpu: 0.5
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limite.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory  : 1Gi
   9 │       cpu: 1
  10 │     defaultRequest:
  11 │        memory: "512Mi"
  12 │        cpu: 0.5
  13 └     type: Container
────────────────────────────────────────



limited-memory-pod_1.yaml (kubernetes)
======================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'test-container' of Pod 'test-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'test-container' of Pod 'test-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'test-container' of 'pod' 'test-pod' in 'oomkill-demo' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'test-container' of Pod 'test-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'test-container' of Pod 'test-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'test-container' of Pod 'test-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'test-container' of Pod 'test-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'test-container' of Pod 'test-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'test-container' of Pod 'test-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'test-container' of Pod 'test-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'test-container' of Pod 'test-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limited-memory-pod_1.yaml:7-15
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limited-memory-pod_1.yaml:7-15
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "test-container" of pod "test-pod" in "oomkill-demo" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container test-pod in oomkill-demo namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limited-memory-pod_1.yaml:8-15
────────────────────────────────────────
   8 ┌   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod test-pod in oomkill-demo namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limited-memory-pod_1.yaml:7-15
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: test-container
   9 │     image: busybox
  10 │     resources:
  11 │       limits:
  12 │         memory: 128Mi
  13 │     command:
  14 │     - sleep
  15 └     - inf
────────────────────────────────────────



limited-pod.yaml (kubernetes)
=============================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'main' of Pod 'limited-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'main' of Pod 'limited-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'main' of 'pod' 'limited-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'main' of Pod 'limited-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'main' of Pod 'limited-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'main' of Pod 'limited-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'main' of Pod 'limited-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'main' of Pod 'limited-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'main' of Pod 'limited-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'main' of Pod 'limited-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limited-pod.yaml:6-13
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limited-pod.yaml:6-13
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "main" of pod "limited-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod limited-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limited-pod.yaml:4
────────────────────────────────────────
   4 [   name: limited-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limited-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limited-pod.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limited-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limited-pod.yaml:6-13
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────



limited-pod1.yaml (kubernetes)
==============================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'main' of Pod 'limited-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'main' of Pod 'limited-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'main' of 'pod' 'limited-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'main' of Pod 'limited-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'main' of Pod 'limited-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'main' of Pod 'limited-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'main' of Pod 'limited-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'main' of Pod 'limited-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'main' of Pod 'limited-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'main' of Pod 'limited-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limited-pod1.yaml:6-13
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limited-pod1.yaml:6-13
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "main" of pod "limited-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod limited-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limited-pod1.yaml:4
────────────────────────────────────────
   4 [   name: limited-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limited-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limited-pod1.yaml:7-13
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limited-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limited-pod1.yaml:6-13
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     command: ["dd", "if=/dev/zero", "of=/dev/null"]
   9 │     name: main
  10 │     resources:
  11 │       limits:
  12 │         cpu: 1
  13 └         memory: 20Mi
────────────────────────────────────────



limited_namespaceaccess.yaml (kubernetes)
=========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): Role 'deployment-manager' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 limited_namespaceaccess.yaml:7-19
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   - extensions
  10 │   - apps
  11 │   resources:
  12 │   - pods
  13 │   verbs:
  14 │   - get
  15 └   - list
  ..   
────────────────────────────────────────



limited_namespaceaccess1.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): Role 'deployment-manager' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 limited_namespaceaccess1.yaml:7-19
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   - extensions
  10 │   - apps
  11 │   resources:
  12 │   - pods
  13 │   verbs:
  14 │   - get
  15 └   - list
  ..   
────────────────────────────────────────



limited_namespaceaccess2.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): Role 'deployment-manager' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 limited_namespaceaccess2.yaml:7-19
────────────────────────────────────────
   7 ┌ - apiGroups:
   8 │   - ''
   9 │   - extensions
  10 │   - apps
  11 │   resources:
  12 │   - pods
  13 │   verbs:
  14 │   - get
  15 └   - list
  ..   
────────────────────────────────────────



limitex-resource.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitex-resource.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 512Mi
   9 │       cpu: 1
  10 │     defaultRequest:
  11 │       memory: 256Mi
  12 │       cpu: 0.5
  13 │     max:
  14 └       memory: 1Gi
  ..   
────────────────────────────────────────



limitex.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitex.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 512Mi
   9 │       cpu: 1
  10 │     defaultRequest:
  11 │       memory: 256Mi
  12 │       cpu: 0.5
  13 │     max:
  14 └       memory: 1Gi
  ..   
────────────────────────────────────────



limitex1.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitex1.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 512Mi
   9 │       cpu: 1
  10 │     defaultRequest:
  11 │       memory: 256Mi
  12 │       cpu: 0.5
  13 │     max:
  14 └       memory: 512Mi
  ..   
────────────────────────────────────────



limitr-1-pod-1.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'container' of 'pod' 'limitr-1-pod-1' in 'ops-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-1' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitr-1-pod-1.yaml:7-9
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitr-1-pod-1.yaml:7-9
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "container" of pod "limitr-1-pod-1" in "ops-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limitr-1-pod-1 in ops-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limitr-1-pod-1.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limitr-1-pod-1 in ops-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limitr-1-pod-1.yaml:7-9
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container
   9 └     image: nginx
────────────────────────────────────────



limitr-1-pod-11.yaml (kubernetes)
=================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'container' of 'pod' 'limitr-1-pod-1' in 'ops-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-1' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'container' of Pod 'limitr-1-pod-1' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitr-1-pod-11.yaml:7-9
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitr-1-pod-11.yaml:7-9
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "container" of pod "limitr-1-pod-1" in "ops-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limitr-1-pod-1 in ops-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limitr-1-pod-11.yaml:8-9
────────────────────────────────────────
   8 ┌   - name: container 
   9 └     image: nginx
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limitr-1-pod-1 in ops-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limitr-1-pod-11.yaml:7-9
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container 
   9 └     image: nginx
────────────────────────────────────────



limitr-1-pod-2.yaml (kubernetes)
================================
Tests: 117 (SUCCESSES: 100, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 10, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-2' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'container' of Pod 'limitr-1-pod-2' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'container' of 'pod' 'limitr-1-pod-2' in 'ops-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'container' of Pod 'limitr-1-pod-2' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-2' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'container' of Pod 'limitr-1-pod-2' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'container' of Pod 'limitr-1-pod-2' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'container' of Pod 'limitr-1-pod-2' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'container' of Pod 'limitr-1-pod-2' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'container' of Pod 'limitr-1-pod-2' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitr-1-pod-2.yaml:7-13
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitr-1-pod-2.yaml:7-13
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "container" of pod "limitr-1-pod-2" in "ops-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limitr-1-pod-2 in ops-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limitr-1-pod-2.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limitr-1-pod-2 in ops-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limitr-1-pod-2.yaml:7-13
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: container
   9 │     image: nginx
  10 │     resources:
  11 │       requests:
  12 │         cpu: 1
  13 └         memory: "128Mi"
────────────────────────────────────────



limitr-1.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitr-1.yaml:7-18
────────────────────────────────────────
   7 ┌   limits:
   8 │   - type: Container
   9 │     max:
  10 │       cpu: "2"
  11 │     min:
  12 │       cpu: "200m"
  13 │     default:
  14 │       cpu: 1
  15 └       memory: 512Mi
  ..   
────────────────────────────────────────



limitr-11.yaml (kubernetes)
===========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitr-11.yaml:7-18
────────────────────────────────────────
   7 ┌   limits:
   8 │   - type: Container
   9 │     max:
  10 │       cpu: "2"
  11 │     min:
  12 │       cpu: "200m"
  13 │     default:
  14 │       cpu: 1
  15 └       memory: 512Mi
  ..   
────────────────────────────────────────



limitr-2.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitr-2.yaml:7-16
────────────────────────────────────────
   7 ┌   limits:
   8 │   - default:
   9 │       cpu: 500m
  10 │     defaultRequest:
  11 │       cpu: 500m
  12 │     max:
  13 │       cpu: "1"
  14 │     min:
  15 │       cpu: 100m
  16 └     type: Container
────────────────────────────────────────



limitrange-1.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange-1.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │     - type: Container
   8 │       default:
   9 │         cpu: 500m
  10 │         memory: 1Gi
  11 │       defaultRequest:
  12 │         cpu: 50m
  13 │         memory: 1Gi
  14 └       max:
  ..   
────────────────────────────────────────



limitrange-def.yaml (kubernetes)
================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitrange-def.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │     - default:
   9 │         cpu: 0.7
  10 │         memory: 500Mi
  11 │       defaultRequest:
  12 │         cpu: 0.3
  13 │         memory: 100Mi
  14 └       type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange-def.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │     - default:
   9 │         cpu: 0.7
  10 │         memory: 500Mi
  11 │       defaultRequest:
  12 │         cpu: 0.3
  13 │         memory: 100Mi
  14 └       type: Container
────────────────────────────────────────



limitrange.yaml (kubernetes)
============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 512Mi
   9 │       cpu: 500m
  10 │     defaultRequest:
  11 │       memory: 256Mi
  12 │       cpu: 100m
  13 │     max:
  14 └       memory: 700M
  ..   
────────────────────────────────────────



limitrange1.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange1.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 512Mi
   9 │       cpu: 500m
  10 │     defaultRequest:
  11 │       memory: 256Mi
  12 │       cpu: 100m
  13 │     max:
  14 └       memory: 700M
  ..   
────────────────────────────────────────



limitrange15.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitrange15.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - max: 
   9 │       cpu: "500m"
  10 │       memory: "512Mi"
  11 │     min:
  12 │       cpu: "200m"
  13 │       memory: "256Mi"
  14 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange15.yaml:7-14
────────────────────────────────────────
   7 ┌   limits:
   8 │   - max: 
   9 │       cpu: "500m"
  10 │       memory: "512Mi"
  11 │     min:
  12 │       cpu: "200m"
  13 │       memory: "256Mi"
  14 └     type: Container
────────────────────────────────────────



limitrange16.yaml (kubernetes)
==============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitrange16.yaml:6-11
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "500m"
   9 │     min:
  10 │       cpu: "200m"
  11 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange16.yaml:6-11
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "500m"
   9 │     min:
  10 │       cpu: "200m"
  11 └     type: Container
────────────────────────────────────────



limitrange2.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitrange2.yaml:6-11
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "500m"
   9 │     min:
  10 │       cpu: "200m"
  11 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange2.yaml:6-11
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "500m"
   9 │     min:
  10 │       cpu: "200m"
  11 └     type: Container
────────────────────────────────────────



limitrange3.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange3.yaml:6-19
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       memory: 512Mi
   9 │       cpu: 500m
  10 │     defaultRequest:
  11 │       memory: 256Mi
  12 │       cpu: 100m
  13 │     max:
  14 └       memory: 700M
  ..   
────────────────────────────────────────



limitrange7.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limitrange7.yaml:6-11
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "500m"
   9 │     min:
  10 │       cpu: "200m"
  11 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange7.yaml:6-11
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "500m"
   9 │     min:
  10 │       cpu: "200m"
  11 └     type: Container
────────────────────────────────────────



limitrange8.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange8.yaml:7-27
────────────────────────────────────────
   7 ┌   limits:
   8 │   - type: Pod
   9 │     max:
  10 │       cpu: "2"
  11 │       memory: "1Gi"
  12 │     min:
  13 │       cpu: "200m"
  14 │       memory: "100Mi"
  15 └   - type: Container
  ..   
────────────────────────────────────────



limitrange9.yaml (kubernetes)
=============================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limitrange9.yaml:7-27
────────────────────────────────────────
   7 ┌   limits:
   8 │   - type: Pod
   9 │     max:
  10 │       cpu: "2" # max CPU per Pod
  11 │       memory: 2Gi # max memory per Pod
  12 │     min:
  13 │       cpu: "200m" # min CPU per Pod
  14 │       memory: 200Mi # min memory per Pod
  15 └   - type: Container
  ..   
────────────────────────────────────────



limits-no-restart.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'web' of 'pod' 'nginx-limits-no-restart' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'web' of Pod 'nginx-limits-no-restart' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits-no-restart.yaml:6-14
────────────────────────────────────────
   6 ┌   restartPolicy: Never
   7 │   containers:
   8 │   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits-no-restart.yaml:6-14
────────────────────────────────────────
   6 ┌   restartPolicy: Never
   7 │   containers:
   8 │   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "web" of pod "nginx-limits-no-restart" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nginx-limits-no-restart in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limits-no-restart.yaml:4
────────────────────────────────────────
   4 [   name: nginx-limits-no-restart
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-limits-no-restart in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-no-restart.yaml:8-14
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nginx-limits-no-restart in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-no-restart.yaml:6-14
────────────────────────────────────────
   6 ┌   restartPolicy: Never
   7 │   containers:
   8 │   - name: web
   9 │     image: nginx
  10 │     env:
  11 │     resources:
  12 │       limits:
  13 │         memory: "128Mi"
  14 └         cpu: "500m"
────────────────────────────────────────



limits-no-restart1.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'web' of 'pod' 'nginx-limits-no-restart' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'web' of Pod 'nginx-limits-no-restart' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'web' of Pod 'nginx-limits-no-restart' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits-no-restart1.yaml:6-13
────────────────────────────────────────
   6 ┌   restartPolicy: Never
   7 │   containers:
   8 │   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits-no-restart1.yaml:6-13
────────────────────────────────────────
   6 ┌   restartPolicy: Never
   7 │   containers:
   8 │   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "web" of pod "nginx-limits-no-restart" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod nginx-limits-no-restart in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limits-no-restart1.yaml:4
────────────────────────────────────────
   4 [   name: nginx-limits-no-restart
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container nginx-limits-no-restart in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-no-restart1.yaml:8-13
────────────────────────────────────────
   8 ┌   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod nginx-limits-no-restart in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-no-restart1.yaml:6-13
────────────────────────────────────────
   6 ┌   restartPolicy: Never
   7 │   containers:
   8 │   - name: web
   9 │     image: nginx
  10 │     resources:
  11 │       limits:
  12 │         memory: "128Mi"
  13 └         cpu: "500m"
────────────────────────────────────────



limits-pod-too-big.yaml (kubernetes)
====================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'main' of Pod 'too-big' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'main' of Pod 'too-big' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'main' of 'pod' 'too-big' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'main' of Pod 'too-big' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'main' of Pod 'too-big' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'main' of Pod 'too-big' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'main' of Pod 'too-big' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'main' of Pod 'too-big' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'main' of Pod 'too-big' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'main' of Pod 'too-big' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'main' of Pod 'too-big' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits-pod-too-big.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits-pod-too-big.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "main" of pod "too-big" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod too-big in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limits-pod-too-big.yaml:4
────────────────────────────────────────
   4 [   name: too-big
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container too-big in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-pod-too-big.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod too-big in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-pod-too-big.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────



limits-pod-too-big1.yaml (kubernetes)
=====================================
Tests: 117 (SUCCESSES: 98, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'main' of Pod 'too-big' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'main' of Pod 'too-big' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'main' of 'pod' 'too-big' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'main' of Pod 'too-big' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'main' of Pod 'too-big' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'main' of Pod 'too-big' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'main' of Pod 'too-big' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'main' of Pod 'too-big' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'main' of Pod 'too-big' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'main' of Pod 'too-big' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'main' of Pod 'too-big' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits-pod-too-big1.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits-pod-too-big1.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "main" of pod "too-big" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod too-big in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limits-pod-too-big1.yaml:4
────────────────────────────────────────
   4 [   name: too-big
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container too-big in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-pod-too-big1.yaml:7-12
────────────────────────────────────────
   7 ┌   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod too-big in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits-pod-too-big1.yaml:6-12
────────────────────────────────────────
   6 ┌   containers:
   7 │   - image: busybox
   8 │     args: ["sleep", "9999999"]
   9 │     name: main
  10 │     resources:
  11 │       requests:
  12 └         cpu: 2
────────────────────────────────────────



limits.yaml (kubernetes)
========================
Tests: 117 (SUCCESSES: 99, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 11, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'limits-pod' of Pod 'limits-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'limits-pod' of Pod 'limits-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'limits-pod' of 'pod' 'limits-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'limits-pod' of Pod 'limits-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'limits-pod' of Pod 'limits-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'limits-pod' of Pod 'limits-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'limits-pod' of Pod 'limits-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'limits-pod' of Pod 'limits-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'limits-pod' of Pod 'limits-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits.yaml:7-13
────────────────────────────────────────
   7 ┌   containers:
   8 │     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits.yaml:7-13
────────────────────────────────────────
   7 ┌   containers:
   8 │     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "limits-pod" of pod "limits-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod limits-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limits.yaml:4
────────────────────────────────────────
   4 [   name: limits-pod
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container limits-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod limits-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits.yaml:7-13
────────────────────────────────────────
   7 ┌   containers:
   8 │     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container limits-pod in pod limits-pod (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limits.yaml:8-13
────────────────────────────────────────
   8 ┌     - name: limits-pod
   9 │       image: docker-registry:5000/nginx
  10 │       resources:
  11 │         requests:
  12 │           cpu: "50m"  # 50/1000th's of a vCPU core
  13 └           memory: "50Mi" # ~50mb
────────────────────────────────────────



limits1.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits1.yaml:6-34
────────────────────────────────────────
   6 ┌   limits:
   7 │   - type: Pod
   8 │     min:
   9 │       cpu: 50m
  10 │       memory: 5Mi
  11 │     max:
  12 │       cpu: 1
  13 │       memory: 1Gi
  14 └   - type: Container
  ..   
────────────────────────────────────────



limits10.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits10.yaml:6-26
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "2"
   9 │       memory: 1Gi
  10 │     min:
  11 │       cpu: 200m
  12 │       memory: 6Mi
  13 │     type: Pod
  14 └   - default:
  ..   
────────────────────────────────────────



limits11.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits11.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits11.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────



limits12.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits12.yaml:6-26
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "2"
   9 │       memory: 1Gi
  10 │     min:
  11 │       cpu: 200m
  12 │       memory: 6Mi
  13 │     type: Pod
  14 └   - default:
  ..   
────────────────────────────────────────



limits13.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits13.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits13.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────



limits14.yaml (kubernetes)
==========================
Tests: 129 (SUCCESSES: 101, FAILURES: 28)
Failures: 28 (UNKNOWN: 0, LOW: 15, MEDIUM: 8, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'app' of Pod 'frontend' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'log-aggregator' of Pod 'frontend' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'app' of Pod 'frontend' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'log-aggregator' of Pod 'frontend' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'app' of 'pod' 'frontend' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'log-aggregator' of 'pod' 'frontend' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'app' of Pod 'frontend' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'log-aggregator' of Pod 'frontend' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'app' of Pod 'frontend' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'log-aggregator' of Pod 'frontend' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'app' of Pod 'frontend' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'log-aggregator' of Pod 'frontend' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'app' of Pod 'frontend' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'log-aggregator' of Pod 'frontend' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits14.yaml:7-25
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits14.yaml:7-25
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "app" of pod "frontend" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "log-aggregator" of pod "frontend" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod frontend in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 limits14.yaml:5
────────────────────────────────────────
   5 [   name: frontend
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container frontend in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container frontend in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod frontend in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 limits14.yaml:7-25
────────────────────────────────────────
   7 ┌   containers:
   8 │   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 └         memory: "128Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container app in pod frontend (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limits14.yaml:8-16
────────────────────────────────────────
   8 ┌   - name: app
   9 │     image: images.my-company.example/app:v4
  10 │     resources:
  11 │       requests:
  12 │         memory: "64Mi"
  13 │         cpu: "250m"
  14 │       limits:
  15 │         memory: "128Mi"
  16 └         cpu: "500m"
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container log-aggregator in pod frontend (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 limits14.yaml:17-25
────────────────────────────────────────
  17 ┌   - name: log-aggregator
  18 │     image: images.my-company.example/log-aggregator:v6
  19 │     resources:
  20 │       requests:
  21 │         memory: "64Mi"
  22 │         cpu: "250m"
  23 │       limits:
  24 │         memory: "128Mi"
  25 └         cpu: "500m"
────────────────────────────────────────



limits16.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits16.yaml:6-26
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "2"
   9 │       memory: 1Gi
  10 │     min:
  11 │       cpu: 200m
  12 │       memory: 6Mi
  13 │     type: Pod
  14 └   - default:
  ..   
────────────────────────────────────────



limits17.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits17.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits17.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────



limits18.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits18.yaml:6-26
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "2"
   9 │       memory: 1Gi
  10 │     min:
  11 │       cpu: 200m
  12 │       memory: 6Mi
  13 │     type: Pod
  14 └   - default:
  ..   
────────────────────────────────────────



limits19.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 2, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0039 (LOW): A LimitRange policy with a default requests and limits for each container should be configured
════════════════════════════════════════
Ensure that a LimitRange policy is configured to limit resource usage for namespaces or nodes

See https://avd.aquasec.com/misconfig/ksv039
────────────────────────────────────────
 limits19.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits19.yaml:6-13
────────────────────────────────────────
   6 ┌   limits:
   7 │   - default:
   8 │       cpu: 200m
   9 │       memory: 512Mi
  10 │     defaultRequest:
  11 │       cpu: 100m
  12 │       memory: 256Mi
  13 └     type: Container
────────────────────────────────────────



limits2.yaml (kubernetes)
=========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits2.yaml:6-34
────────────────────────────────────────
   6 ┌   limits:
   7 │   - type: Pod
   8 │     min:
   9 │       cpu: 60m
  10 │       memory: 5Mi
  11 │     max:
  12 │       cpu: 1
  13 │       memory: 1Gi
  14 └   - type: Container
  ..   
────────────────────────────────────────



limits20.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 115, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 limits20.yaml:6-26
────────────────────────────────────────
   6 ┌   limits:
   7 │   - max:
   8 │       cpu: "2"
   9 │       memory: 1Gi
  10 │     min:
  11 │       cpu: 200m
  12 │       memory: 6Mi
  13 │     type: Pod
  14 └   - default:
  ..   
────────────────────────────────────────


