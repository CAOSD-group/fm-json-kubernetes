
Report Summary

┌─────────────────────────────────────────────────────────┬────────────┬───────────────────┐
│                         Target                          │    Type    │ Misconfigurations │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_MODULE_LOAD.yaml                                     │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_NET_MITM.yaml                                        │ kubernetes │        17         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_NSENTER.yaml                                         │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_NSENTER_1.yaml                                       │ kubernetes │        21         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_PRIV_MOUNT.yaml                                      │ kubernetes │        19         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_SYS_PTRACE.yaml                                      │ kubernetes │        22         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_UMH_CORE_PATTERN.yaml                                │ kubernetes │        21         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_VAR_LOG_SYMLINK.yaml                                 │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_VAR_LOG_SYMLINK_1.yaml                               │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_VAR_LOG_SYMLINK_2.yaml                               │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CE_VAR_LOG_SYMLINK_3.yaml                               │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CertificateSigningRequest-def.yaml                      │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CertificateSigningRequest.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CertificateSigningRequest1.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ CertificateSigningRequest2.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_21.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_22.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_23.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_24.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_25.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_26.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_27.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_28.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_29.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_30.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_31.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_32.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_33.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_34.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_35.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_36.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_37.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_38.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_39.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_40.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_41.yaml                                  │ kubernetes │        14         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_42.yaml                                  │ kubernetes │        14         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_43.yaml                                  │ kubernetes │        14         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_44.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_45.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_7.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_8.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager78_9.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_10.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_11.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_12.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_13.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_14.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_15.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_16.yaml                                  │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_17.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_18.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_19.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_20.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_21.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_22.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_23.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_24.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_25.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_26.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_27.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_28.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_29.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_30.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_31.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_32.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_33.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_34.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_35.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_36.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_37.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_38.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_39.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_40.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_41.yaml                                  │ kubernetes │         9         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_42.yaml                                  │ kubernetes │         9         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_43.yaml                                  │ kubernetes │         9         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_44.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_45.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_7.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_8.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager80_9.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_10.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_11.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_12.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_13.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_14.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_15.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_16.yaml                                  │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_17.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_18.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_19.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_20.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_21.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_22.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_23.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_24.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_25.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_26.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_27.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_28.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_29.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_30.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_31.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_32.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_33.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_34.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_35.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_36.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_37.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_38.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_39.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_40.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_41.yaml                                  │ kubernetes │         9         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_42.yaml                                  │ kubernetes │         9         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_43.yaml                                  │ kubernetes │         9         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_44.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_45.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_7.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_8.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager81_9.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_10.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_11.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_12.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_13.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_14.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_15.yaml                                  │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_16.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_17.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_18.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_19.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_20.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_21.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_22.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_23.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_24.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_25.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_26.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_27.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_28.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_29.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_30.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_31.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_32.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_33.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_34.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_35.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_36.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_37.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_38.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_39.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_40.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_41.yaml                                  │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_42.yaml                                  │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_43.yaml                                  │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_44.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_45.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_7.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_8.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager82_9.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_10.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_11.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_12.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_13.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_14.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_15.yaml                                  │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_16.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_17.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_18.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_19.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_20.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_21.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_22.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_23.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_24.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_25.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_26.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_27.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_28.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_29.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_30.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_31.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_32.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_33.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_34.yaml                                  │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_35.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_36.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_37.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_38.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_39.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_40.yaml                                  │ kubernetes │        15         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_41.yaml                                  │ kubernetes │        15         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_42.yaml                                  │ kubernetes │        15         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_43.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_44.yaml                                  │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_6.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_7.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_8.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager86_9.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager91.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager99.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_10.yaml                                   │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_11.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_12.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_13.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_14.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_15.yaml                                   │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_16.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_17.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_18.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_19.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_20.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_21.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_22.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_23.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_24.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_25.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_26.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_27.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_28.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_29.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_30.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_31.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_32.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_33.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_34.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_35.yaml                                   │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_36.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_37.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_38.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_39.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_40.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_41.yaml                                   │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_42.yaml                                   │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_43.yaml                                   │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_44.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_45.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_7.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_8.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-manager9_9.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-secret.yaml                                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-signing-request.yaml                               │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-signing-request1.yaml                              │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-temp-rolebinding.yaml                              │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-temp-rolebinding1.yaml                             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-temp-rolebinding2.yaml                             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-temp-rolebinding3.yaml                             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-temp-rolebinding4.yaml                             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-temp-rolebinding5.yaml                             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert-tls.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert3_1.yaml                                            │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_job.yaml                               │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_job1.yaml                              │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_role.yaml                              │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_role1.yaml                             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_role1_1.yaml                           │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_role1_2.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_role_1.yaml                            │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cert_provisioner_role_2.yaml                            │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certbot2.yaml                                           │ kubernetes │        19         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certbot2_1.yaml                                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certbot2_2.yaml                                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certbot2_3.yaml                                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-key-permission.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-key-permission1.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-sigining-request.yaml                       │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-signing-request.template.yaml               │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-signing-request.template1.yaml              │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-signing-request.yaml                        │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-updater-ds.yaml                             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-updater-ds_1.yaml                           │ kubernetes │        18         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-updater-ns.yaml                             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-updater-sa.yaml                             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-updater-sa_1.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate-updater-sa_2.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate10.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate100.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate101.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate102.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate103.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate104.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate105.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate107.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate108.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate109.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate11.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate110.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate111.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate112.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate113.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate114.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate115.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate116.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate117.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate118.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate119.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate12.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate120.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate121.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate122.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate123.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate125.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate126.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate127.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate128.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate129.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate13.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate130.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate131.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate132.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate133.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate134.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate135.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate136.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate137.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate138.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate139_1.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate14.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate140.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate141.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate15.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate16.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate17.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172.yaml                                     │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_10.yaml                                  │ kubernetes │         7         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_2.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_5.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_6.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_7.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_8.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate172_9.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate18.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate19.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate20.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate21.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate22.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate22_1.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate23.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate24.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate25.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate26.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate27.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate28.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate29.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate30.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate31.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate32.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate34.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate35.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate36.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate37.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate38.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate39.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate40.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate41.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate42.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate43.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate44.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate45.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate46.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate47.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate48.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate49.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate5.yaml                                       │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate50.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate52.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate53.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate54.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate55.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate56.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate57.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate58.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate59.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate6.yaml                                       │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate60.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate61.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate62.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate63.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate64.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate65.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate66_1.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate67.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate68.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate7.yaml                                       │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate78.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate79.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate8.yaml                                       │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate80.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate81.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate82.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate83.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate84.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate85.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate86.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate87.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate88.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate89.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate9.yaml                                       │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate90.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate91.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate92.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate93.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate94.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate95.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate95_1.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate96.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate97.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate98.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificate99.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources.yaml                │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_10.yaml             │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_11.yaml             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_12.yaml             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_13.yaml             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_14.yaml             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_15.yaml             │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_16.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_17.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_18.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_19.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_20.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_21.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_22.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_23.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_24.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_25.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_26.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_27.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_28.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_29.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_30.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_31.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_32.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_33.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_34.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_35.yaml             │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_36.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_37.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_38.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_39.yaml             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_40.yaml             │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_41.yaml             │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_42.yaml             │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_43.yaml             │ kubernetes │         8         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_44.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_45.yaml             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_7.yaml              │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_8.yaml              │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates-cert-manager-resources_9.yaml              │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest1.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest10.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest11.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest12.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest13.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest14.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest15.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest16.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest17.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest18.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest19.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest2.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest20.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest21.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest22.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest23.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest24.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest25.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest26.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest27.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest28.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest29.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest3.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest33.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest34.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest35.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest36.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest37.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest38.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest39.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest4.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest40.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest41.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest42.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest43.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest44.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest45.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest46.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest47.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest48.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest49.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest5.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest50.yaml │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest6.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest7.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest8.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1.CertificateSigningRequest9.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle.yaml    │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle1.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle10.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle11.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle12.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle13.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle14.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle15.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle16.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle17.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle18.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle19.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle2.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle20.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle21.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle22.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle23.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle24.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle25.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle26.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle27.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle28.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle29.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle3.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle30.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle31.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle32.yaml  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle4.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle5.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle6.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle7.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle8.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certificates.k8s.io.v1alpha1.ClusterTrustBundle9.yaml   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certman-ingress-letsenc.yaml                            │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17.yaml                                      │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_10.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_11.yaml                                   │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_12.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_13.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_14.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_15.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_16.yaml                                   │ kubernetes │         5         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_17.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_18.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_19.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_20.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_21.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_22.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_23.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_24.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_25.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_26.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_27.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_28.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_29.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_30.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_31.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_32.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_33.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_34.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_35.yaml                                   │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_36.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_37.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_38.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_39.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_40.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_41.yaml                                   │ kubernetes │        14         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_42.yaml                                   │ kubernetes │        14         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_43.yaml                                   │ kubernetes │        14         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_44.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_45.yaml                                   │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_7.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_8.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certmanager17_9.yaml                                    │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-persistentvolumeclaim.yaml                        │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv-claim.yaml                                     │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv-claim1.yaml                                    │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv-volume.yaml                                    │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv-volume1.yaml                                   │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv.yaml                                           │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv_1.yaml                                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs-pv_2.yaml                                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs.yaml                                              │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs1.yaml                                             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7.yaml                                             │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7_1.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7_2.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7_3.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7_4.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7_5.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ certs7_6.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cf-mongo.yaml                                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cf.yaml                                                 │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cfg.yaml                                                │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cflor-clusterrolebinding.yaml                           │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cflor-ps-clusterrolebinding.yaml                        │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ cgroup-test.yaml                                        │ kubernetes │        22         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-config-server.yaml                           │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-config-server_1.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-core.yaml                                    │ kubernetes │        16         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-core_1.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-core_2.yaml                                  │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-demand.yaml                                  │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-demand_1.yaml                                │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-notifications.yaml                           │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-notifications_1.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-production-ml.yaml                           │ kubernetes │        19         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-production-ml_1.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-storage.yaml                                 │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-storage_1.yaml                               │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-supply.yaml                                  │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-supply_1.yaml                                │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-vault.yaml                                   │ kubernetes │        20         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainoptim-vault_1.yaml                                 │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-1.yaml                           │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-11.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-110.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-111.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-112.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-113.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-114.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-115.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-116.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-117.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-118.yaml                         │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-119.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-12.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-120.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-121.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-122.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-123.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-124.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-125.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-126.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-127.yaml                         │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-128.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-129.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-13.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-130.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-131.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-132.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-133.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-134.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-135.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-136.yaml                         │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-137.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-138.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-139.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-14.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-140.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-141.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-142.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-143.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-144.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-15.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-16.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-17.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-18.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-00-apply-19.yaml                          │ kubernetes │         2         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-1.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-11.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-110.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-111.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-112.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-113.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-114.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-115.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-116.yaml                         │ kubernetes │        32         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-117.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-118.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-119.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-12.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-120.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-121.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-122.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-123.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-124.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-125.yaml                         │ kubernetes │        32         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-126.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-127.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-128.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-129.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-13.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-130.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-131.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-132.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-133.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-134.yaml                         │ kubernetes │        32         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-135.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-136.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-137.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-138.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-139.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-14.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-140.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-141.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-142.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-143.yaml                         │ kubernetes │        32         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-144.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-15.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-16.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-17.yaml                          │ kubernetes │        32         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-18.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-19.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-2.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-210.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-211.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-212.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-214.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-215.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-216.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-218.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-219.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-22.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-23.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-24.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-26.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-27.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-28.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-3.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-31.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-310.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-311.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-312.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-313.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-314.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-315.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-316.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-317.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-318.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-319.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-32.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-33.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-34.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-35.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-36.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-37.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-38.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-39.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-4.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-41.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-410.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-411.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-412.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-413.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-414.yaml                         │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-42.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-43.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-44.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-45.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-46.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-47.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-48.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-49.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-511.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-514.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-52.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-55.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-58.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-6.yaml                           │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-61.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-62.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-63.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-64.yaml                          │ kubernetes │         1         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-7.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-71.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-72.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-73.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-01-apply-74.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1.yaml                           │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-11.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-110.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1100.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1101.yaml                        │ kubernetes │        18         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1102.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1103.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1104.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1105.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1106.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1108.yaml                        │ kubernetes │        17         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1109.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-111.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1110.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1111.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1112.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1113.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-1114.yaml                        │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-112.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-113.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-114.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-116.yaml                         │ kubernetes │        17         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-117.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-118.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-119.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-12.yaml                          │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-120.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-121.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-122.yaml                         │ kubernetes │         0         │
├─────────────────────────────────────────────────────────┼────────────┼───────────────────┤
│ chainsaw-step-02-apply-123.yaml                         │ kubernetes │         0         │
└─────────────────────────────────────────────────────────┴────────────┴───────────────────┘
Legend:
- '-': Not scanned
- '0': Clean (no security findings detected)


CE_MODULE_LOAD.yaml (kubernetes)
================================
Tests: 115 (SUCCESSES: 95, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kh-calibration-mod-load-pod' of 'pod' 'kh-calibration-mod-load' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'kh-calibration-mod-load-pod' of Pod 'kh-calibration-mod-load' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:9-17
────────────────────────────────────────
   9 ┌   containers:
  10 │     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kh-calibration-mod-load-pod" of pod "kh-calibration-mod-load" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod kh-calibration-mod-load in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:5-7
────────────────────────────────────────
   5 ┌   name: kh-calibration-mod-load
   6 │   labels:
   7 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod kh-calibration-mod-load in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:9-17
────────────────────────────────────────
   9 ┌   containers:
  10 │     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────


AVD-KSV-0120 (HIGH): container {"__defsec_metadata": {"endline": 17, "filepath": "CE_MODULE_LOAD.yaml", "offset": 0, "startline": 10}, "args": ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"], "command": ["/bin/bash", "-c", "--"], "image": "centos:latest", "name": "kh-calibration-mod-load-pod", "securityContext": {"__defsec_metadata": {"endline": 15, "filepath": "CE_MODULE_LOAD.yaml", "offset": 0, "startline": 14}, "capabilities": {"__defsec_metadata": {"endline": 15, "filepath": "CE_MODULE_LOAD.yaml", "offset": 0, "startline": 15}, "add": ["SYS_MODULE"]}}} of pod kh-calibration-mod-load in default namespace should not include 'SYS_MODULE' in securityContext.capabilities.add
════════════════════════════════════════
The SYS_MODULE capability grants attackers the ability to install and remove kernel modules, posing serious security risks.

See https://avd.aquasec.com/misconfig/ksv120
────────────────────────────────────────
 CE_MODULE_LOAD.yaml:10-17
────────────────────────────────────────
  10 ┌     - name: kh-calibration-mod-load-pod
  11 │       image: centos:latest
  12 │       securityContext:
  13 │         #privileged: true # TODO: implenent an OR, since you either need priv OR caps
  14 │         capabilities:
  15 │           add: ["SYS_MODULE"]
  16 │       command: [ "/bin/bash", "-c" , "--"]
  17 └       args: ["lsmod | awk 'NR==2{print $1}' | xargs -I {} modprobe {} & sleep 3000"] 
────────────────────────────────────────



CE_NET_MITM.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 98, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 8, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'netadmin-pod' of Pod 'netadmin-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'netadmin-pod' of Pod 'netadmin-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'netadmin-pod' of 'pod' 'netadmin-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0009 (HIGH): Pod 'netadmin-pod' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 CE_NET_MITM.yaml:9-26
────────────────────────────────────────
   9 ┌   hostNetwork: true
  10 │   containers:
  11 │     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 └       resources:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'netadmin-pod' of Pod 'netadmin-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'netadmin-pod' of Pod 'netadmin-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'netadmin-pod' of Pod 'netadmin-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'netadmin-pod' of Pod 'netadmin-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'netadmin-pod' of Pod 'netadmin-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'netadmin-pod' of Pod 'netadmin-pod' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_NET_MITM.yaml:9-26
────────────────────────────────────────
   9 ┌   hostNetwork: true
  10 │   containers:
  11 │     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 └       resources:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "netadmin-pod" of pod "netadmin-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod netadmin-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_NET_MITM.yaml:5-7
────────────────────────────────────────
   5 ┌   name: netadmin-pod
   6 │   labels:
   7 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod netadmin-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_NET_MITM.yaml:9-26
────────────────────────────────────────
   9 ┌   hostNetwork: true
  10 │   containers:
  11 │     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 └       resources:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container netadmin-pod in pod netadmin-pod (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 CE_NET_MITM.yaml:11-26
────────────────────────────────────────
  11 ┌     - name: netadmin-pod
  12 │       #image: entlein/kh-verify:0.0.1
  13 │       image: ghcr.io/k8sstormcenter/lightening-kh-verify:latest
  14 │       securityContext:
  15 │         capabilities:
  16 │           add: ["NET_ADMIN"]
  17 │       resources:
  18 │         limits:
  19 └           memory: "256Mi"
  ..   
────────────────────────────────────────



CE_NSENTER.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 95, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kh-calibration-ce-1-pod' of 'pod' 'kh-calibration-ce-1' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0010 (HIGH): Pod 'kh-calibration-ce-1' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 CE_NSENTER.yaml:8-18
────────────────────────────────────────
   8 ┌   hostPID: true
   9 │   containers:
  10 │   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 └     - -c
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kh-calibration-ce-1-pod' of Pod 'kh-calibration-ce-1' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_NSENTER.yaml:8-18
────────────────────────────────────────
   8 ┌   hostPID: true
   9 │   containers:
  10 │   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 └     - -c
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kh-calibration-ce-1-pod" of pod "kh-calibration-ce-1" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_NSENTER.yaml:10-18
────────────────────────────────────────
  10 ┌   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 │     - -c
  17 │     args:
  18 └     - nsenter -t 1 -a /bin/bash && sleep infinity;
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod kh-calibration-ce-1 in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_NSENTER.yaml:4-6
────────────────────────────────────────
   4 ┌   name: kh-calibration-ce-1
   5 │   labels:
   6 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod kh-calibration-ce-1 in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_NSENTER.yaml:8-18
────────────────────────────────────────
   8 ┌   hostPID: true
   9 │   containers:
  10 │   - name: kh-calibration-ce-1-pod
  11 │     image: ubuntu
  12 │     securityContext:
  13 │       privileged: true
  14 │     command:
  15 │     - /bin/sh
  16 └     - -c
  ..   
────────────────────────────────────────



CE_NSENTER_1.yaml (kubernetes)
==============================
Tests: 115 (SUCCESSES: 94, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 12, MEDIUM: 6, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kh-calibration-ce-11-pod' of 'pod' 'kh-calibration-ce-11' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0005 (HIGH): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should not include 'SYS_ADMIN' in 'securityContext.capabilities.add'
════════════════════════════════════════
SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root.

See https://avd.aquasec.com/misconfig/ksv005
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'kh-calibration-ce-11-pod' of Pod 'kh-calibration-ce-11' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_NSENTER_1.yaml:8-14
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kh-calibration-ce-11-pod" of pod "kh-calibration-ce-11" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod kh-calibration-ce-11 in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_NSENTER_1.yaml:4-6
────────────────────────────────────────
   4 ┌   name: kh-calibration-ce-11
   5 │   labels:
   6 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod kh-calibration-ce-11 in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_NSENTER_1.yaml:8-14
────────────────────────────────────────
   8 ┌   containers:
   9 │   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kh-calibration-ce-11-pod in pod kh-calibration-ce-11 (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 CE_NSENTER_1.yaml:9-14
────────────────────────────────────────
   9 ┌   - name: kh-calibration-ce-11-pod
  10 │     image: ghcr.io/k8sstormcenter/lightening-ce-nsenter:latest
  11 │     securityContext:
  12 │       capabilities:
  13 │         add:
  14 └         - SYS_ADMIN
────────────────────────────────────────



CE_PRIV_MOUNT.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 96, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'priv-mount-pod' of 'pod' 'priv-mount-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'priv-mount-pod' of Pod 'priv-mount-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:9-15
────────────────────────────────────────
   9 ┌   containers:
  10 │     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "priv-mount-pod" of pod "priv-mount-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:10-15
────────────────────────────────────────
  10 ┌     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod priv-mount-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:5-7
────────────────────────────────────────
   5 ┌   name: priv-mount-pod
   6 │   labels:
   7 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod priv-mount-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_PRIV_MOUNT.yaml:9-15
────────────────────────────────────────
   9 ┌   containers:
  10 │     - name: priv-mount-pod
  11 │       image: ubuntu
  12 │       securityContext:
  13 │         privileged: true
  14 │       command: [ "/bin/sh", "-c" ]
  15 └       args: [ "mount -t proc proc /proc && sleep infinity" ]
────────────────────────────────────────



CE_SYS_PTRACE.yaml (kubernetes)
===============================
Tests: 115 (SUCCESSES: 93, FAILURES: 22)
Failures: 22 (UNKNOWN: 0, LOW: 12, MEDIUM: 6, HIGH: 4, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'kh-calibration-ptrace' of 'pod' 'kh-calibration-ptrace' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0005 (HIGH): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should not include 'SYS_ADMIN' in 'securityContext.capabilities.add'
════════════════════════════════════════
SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root.

See https://avd.aquasec.com/misconfig/ksv005
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0010 (HIGH): Pod 'kh-calibration-ptrace' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:10-20
────────────────────────────────────────
  10 ┌   hostPID: true
  11 │   containers:
  12 │     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 └           add:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0022 (MEDIUM): Container 'kh-calibration-ptrace' of Pod 'kh-calibration-ptrace' should not set 'securityContext.capabilities.add'
════════════════════════════════════════
According to pod security standard 'Capabilities', capabilities beyond the default set must not be added.

See https://avd.aquasec.com/misconfig/ksv022
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:10-20
────────────────────────────────────────
  10 ┌   hostPID: true
  11 │   containers:
  12 │     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 └           add:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "kh-calibration-ptrace" of pod "kh-calibration-ptrace" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod kh-calibration-ptrace in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:6-8
────────────────────────────────────────
   6 ┌   name: kh-calibration-ptrace
   7 │   labels:
   8 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod kh-calibration-ptrace in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:10-20
────────────────────────────────────────
  10 ┌   hostPID: true
  11 │   containers:
  12 │     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 └           add:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container kh-calibration-ptrace in pod kh-calibration-ptrace (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 CE_SYS_PTRACE.yaml:12-20
────────────────────────────────────────
  12 ┌     - name: kh-calibration-ptrace
  13 │       image: andyneff/hello-world-gdb
  14 │       command: [ "/bin/sh", "-c" ]
  15 │       args: [ "gdb && sleep infinity" ]
  16 │       securityContext:
  17 │         capabilities:
  18 │           add:
  19 │           - SYS_PTRACE
  20 └           - SYS_ADMIN
────────────────────────────────────────



CE_UMH_CORE_PATTERN.yaml (kubernetes)
=====================================
Tests: 115 (SUCCESSES: 94, FAILURES: 21)
Failures: 21 (UNKNOWN: 0, LOW: 13, MEDIUM: 6, HIGH: 2, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'umh-core-container' of 'pod' 'umh-core-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'umh-core-container' of Pod 'umh-core-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'umh-core-container' of Pod 'umh-core-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Pod 'umh-core-pod' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:12-26
────────────────────────────────────────
  12 ┌   containers:
  13 │     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 └       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:12-26
────────────────────────────────────────
  12 ┌   containers:
  13 │     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 └       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "umh-core-container" of pod "umh-core-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0105 (LOW): securityContext.runAsUser should be set to a value greater than 0
════════════════════════════════════════
Containers should be forbidden from running with a root UID.

See https://avd.aquasec.com/misconfig/ksv105
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:22
────────────────────────────────────────
  22 [         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod umh-core-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:8-10
────────────────────────────────────────
   8 ┌   name: umh-core-pod
   9 │   labels:
  10 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod umh-core-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:12-26
────────────────────────────────────────
  12 ┌   containers:
  13 │     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 └       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container umh-core-container in pod umh-core-pod (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 CE_UMH_CORE_PATTERN.yaml:13-22
────────────────────────────────────────
  13 ┌     - name: umh-core-container
  14 │       image: ghcr.io/k8sstormcenter/lightening-ce-umh-core-pattern:latest
  15 │       imagePullPolicy: Always
  16 │       volumeMounts:
  17 │       - mountPath: /sysproc
  18 │         name: nodeproc
  19 │       #command: [ "/bin/sh", "-c", "--" ]
  20 │       #args: [ "echo '|/bin/sh' > /sysproc/core_pattern && while true; do sleep 300; done;" ]
  21 │       securityContext:
  22 └         runAsUser: 0
────────────────────────────────────────



CE_VAR_LOG_SYMLINK_3.yaml (kubernetes)
======================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'varlog-container' of Pod 'varlog-pod' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'varlog-container' of 'pod' 'varlog-pod' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'varlog-container' of Pod 'varlog-pod' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'varlog-container' of Pod 'varlog-pod' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'varlog-container' of Pod 'varlog-pod' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'varlog-container' of Pod 'varlog-pod' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): Pod 'varlog-pod' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:9-25
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 └     - -c
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:9-25
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 └     - -c
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "varlog-container" of pod "varlog-pod" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod varlog-pod in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:4-7
────────────────────────────────────────
   4 ┌   name: varlog-pod
   5 │   namespace: default
   6 │   labels:
   7 └     app: kubehound-edge-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container varlog-pod in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:10-20
────────────────────────────────────────
  10 ┌   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 │     - -c
  18 └     - --
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod varlog-pod in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 CE_VAR_LOG_SYMLINK_3.yaml:9-25
────────────────────────────────────────
   9 ┌   containers:
  10 │   - name: varlog-container
  11 │     image: ubuntu
  12 │     volumeMounts:
  13 │     - mountPath: /host/var/log
  14 │       name: nodelog
  15 │     command:
  16 │     - /bin/sh
  17 └     - -c
  ..   
────────────────────────────────────────



CertificateSigningRequest-def.yaml (kubernetes)
===============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CertificateSigningRequest-def.yaml:6-9
────────────────────────────────────────
   6 ┌   request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2F6Q0NBVk1DQVFBd0pqRU9NQXdHQTFVRUF3d0ZjbUZ0YVhReEZEQVNCZ05WQkFvTUMyUmxkbVZzYjNCdApaVzUwTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF3dDcvQno3cjBpMmFwaWJRCnoxVmhQbVFqYU9zRlBrODRHRjUrZS9rV0xyNjdnLzZENEhIUHdSLzVXNE9nNGg4WDB4NXk3eTRhTVBDT2ZoVFYKcU80cEdoc1poaldTM2tqb2Z1VXFYdDNnRHoycjZvcVBSYnUzQVBVbXV0bCtlcWtJRXNmd1pBck5OaFc2bmJjWgpibnczSlNoWXdyZnVZY2t3blR2cVdrVkhmdHRGWldiazRwWHZ6V1I4UEo1OWc5NnhtL3NQNkluU2paanpsSlpvCkJHWHlPTG9RMU85MGNHMEszZmluTlBTbnlXQzhXMGRhdXdmNmV0U1BZS0tpcDl1WTh4VW83TitjMDFDVHR1UjYKQWRRRTJrOU1TSWI0RlVNLzhYaUl4T1ltWW55NUpiU1VncEkvNGNLUXJOelpCQVUrWURQaGRycFRtTDJ5UmE3RgpsdlB1OHdJREFRQUJvQUF3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQURnZ055MG5TSjNmbDd3T1RCUkVLL2cyClQ0dEpBWEJtQU9Bd21oeThLYVMrZVVmUVh3SmdtMFJDRDlYVSt1N2JtbjFXQmg5S0NsZVNXS0JSbDRSbmt4MVMKQVJrMVRKRHFjK2wxdXhnUnJvQ0JwOWNBcmpoNTRUU0FYSDlXT0lWbTk0bmF5VUtWSmFYNDdMSUozZmpQeUR1RQpqeklPL1pTVWJwTHAwQVp3MnRySGpnUWpranhmY1YxZUs5SmZSS005RGFrWVJlWGs4a29GbTR4ay9pVmRwclNZCmtzSDZlKzNKMTlNMnNYL1B6WDR0YnFnY2NCVXVWdUpDSDRJTEVtMU5tN0NSSDFHWFRkNzAweTYyaElYQ2JTOCsKZThGUzRJdmJ0bStVckF3MlQxU1VFTkhaSUpxYlJ5SjNJeHd4UTJtaG9XUk82TDM2enU4SFArVUZGVkM5ZGg0PQotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
   7 │   signerName: kubernetes.io/kube-apiserver-client
   8 │   usages:
   9 └     - client auth
────────────────────────────────────────



CertificateSigningRequest.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CertificateSigningRequest.yaml:7-10
────────────────────────────────────────
   7 ┌   request: <base 64 encoded csr - run 'cat xyz.csr | base64 -w0' to encode and paste the output here>
   8 │   usages:
   9 │     - client auth
  10 └   signerName: <name_of_signer>
────────────────────────────────────────



CertificateSigningRequest1.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CertificateSigningRequest1.yaml:7-10
────────────────────────────────────────
   7 ┌   request: <base 64 encoded csr - run 'cat xyz.csr | base64 -w0' to encode and paste the output here>
   8 │   usages:
   9 │     - client auth
  10 └   signerName: <name_of_signer>
────────────────────────────────────────



CertificateSigningRequest2.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 CertificateSigningRequest2.yaml:6-10
────────────────────────────────────────
   6 ┌   request: <base64_encoded_csr>
   7 │   signerName: kubernetes.io/kube-apiserver-client
   8 │   expirationSeconds: 86400  # one day
   9 │   usages:
  10 └   - client auth
────────────────────────────────────────



cert-manager78_35.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager78_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager78_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager78_39.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager78_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



cert-manager78_40.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager78_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



cert-manager78_41.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager-cainjector' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager78_41.yaml:13-46
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager-cainjector" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager78_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────



cert-manager78_42.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager78_42.yaml:13-55
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager78_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.8.2
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────



cert-manager78_43.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager-webhook' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager78_43.yaml:13-73
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager-webhook" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager78_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.8.2
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────



cert-manager80_11.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_11.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cert-manager80_11.yaml:37-46
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 │   - watch
  46 └   - update
────────────────────────────────────────



cert-manager80_12.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_12.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager80_13.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_13.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager80_14.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_14.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 │   - watch
  58 └   - create
  ..   
────────────────────────────────────────



cert-manager80_15.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_15.yaml:51-58
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - secrets
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



cert-manager80_16.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_16.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resources:
  40 │   - secrets
  41 │   verbs:
  42 │   - get
  43 │   - list
  44 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager80_16.yaml:97-104
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - ''
  99 │   resources:
 100 │   - secrets
 101 │   verbs:
 102 │   - get
 103 │   - list
 104 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cert-manager80_16.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager80_16.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager80_16.yaml:63-73
────────────────────────────────────────
  63 ┌ - apiGroups:
  64 │   - networking.k8s.io
  65 │   resources:
  66 │   - ingresses
  67 │   verbs:
  68 │   - get
  69 │   - list
  70 │   - watch
  71 └   - create
  ..   
────────────────────────────────────────



cert-manager80_35.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager80_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager80_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager80_39.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager80_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



cert-manager80_40.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager80_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



cert-manager80_41.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 106, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager80_41.yaml:13-51
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-cainjector in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager80_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────



cert-manager80_42.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 106, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager80_42.yaml:13-61
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-controller in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager80_42.yaml:38-59
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.10.1
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --feature-gates=ExperimentalGatewayAPISupport=true
  46 └         ports:
  ..   
────────────────────────────────────────



cert-manager80_43.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 106, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager80_43.yaml:13-83
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-webhook in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager80_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.10.1
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────



cert-manager81_11.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_11.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cert-manager81_11.yaml:37-46
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 │   - watch
  46 └   - update
────────────────────────────────────────



cert-manager81_12.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_12.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager81_13.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_13.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager81_14.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_14.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 │   - watch
  58 └   - create
  ..   
────────────────────────────────────────



cert-manager81_15.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_15.yaml:51-58
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - secrets
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



cert-manager81_16.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_16.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resources:
  40 │   - secrets
  41 │   verbs:
  42 │   - get
  43 │   - list
  44 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager81_16.yaml:97-104
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - ''
  99 │   resources:
 100 │   - secrets
 101 │   verbs:
 102 │   - get
 103 │   - list
 104 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cert-manager81_16.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager81_16.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager81_16.yaml:63-73
────────────────────────────────────────
  63 ┌ - apiGroups:
  64 │   - networking.k8s.io
  65 │   resources:
  66 │   - ingresses
  67 │   verbs:
  68 │   - get
  69 │   - list
  70 │   - watch
  71 └   - create
  ..   
────────────────────────────────────────



cert-manager81_35.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager81_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager81_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager81_39.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager81_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



cert-manager81_40.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager81_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



cert-manager81_41.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 106, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager81_41.yaml:13-51
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-cainjector in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager81_41.yaml:34-49
────────────────────────────────────────
  34 ┌       - name: cert-manager-cainjector
  35 │         image: quay.io/jetstack/cert-manager-cainjector:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --leader-election-namespace=kube-system
  40 │         env:
  41 │         - name: POD_NAMESPACE
  42 └           valueFrom:
  ..   
────────────────────────────────────────



cert-manager81_42.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 106, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager81_42.yaml:13-62
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-controller in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager81_42.yaml:38-60
────────────────────────────────────────
  38 ┌       - name: cert-manager-controller
  39 │         image: quay.io/jetstack/cert-manager-controller:v1.11.0
  40 │         imagePullPolicy: IfNotPresent
  41 │         args:
  42 │         - --v=2
  43 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  44 │         - --leader-election-namespace=kube-system
  45 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.11.0
  46 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────



cert-manager81_43.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 106, FAILURES: 9)
Failures: 9 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager81_43.yaml:13-83
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-webhook in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager81_43.yaml:34-81
────────────────────────────────────────
  34 ┌       - name: cert-manager-webhook
  35 │         image: quay.io/jetstack/cert-manager-webhook:v1.11.0
  36 │         imagePullPolicy: IfNotPresent
  37 │         args:
  38 │         - --v=2
  39 │         - --secure-port=10250
  40 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  41 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  42 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────



cert-manager82_10.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_10.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cert-manager82_10.yaml:37-47
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 └   - watch
  ..   
────────────────────────────────────────



cert-manager82_11.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_11.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager82_12.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_12.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager82_13.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_13.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 │   - watch
  58 └   - create
  ..   
────────────────────────────────────────



cert-manager82_14.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_14.yaml:51-58
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - secrets
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



cert-manager82_15.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_15.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resources:
  40 │   - secrets
  41 │   verbs:
  42 │   - get
  43 │   - list
  44 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager82_15.yaml:97-104
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - ''
  99 │   resources:
 100 │   - secrets
 101 │   verbs:
 102 │   - get
 103 │   - list
 104 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cert-manager82_15.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager82_15.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager82_15.yaml:63-73
────────────────────────────────────────
  63 ┌ - apiGroups:
  64 │   - networking.k8s.io
  65 │   resources:
  66 │   - ingresses
  67 │   verbs:
  68 │   - get
  69 │   - list
  70 │   - watch
  71 └   - create
  ..   
────────────────────────────────────────



cert-manager82_35.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager82_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager82_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager82_39.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager82_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



cert-manager82_40.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager82_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



cert-manager82_41.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager82_41.yaml:13-53
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-cainjector in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager82_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────



cert-manager82_42.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager82_42.yaml:13-77
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-controller in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager82_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────



cert-manager82_43.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager82_43.yaml:13-85
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-webhook in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager82_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────



cert-manager86_10.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_10.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cert-manager86_10.yaml:37-46
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 │   - watch
  46 └   - update
────────────────────────────────────────



cert-manager86_11.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_11.yaml:27-37
────────────────────────────────────────
  27 ┌ - apiGroups:
  28 │   - ''
  29 │   resources:
  30 │   - secrets
  31 │   verbs:
  32 │   - get
  33 │   - list
  34 │   - watch
  35 └   - create
  ..   
────────────────────────────────────────



cert-manager86_12.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_12.yaml:27-37
────────────────────────────────────────
  27 ┌ - apiGroups:
  28 │   - ''
  29 │   resources:
  30 │   - secrets
  31 │   verbs:
  32 │   - get
  33 │   - list
  34 │   - watch
  35 └   - create
  ..   
────────────────────────────────────────



cert-manager86_13.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_13.yaml:49-59
────────────────────────────────────────
  49 ┌ - apiGroups:
  50 │   - ''
  51 │   resources:
  52 │   - secrets
  53 │   verbs:
  54 │   - get
  55 │   - list
  56 │   - watch
  57 └   - create
  ..   
────────────────────────────────────────



cert-manager86_14.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_14.yaml:50-57
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 └   - watch
────────────────────────────────────────



cert-manager86_15.yaml (kubernetes)
===================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_15.yaml:36-43
────────────────────────────────────────
  36 ┌ - apiGroups:
  37 │   - ''
  38 │   resources:
  39 │   - secrets
  40 │   verbs:
  41 │   - get
  42 │   - list
  43 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager86_15.yaml:96-103
────────────────────────────────────────
  96 ┌ - apiGroups:
  97 │   - ''
  98 │   resources:
  99 │   - secrets
 100 │   verbs:
 101 │   - get
 102 │   - list
 103 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cert-manager86_15.yaml:51-61
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - pods
  55 │   - services
  56 │   verbs:
  57 │   - get
  58 │   - list
  59 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager86_15.yaml:51-61
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - pods
  55 │   - services
  56 │   verbs:
  57 │   - get
  58 │   - list
  59 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager86_15.yaml:62-72
────────────────────────────────────────
  62 ┌ - apiGroups:
  63 │   - networking.k8s.io
  64 │   resources:
  65 │   - ingresses
  66 │   verbs:
  67 │   - get
  68 │   - list
  69 │   - watch
  70 └   - create
  ..   
────────────────────────────────────────



cert-manager86_32.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cert-manager-cainjector:leaderelection' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cert-manager86_32.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - configmaps
  17 │   resourceNames:
  18 │   - cert-manager-cainjector-leader-election
  19 │   - cert-manager-cainjector-leader-election-core
  20 │   verbs:
  21 └   - get
  ..   
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cert-manager-cainjector:leaderelection' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cert-manager86_32.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - configmaps
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager86_33.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0049 (MEDIUM): Role 'cert-manager:leaderelection' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cert-manager86_33.yaml:13-22
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - configmaps
  17 │   resourceNames:
  18 │   - cert-manager-controller
  19 │   verbs:
  20 │   - get
  21 │   - update
  22 └   - patch
────────────────────────────────────────


AVD-KSV-0049 (MEDIUM): Role 'cert-manager:leaderelection' should not have access to resource 'configmaps' for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Some workloads leverage configmaps to store sensitive data or configuration parameters that affect runtime behavior that can be modified by an attacker or combined with another issue to potentially lead to compromise.

See https://avd.aquasec.com/misconfig/ksv049
────────────────────────────────────────
 cert-manager86_33.yaml:23-28
────────────────────────────────────────
  23 ┌ - apiGroups:
  24 │   - ''
  25 │   resources:
  26 │   - configmaps
  27 │   verbs:
  28 └   - create
────────────────────────────────────────



cert-manager86_34.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager86_34.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager86_34.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager86_38.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager86_38.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



cert-manager86_39.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager86_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: 10250
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



cert-manager86_40.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 100, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager-cainjector' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager86_40.yaml:13-43
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager-cainjector" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager86_40.yaml:32-43
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────



cert-manager86_41.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 100, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager86_41.yaml:13-51
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager86_41.yaml:36-51
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.5.4
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────



cert-manager86_42.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 100, FAILURES: 15)
Failures: 15 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 1, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager-webhook' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager86_42.yaml:13-70
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager-webhook" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager86_42.yaml:32-70
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.5.4
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────



cert-manager9_10.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_10.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 cert-manager9_10.yaml:37-47
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 └   - watch
  ..   
────────────────────────────────────────



cert-manager9_11.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_11.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager9_12.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_12.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



cert-manager9_13.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_13.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 │   - watch
  58 └   - create
  ..   
────────────────────────────────────────



cert-manager9_14.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_14.yaml:51-58
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - secrets
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



cert-manager9_15.yaml (kubernetes)
==================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_15.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resources:
  40 │   - secrets
  41 │   verbs:
  42 │   - get
  43 │   - list
  44 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 cert-manager9_15.yaml:97-104
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - ''
  99 │   resources:
 100 │   - secrets
 101 │   verbs:
 102 │   - get
 103 │   - list
 104 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 cert-manager9_15.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager9_15.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 cert-manager9_15.yaml:63-73
────────────────────────────────────────
  63 ┌ - apiGroups:
  64 │   - networking.k8s.io
  65 │   resources:
  66 │   - ingresses
  67 │   verbs:
  68 │   - get
  69 │   - list
  70 │   - watch
  71 └   - create
  ..   
────────────────────────────────────────



cert-manager9_35.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager9_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert-manager9_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



cert-manager9_39.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager9_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



cert-manager9_40.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager9_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



cert-manager9_41.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager9_41.yaml:13-53
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-cainjector in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager9_41.yaml:35-51
────────────────────────────────────────
  35 ┌       - name: cert-manager-cainjector
  36 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --leader-election-namespace=kube-system
  41 │         env:
  42 │         - name: POD_NAMESPACE
  43 └           valueFrom:
  ..   
────────────────────────────────────────



cert-manager9_42.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager9_42.yaml:13-77
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-controller in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager9_42.yaml:39-75
────────────────────────────────────────
  39 ┌       - name: cert-manager-controller
  40 │         image: quay.io/jetstack/cert-manager-controller:v1.14.4
  41 │         imagePullPolicy: IfNotPresent
  42 │         args:
  43 │         - --v=2
  44 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  45 │         - --leader-election-namespace=kube-system
  46 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.4
  47 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────



cert-manager9_43.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-manager9_43.yaml:13-85
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-webhook in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 cert-manager9_43.yaml:35-83
────────────────────────────────────────
  35 ┌       - name: cert-manager-webhook
  36 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.4
  37 │         imagePullPolicy: IfNotPresent
  38 │         args:
  39 │         - --v=2
  40 │         - --secure-port=10250
  41 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  42 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  43 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────



cert-signing-request.yaml (kubernetes)
======================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-signing-request.yaml:7-12
────────────────────────────────────────
   7 ┌   request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQkV6Q0J1d0lCQURBZ01SNHdIQVlEVlFRREV4VmhjbWR2WTJRdWRHaGxkMkZzYkdSaGJ5NWpiMjB3V1RBVApCZ2NxaGtqT1BRSUJCZ2dxaGtqT1BRTUJCd05DQUFRMjlGamNjbS9wb08zdU5VdG50ZS81OHNJVkpPUVVYWVloCm5jcDNUMVNiV01tc2ROV3JOcmRYYm5CbUVaUHA0cmVuQ1UzNThrUi93TGpjdHVIRVQwU1dvRGt3TndZSktvWkkKaHZjTkFRa09NU293S0RBbUJnTlZIUkVFSHpBZGdoVmhjbWR2WTJRdWRHaGxkMkZzYkdSaGJ5NWpiMjJIQk1DbwpDZ0l3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnTFNpY3kxVVdTQmVMVmJ3U2dLMHVwS01RZ1ViOVU2ZzdUU05RCnNZeHFOTThDSUNHbDZSNENCaUxZTUNpMHpDeUM0UTJaOFFSemNYbHVCd3hMaGNscUR1NmMKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
   8 │   signerName: argocd.thewalldao.com/serving
   9 │   usages:
  10 │     - digital signature
  11 │     - key encipherment
  12 └     - server auth
────────────────────────────────────────



cert-signing-request1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert-signing-request1.yaml:7-12
────────────────────────────────────────
   7 ┌   request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQkV6Q0J1d0lCQURBZ01SNHdIQVlEVlFRREV4VmhjbWR2WTJRdWRHaGxkMkZzYkdSaGJ5NWpiMjB3V1RBVApCZ2NxaGtqT1BRSUJCZ2dxaGtqT1BRTUJCd05DQUFRMjlGamNjbS9wb08zdU5VdG50ZS81OHNJVkpPUVVYWVloCm5jcDNUMVNiV01tc2ROV3JOcmRYYm5CbUVaUHA0cmVuQ1UzNThrUi93TGpjdHVIRVQwU1dvRGt3TndZSktvWkkKaHZjTkFRa09NU293S0RBbUJnTlZIUkVFSHpBZGdoVmhjbWR2WTJRdWRHaGxkMkZzYkdSaGJ5NWpiMjJIQk1DbwpDZ0l3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnTFNpY3kxVVdTQmVMVmJ3U2dLMHVwS01RZ1ViOVU2ZzdUU05RCnNZeHFOTThDSUNHbDZSNENCaUxZTUNpMHpDeUM0UTJaOFFSemNYbHVCd3hMaGNscUR1NmMKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
   8 │   signerName: argocd.thewalldao.com/serving
   9 │   usages:
  10 │     - digital signature
  11 │     - key encipherment
  12 └     - server auth
────────────────────────────────────────



cert-temp-rolebinding.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'temp-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cert-temp-rolebinding.yaml:6-7
────────────────────────────────────────
   6 ┌   name: temp-admin
   7 └   namespace: default
────────────────────────────────────────



cert-temp-rolebinding1.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'temp-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cert-temp-rolebinding1.yaml:6-7
────────────────────────────────────────
   6 ┌   name: temp-admin
   7 └   namespace: default
────────────────────────────────────────



cert-temp-rolebinding2.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'temp-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cert-temp-rolebinding2.yaml:6-7
────────────────────────────────────────
   6 ┌   name: temp-admin
   7 └   namespace: default
────────────────────────────────────────



cert-temp-rolebinding3.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'temp-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cert-temp-rolebinding3.yaml:6-7
────────────────────────────────────────
   6 ┌   name: temp-admin
   7 └   namespace: default
────────────────────────────────────────



cert-temp-rolebinding4.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'temp-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cert-temp-rolebinding4.yaml:6-7
────────────────────────────────────────
   6 ┌   name: temp-admin
   7 └   namespace: default
────────────────────────────────────────



cert-temp-rolebinding5.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'temp-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cert-temp-rolebinding5.yaml:6-7
────────────────────────────────────────
   6 ┌   name: temp-admin
   7 └   namespace: default
────────────────────────────────────────



cert3_1.yaml (kubernetes)
=========================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert3_1.yaml:8-22
────────────────────────────────────────
   8 ┌   tls:
   9 │   - hosts:
  10 │     - student.ehb.petstore.com
  11 │     secretName: student.ehb.petstore.com
  12 │   rules:
  13 │   - host: student.ehb.petstore.com
  14 │     http:
  15 │       paths:
  16 └       - path: /
  ..   
────────────────────────────────────────



cert_provisioner_job.yaml (kubernetes)
======================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert_provisioner_job.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'provisioner' of Job 'cert-provisioner-job' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cert_provisioner_job.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'provisioner' of Job 'cert-provisioner-job' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert_provisioner_job.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert_provisioner_job.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert_provisioner_job.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert_provisioner_job.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert_provisioner_job.yaml:7-43
────────────────────────────────────────
   7 ┌   template:
   8 │     metadata:
   9 │       name: cert-provisioner-job
  10 │     spec:
  11 │       serviceAccountName: pl-cert-provisioner-service-account
  12 │       containers:
  13 │       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): job cert-provisioner-job in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cert_provisioner_job.yaml:5
────────────────────────────────────────
   5 [   name: cert-provisioner-job
────────────────────────────────────────



cert_provisioner_job1.yaml (kubernetes)
=======================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 6, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cert_provisioner_job1.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'provisioner' of Job 'cert-provisioner-job' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cert_provisioner_job1.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'provisioner' of Job 'cert-provisioner-job' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cert_provisioner_job1.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cert_provisioner_job1.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cert_provisioner_job1.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'provisioner' of Job 'cert-provisioner-job' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cert_provisioner_job1.yaml:13-32
────────────────────────────────────────
  13 ┌       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 │         env:
  16 │         - name: PL_NAMESPACE
  17 │           valueFrom:
  18 │             fieldRef:
  19 │               fieldPath: metadata.namespace
  20 │         envFrom:
  21 └         - configMapRef:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cert_provisioner_job1.yaml:7-43
────────────────────────────────────────
   7 ┌   template:
   8 │     metadata:
   9 │       name: cert-provisioner-job
  10 │     spec:
  11 │       serviceAccountName: pl-cert-provisioner-service-account
  12 │       containers:
  13 │       - name: provisioner
  14 │         image: vizier-cert_provisioner_image:latest
  15 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): job cert-provisioner-job in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cert_provisioner_job1.yaml:5
────────────────────────────────────────
   5 [   name: cert-provisioner-job
────────────────────────────────────────



cert_provisioner_role1_1.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'pl-cert-provisioner-role' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert_provisioner_role1_1.yaml:6-17
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   verbs:
  11 │   - create
  12 │   - delete
  13 │   - get
  14 └   - list
  ..   
────────────────────────────────────────



cert_provisioner_role_1.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'pl-cert-provisioner-role' shouldn't have access to manage secrets in namespace 'default'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 cert_provisioner_role_1.yaml:6-17
────────────────────────────────────────
   6 ┌ - apiGroups:
   7 │   - ''
   8 │   resources:
   9 │   - secrets
  10 │   verbs:
  11 │   - create
  12 │   - delete
  13 │   - get
  14 └   - list
  ..   
────────────────────────────────────────



certbot2.yaml (kubernetes)
==========================
Tests: 116 (SUCCESSES: 97, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 11, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'certbot' of 'cronjob' 'certbot-cronjob-5c8811' in 'certbot' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'certbot' of CronJob 'certbot-cronjob-5c8811' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certbot2.yaml:7-27
────────────────────────────────────────
   7 ┌   jobTemplate:
   8 │     metadata: null
   9 │     spec:
  10 │       template:
  11 │         metadata: null
  12 │         spec:
  13 │           containers:
  14 │           - command:
  15 └             - /bin/sh
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "certbot" of cronjob "certbot-cronjob-5c8811" in "certbot" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container certbot-cronjob-5c8811 in certbot namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): cronjob certbot-cronjob-5c8811 in certbot namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 certbot2.yaml:13-26
────────────────────────────────────────
  13 ┌           containers:
  14 │           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 └             image: certbot/certbot
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container certbot in cronjob certbot-cronjob-5c8811 (namespace: certbot) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certbot2.yaml:14-24
────────────────────────────────────────
  14 ┌           - command:
  15 │             - /bin/sh
  16 │             - -c
  17 │             - certbot certonly --email  -d "example.com" -d "*.example.com" && cd
  18 │               /etc/letsencrypt/live/example.com && kubectl delete secret certbot-tls-secret
  19 │               -n default || true && kubectl create secret tls certbot-tls-secret -n
  20 │               default --cert=fullchain.pem --key=privkey.pem
  21 │             image: certbot/certbot
  22 └             imagePullPolicy: Always
  ..   
────────────────────────────────────────



certbot2_2.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'certbot-role' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certbot2_2.yaml:6-10
────────────────────────────────────────
   6 ┌ - resources:
   7 │   - secrets
   8 │   verbs:
   9 │   - create
  10 └   - delete
────────────────────────────────────────



certificate-sigining-request.yaml (kubernetes)
==============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate-sigining-request.yaml:6-9
────────────────────────────────────────
   6 ┌   request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXZLdVJIQk5jNWFGbUpiZGt5a3BPRThBekZzbWZQWVZBeXJoamtDRnZhZDhLCmo4QzRxQ0phZk83bCtPLzgybUM0cWpJeXhIZkYxMjBqQ0cxdnUzUzZCbWtMQlI3U0ZNWTR5Sm5nbDBvSk90UTEKMUlmbEl4SkZxMHpaeGVEZnlQa0ZRVkpmTElOOGRJUVZiRERrcW5xREp0dXpPbzk3V1BBMjlaMmdkUjJTR05OaAovZ2pGRWVETjZDRHNIbWQvaGhSNHA1MFZYYVF4OUFqSTY0ZUZoeXBYUGVMUWV2VXpINGZNdTZkUC9oUmhEQW9JCldoSWNwdGVybTdRSTM5MHRPTElkVGxDcCtpYzEwbmlJemZkeHlaQkRxK3Z3OW50UUJPbDJTeFRseUl0eDRDUU8KTnAyNVdDcGlmNWZKOUtpd3FnWWI4dUdELzNwWWM5dVMrbkVMWFl1THV3SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBS0pVUDlXbTRvSmpOc1RlbzFnNnY0MFlDMDh5dVVjMm9SYi9ORmpBcmFWcE84VFVjMmIzCkU0R1R5QWNFUlpBL1gvRnlzcTU3QWlGZmV4VkpUdDJYWHJCK20xV2ZKUUE2T2FpMjJtODlLc2w5RnVCaTFJY20KZjBQQzkrUnFPZk1mSVRabGozamUwRSsvb1QvRU9MdHZXNkQ1RWpZTnpEeFJlc3ZVaE5Gc21hcjZDVTZXRlBUdgowQjhEMVBnV0VvcHF6blAzUUhmTHY3UWd0YTFHQ3M5VnRGZE1mMnhCR1FwcG9POXNoaTE4b2hjSlg2eVdWV05iCmMzMkthMStxTFhzVElJNDBGbWhMT2c3aFRWeFBTRjFKMHZQekhNYktocGpuSHlPM1kvRHJIZEx6UHRKaXFtenMKR3NOSkZJZVVRSTZSY1NCRklJamhTVk16Wm93eG5Mcm1GMTg9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
   7 │   signerName: kubernetes.io/kube-apiserver-client
   8 │   usages:
   9 └   - client auth
────────────────────────────────────────



certificate-signing-request.template.yaml (kubernetes)
======================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate-signing-request.template.yaml:19-29
────────────────────────────────────────
  19 ┌   signerName: "kubernetes.io/kube-apiserver-client"
  20 │ 
  21 │   groups: ["database-engineers"]
  22 │ 
  23 │   ## Put the base 64 encoded CSR here.
  24 │   request: ""
  25 │ 
  26 │   usages: [
  27 └     "digital signature",
  ..   
────────────────────────────────────────



certificate-signing-request.template1.yaml (kubernetes)
=======================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate-signing-request.template1.yaml:19-29
────────────────────────────────────────
  19 ┌   signerName: "kubernetes.io/kube-apiserver-client"
  20 │ 
  21 │   groups: ["database-engineers"]
  22 │ 
  23 │   ## Put the base 64 encoded CSR here.
  24 │   request: ""
  25 │ 
  26 │   usages: [
  27 └     "digital signature",
  ..   
────────────────────────────────────────



certificate-signing-request.yaml (kubernetes)
=============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate-signing-request.yaml:6-10
────────────────────────────────────────
   6 ┌   request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUtJL3pMQ0J0ZW42ZUdpVm4xMkxFQmJMWkN5TkkxV25DRE1mVTNxeHEydXRmQ0huCjVmYzdQQm9tUWtJU2xxK2hpYjNEbnFaNnR5NXV1bGl3M1pqKzB4U1NWYkIxUDdYZkhIakZLQ3NqUjY0L1dJd1kKTjllaWxQUzFkZWxpT21KNU5mb3N2SmJUK1NBMWZMUUt0VURLWjhRL25uaEN6WW9KUXV3VzVkVkcwVUkvb3Y4dApIYWV5NE45a1lUM1FFdFVqeTl1M0YvY1d0bEQ3eUZUdi9qNVpKblJMTnRENjd3L01KTGM2bzB2eWxLdUIydUF6CnhTc2JCNmpsOUpLcG41aEVoSlJZdkd5QlNkb1UzZDEwZ2FpSDU1d0VaSTJiaVBDU3E2T1MvV3VXcGVrT2lOSlEKeG9Wc3RXeTJhV3dqTktTV1lWT1RnUjhFQlpUMEtGaE1DaHVqdnRFQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQkM3M242b0JETjFGQkRMVm9OcnRRbGxMK0wvUzJ6enMvMHdWeWx6djcwWVA2Z3Y3cjFlaUJHCjFvSWhmVEllK051THlrb0wxQ1owVTZvRU92eTFVanNORTVxbnN4Njl6ZlFWS1NvWlV4TmhCQ2V6V1dieEFIcFkKK2VvcUtWZkdxdTFXb0w0Q3NpQ2RUVUpsclBDb2pxSU44K2ZQRDducEpJNWpBVkVCTGtWNFA0M2ZDbXpGSnBmagpLaSs1TVZZRndGK3RSUlRRc0tVZXEweW9JYnpzQjlqQUg3VzdudG1OSDZFSVJaOW8wZ3BaWm1uMHJJVjZuS1NlCjIveExRTTdlTlUrMDNYTTNidTQ2ejREYkg2WktNUnJIbEVoMnBlelg3eE54K1FaWlFOb084blhqMXlkM2oraGgKVkMwZHNoc3Iyd2tEQmVPczc2QTNvTmIrTWRpWTBYcWkKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
   7 │   signerName: kubernetes.io/kube-apiserver-client
   8 │   expirationSeconds: 86400  # one day
   9 │   usages:
  10 └   - client auth
────────────────────────────────────────



certificate-updater-ds.yaml (kubernetes)
========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate-updater-ds.yaml:7-15
────────────────────────────────────────
   7 ┌   podSelector:
   8 │     matchLabels:
   9 │       k8s-app: certificate-updater
  10 │   policyTypes:
  11 │   - Ingress
  12 │   - Egress
  13 │   ingress: []
  14 │   egress:
  15 └   - {}
────────────────────────────────────────



certificate-updater-ds_1.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 97, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 8, MEDIUM: 5, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'certificate-updater' of DaemonSet 'certificate-updater' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'certificate-updater' of 'daemonset' 'certificate-updater' in 'certificate-updater' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0008 (HIGH): DaemonSet 'certificate-updater' should not set 'spec.template.spec.hostIPC' to true
════════════════════════════════════════
Sharing the host’s IPC namespace allows container processes to communicate with processes on the host.

See https://avd.aquasec.com/misconfig/ksv008
────────────────────────────────────────
 certificate-updater-ds_1.yaml:10-98
────────────────────────────────────────
  10 ┌   selector:
  11 │     matchLabels:
  12 │       k8s-app: certificate-updater
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: certificate-updater
  17 │     spec:
  18 └       hostPID: true
  ..   
────────────────────────────────────────


AVD-KSV-0010 (HIGH): DaemonSet 'certificate-updater' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 certificate-updater-ds_1.yaml:10-98
────────────────────────────────────────
  10 ┌   selector:
  11 │     matchLabels:
  12 │       k8s-app: certificate-updater
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: certificate-updater
  17 │     spec:
  18 └       hostPID: true
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'certificate-updater' of DaemonSet 'certificate-updater' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0023 (MEDIUM): DaemonSet 'certificate-updater' should not set 'spec.template.volumes.hostPath'
════════════════════════════════════════
According to pod security standard 'HostPath Volumes', HostPath volumes must be forbidden.

See https://avd.aquasec.com/misconfig/ksv023
────────────────────────────────────────
 certificate-updater-ds_1.yaml:10-98
────────────────────────────────────────
  10 ┌   selector:
  11 │     matchLabels:
  12 │       k8s-app: certificate-updater
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: certificate-updater
  17 │     spec:
  18 └       hostPID: true
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate-updater-ds_1.yaml:10-98
────────────────────────────────────────
  10 ┌   selector:
  11 │     matchLabels:
  12 │       k8s-app: certificate-updater
  13 │   template:
  14 │     metadata:
  15 │       labels:
  16 │         k8s-app: certificate-updater
  17 │     spec:
  18 └       hostPID: true
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "certificate-updater" of daemonset "certificate-updater" in "certificate-updater" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): daemonset certificate-updater in certificate-updater namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 certificate-updater-ds_1.yaml:18-98
────────────────────────────────────────
  18 ┌       hostPID: true
  19 │       hostIPC: true
  20 │       containers:
  21 │       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 └         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container certificate-updater in daemonset certificate-updater (namespace: certificate-updater) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certificate-updater-ds_1.yaml:21-68
────────────────────────────────────────
  21 ┌       - name: certificate-updater
  22 │         image: cr.yandex/yc/mk8s-openssl:stable
  23 │         command:
  24 │         - sh
  25 │         - -c
  26 │         - "while true; do\n  diff -x '.*' -r /mnt/user-cert-path/ /usr/local/share/ca-certificates\n\
  27 │           \  if [ $? -ne 0 ];\n    then\n        echo \"Removing all old certificates\"\
  28 │           \n        rm -r /usr/local/share/ca-certificates/*\n        echo \"Copying\
  29 └           \ certificates from configmap\"\n        cp /mnt/sbin/update-ca-certificates\
  ..   
────────────────────────────────────────



certificate172_10.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 108, FAILURES: 7)
Failures: 7 (UNKNOWN: 0, LOW: 5, MEDIUM: 1, HIGH: 1, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'keycloak' of Deployment 'keycloak' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certificate172_10.yaml:23-120
────────────────────────────────────────
  23 ┌       - command:
  24 │         - /opt/keycloak/bin/kc.sh
  25 │         - start
  26 │         env:
  27 │         - name: KC_HEALTH_ENABLED
  28 │           value: 'true'
  29 │         - name: KC_HTTP_ENABLED
  30 │           value: 'true'
  31 └         - name: JAVA_OPTS_APPEND
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'keycloak' of Deployment 'keycloak' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 certificate172_10.yaml:23-120
────────────────────────────────────────
  23 ┌       - command:
  24 │         - /opt/keycloak/bin/kc.sh
  25 │         - start
  26 │         env:
  27 │         - name: KC_HEALTH_ENABLED
  28 │           value: 'true'
  29 │         - name: KC_HTTP_ENABLED
  30 │           value: 'true'
  31 └         - name: JAVA_OPTS_APPEND
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'keycloak' of Deployment 'keycloak' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certificate172_10.yaml:23-120
────────────────────────────────────────
  23 ┌       - command:
  24 │         - /opt/keycloak/bin/kc.sh
  25 │         - start
  26 │         env:
  27 │         - name: KC_HEALTH_ENABLED
  28 │           value: 'true'
  29 │         - name: KC_HTTP_ENABLED
  30 │           value: 'true'
  31 └         - name: JAVA_OPTS_APPEND
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'keycloak' of Deployment 'keycloak' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certificate172_10.yaml:23-120
────────────────────────────────────────
  23 ┌       - command:
  24 │         - /opt/keycloak/bin/kc.sh
  25 │         - start
  26 │         env:
  27 │         - name: KC_HEALTH_ENABLED
  28 │           value: 'true'
  29 │         - name: KC_HTTP_ENABLED
  30 │           value: 'true'
  31 └         - name: JAVA_OPTS_APPEND
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'keycloak' of Deployment 'keycloak' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certificate172_10.yaml:23-120
────────────────────────────────────────
  23 ┌       - command:
  24 │         - /opt/keycloak/bin/kc.sh
  25 │         - start
  26 │         env:
  27 │         - name: KC_HEALTH_ENABLED
  28 │           value: 'true'
  29 │         - name: KC_HTTP_ENABLED
  30 │           value: 'true'
  31 └         - name: JAVA_OPTS_APPEND
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate172_10.yaml:11-137
────────────────────────────────────────
  11 ┌   replicas: 2
  12 │   selector:
  13 │     matchLabels:
  14 │       app.kubernetes.io/name: keycloak
  15 │   strategy:
  16 │     type: Recreate
  17 │   template:
  18 │     metadata:
  19 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container keycloak in deployment keycloak (namespace: test) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certificate172_10.yaml:23-120
────────────────────────────────────────
  23 ┌       - command:
  24 │         - /opt/keycloak/bin/kc.sh
  25 │         - start
  26 │         env:
  27 │         - name: KC_HEALTH_ENABLED
  28 │           value: 'true'
  29 │         - name: KC_HTTP_ENABLED
  30 │           value: 'true'
  31 └         - name: JAVA_OPTS_APPEND
  ..   
────────────────────────────────────────



certificate172_5.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate172_5.yaml:12-20
────────────────────────────────────────
  12 ┌   ports:
  13 │   - appProtocol: https
  14 │     name: https
  15 │     port: 8443
  16 │     protocol: TCP
  17 │     targetPort: https
  18 │   selector:
  19 │     app.kubernetes.io/name: keycloak
  20 └   type: ClusterIP
────────────────────────────────────────



certificate172_6.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate172_6.yaml:12-27
────────────────────────────────────────
  12 ┌   clusterIP: None
  13 │   ports:
  14 │   - appProtocol: tcp
  15 │     name: jgroups
  16 │     port: 7800
  17 │     protocol: TCP
  18 │     targetPort: jgroups
  19 │   - appProtocol: tcp
  20 └     name: jgroups-ssl
  ..   
────────────────────────────────────────



certificate172_7.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0109 (HIGH): ConfigMap 'keycloak-66d00e6e' in 'test' namespace stores secrets in key(s) or value(s) '{"                                      keystore_password"}'
════════════════════════════════════════
Storing secrets in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-0109
────────────────────────────────────────



certificate172_8.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate172_8.yaml:11-14
────────────────────────────────────────
  11 ┌   minAvailable: 1
  12 │   selector:
  13 │     matchLabels:
  14 └       app.kubernetes.io/name: keycloak
────────────────────────────────────────



certificate172_9.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificate172_9.yaml:11-30
────────────────────────────────────────
  11 ┌   ingress:
  12 │   - from:
  13 │     - podSelector: {}
  14 │     ports:
  15 │     - port: 8443
  16 │       protocol: TCP
  17 │   - from:
  18 │     - podSelector:
  19 └         matchLabels:
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_10.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_10.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 certificates-cert-manager-resources_10.yaml:37-47
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 └   - watch
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_11.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_11.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_12.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_12.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_13.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_13.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 │   - watch
  58 └   - create
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_14.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_14.yaml:51-58
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - secrets
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



certificates-cert-manager-resources_15.yaml (kubernetes)
========================================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_15.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resources:
  40 │   - secrets
  41 │   verbs:
  42 │   - get
  43 │   - list
  44 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certificates-cert-manager-resources_15.yaml:97-104
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - ''
  99 │   resources:
 100 │   - secrets
 101 │   verbs:
 102 │   - get
 103 │   - list
 104 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 certificates-cert-manager-resources_15.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 certificates-cert-manager-resources_15.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 certificates-cert-manager-resources_15.yaml:63-73
────────────────────────────────────────
  63 ┌ - apiGroups:
  64 │   - networking.k8s.io
  65 │   resources:
  66 │   - ingresses
  67 │   verbs:
  68 │   - get
  69 │   - list
  70 │   - watch
  71 └   - create
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_35.yaml (kubernetes)
========================================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 certificates-cert-manager-resources_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 certificates-cert-manager-resources_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



certificates-cert-manager-resources_39.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates-cert-manager-resources_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



certificates-cert-manager-resources_40.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates-cert-manager-resources_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



certificates-cert-manager-resources_41.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-cainjector' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:13-54
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   revisionHistoryLimit: null
  15 │   selector:
  16 │     matchLabels:
  17 │       app.kubernetes.io/name: cainjector
  18 │       app.kubernetes.io/instance: cert-manager
  19 │       app.kubernetes.io/component: cainjector
  20 │   template:
  21 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-cainjector in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certificates-cert-manager-resources_41.yaml:36-52
────────────────────────────────────────
  36 ┌       - name: cert-manager-cainjector
  37 │         image: quay.io/jetstack/cert-manager-cainjector:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --leader-election-namespace=kube-system
  42 │         env:
  43 │         - name: POD_NAMESPACE
  44 └           valueFrom:
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_42.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-controller' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:13-78
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   revisionHistoryLimit: null
  15 │   selector:
  16 │     matchLabels:
  17 │       app.kubernetes.io/name: cert-manager
  18 │       app.kubernetes.io/instance: cert-manager
  19 │       app.kubernetes.io/component: controller
  20 │   template:
  21 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-controller in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certificates-cert-manager-resources_42.yaml:40-76
────────────────────────────────────────
  40 ┌       - name: cert-manager-controller
  41 │         image: quay.io/jetstack/cert-manager-controller:v1.14.1
  42 │         imagePullPolicy: IfNotPresent
  43 │         args:
  44 │         - --v=2
  45 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  46 │         - --leader-election-namespace=kube-system
  47 │         - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.14.1
  48 └         - --max-concurrent-challenges=60
  ..   
────────────────────────────────────────



certificates-cert-manager-resources_43.yaml (kubernetes)
========================================================
Tests: 115 (SUCCESSES: 107, FAILURES: 8)
Failures: 8 (UNKNOWN: 0, LOW: 7, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0011 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager-webhook' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:13-86
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   revisionHistoryLimit: null
  15 │   selector:
  16 │     matchLabels:
  17 │       app.kubernetes.io/name: webhook
  18 │       app.kubernetes.io/instance: cert-manager
  19 │       app.kubernetes.io/component: webhook
  20 │   template:
  21 └     metadata:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager-webhook in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certificates-cert-manager-resources_43.yaml:36-84
────────────────────────────────────────
  36 ┌       - name: cert-manager-webhook
  37 │         image: quay.io/jetstack/cert-manager-webhook:v1.14.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --secure-port=10250
  42 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  43 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  44 └         - --dynamic-serving-dns-names=cert-manager-webhook
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest1.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest1.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest10.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest10.yaml:33-44
────────────────────────────────────────
  33 ┌   expirationSeconds: 1305381319
  34 │   extra:
  35 │     "24":
  36 │     - "25"
  37 │   groups:
  38 │   - "23"
  39 │   request: OA==
  40 │   signerName: "20"
  41 └   uid: "22"
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest11.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest11.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest12.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest12.yaml:37-48
────────────────────────────────────────
  37 ┌   expirationSeconds: 8
  38 │   extra:
  39 │     extraKey:
  40 │     - extraValue
  41 │   groups:
  42 │   - groupsValue
  43 │   request: AQ==
  44 │   signerName: signerNameValue
  45 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest13.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest13.yaml:34-45
────────────────────────────────────────
  34 ┌   expirationSeconds: 1305381319
  35 │   extra:
  36 │     "24":
  37 │     - "25"
  38 │   groups:
  39 │   - "23"
  40 │   request: OA==
  41 │   signerName: "20"
  42 └   uid: "22"
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest14.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest14.yaml:34-45
────────────────────────────────────────
  34 ┌   expirationSeconds: 1305381319
  35 │   extra:
  36 │     "24":
  37 │     - "25"
  38 │   groups:
  39 │   - "23"
  40 │   request: OA==
  41 │   signerName: "20"
  42 └   uid: "22"
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest15.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest15.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest16.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest16.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest17.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest17.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest18.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest18.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest19.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest19.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest2.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest2.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest20.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest20.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest21.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest21.yaml:37-48
────────────────────────────────────────
  37 ┌   expirationSeconds: 8
  38 │   extra:
  39 │     extraKey:
  40 │     - extraValue
  41 │   groups:
  42 │   - groupsValue
  43 │   request: AQ==
  44 │   signerName: signerNameValue
  45 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest22.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest22.yaml:34-45
────────────────────────────────────────
  34 ┌   expirationSeconds: 1305381319
  35 │   extra:
  36 │     "24":
  37 │     - "25"
  38 │   groups:
  39 │   - "23"
  40 │   request: OA==
  41 │   signerName: "20"
  42 └   uid: "22"
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest23.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest23.yaml:34-45
────────────────────────────────────────
  34 ┌   expirationSeconds: 1305381319
  35 │   extra:
  36 │     "24":
  37 │     - "25"
  38 │   groups:
  39 │   - "23"
  40 │   request: OA==
  41 │   signerName: "20"
  42 └   uid: "22"
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest24.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest24.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest25.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest25.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest26.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest26.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest27.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest27.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest28.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest28.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest29.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest29.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest3.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest3.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest33.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest33.yaml:33-43
────────────────────────────────────────
  33 ┌   extra:
  34 │     "23":
  35 │     - "24"
  36 │   groups:
  37 │   - "22"
  38 │   request: OA==
  39 │   signerName: "19"
  40 │   uid: "21"
  41 └   usages:
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest34.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest34.yaml:33-43
────────────────────────────────────────
  33 ┌   extra:
  34 │     "23":
  35 │     - "24"
  36 │   groups:
  37 │   - "22"
  38 │   request: OA==
  39 │   signerName: "19"
  40 │   uid: "21"
  41 └   usages:
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest35.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest35.yaml:33-43
────────────────────────────────────────
  33 ┌   extra:
  34 │     "23":
  35 │     - "24"
  36 │   groups:
  37 │   - "22"
  38 │   request: OA==
  39 │   signerName: "19"
  40 │   uid: "21"
  41 └   usages:
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest36.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest36.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest37.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest37.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest38.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest38.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest39.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest39.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest4.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest4.yaml:33-44
────────────────────────────────────────
  33 ┌   expirationSeconds: 1305381319
  34 │   extra:
  35 │     "24":
  36 │     - "25"
  37 │   groups:
  38 │   - "23"
  39 │   request: OA==
  40 │   signerName: "20"
  41 └   uid: "22"
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest40.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest40.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest41.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest41.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest42.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest42.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest43.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest43.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest44.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest44.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest45.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest45.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest46.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest46.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest47.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest47.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest48.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest48.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest49.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest49.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest5.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest5.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest50.yaml (kubernetes)
====================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest50.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest6.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest6.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest7.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest7.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest8.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest8.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1.CertificateSigningRequest9.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1.CertificateSigningRequest9.yaml:36-47
────────────────────────────────────────
  36 ┌   expirationSeconds: 8
  37 │   extra:
  38 │     extraKey:
  39 │     - extraValue
  40 │   groups:
  41 │   - groupsValue
  42 │   request: AQ==
  43 │   signerName: signerNameValue
  44 └   uid: uidValue
  ..   
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle.yaml (kubernetes)
=================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle1.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle1.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle10.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle10.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle11.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle11.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle12.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle12.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle13.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle13.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle14.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle14.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle15.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle15.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle16.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle16.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle17.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle17.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle18.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle18.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle19.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle19.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle2.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle2.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle20.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle20.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle21.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle21.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle22.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle22.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle23.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle23.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle24.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle24.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle25.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle25.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle26.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle26.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle27.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle27.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle28.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle28.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle29.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle29.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle3.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle3.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle30.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle30.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle31.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle31.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle32.yaml (kubernetes)
===================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle32.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle4.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle4.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle5.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle5.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle6.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle6.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle7.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle7.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle8.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle8.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certificates.k8s.io.v1alpha1.ClusterTrustBundle9.yaml (kubernetes)
==================================================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certificates.k8s.io.v1alpha1.ClusterTrustBundle9.yaml:36-37
────────────────────────────────────────
  36 ┌   signerName: signerNameValue
  37 └   trustBundle: trustBundleValue
────────────────────────────────────────



certman-ingress-letsenc.yaml (kubernetes)
=========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certman-ingress-letsenc.yaml:14-29
────────────────────────────────────────
  14 ┌   ingressClassName: nginx
  15 │   rules:
  16 │   - host: argocd.c0dexec.dev
  17 │     http:
  18 │       paths:
  19 │       - path: /
  20 │         pathType: Prefix
  21 │         backend:
  22 └           service:
  ..   
────────────────────────────────────────



certmanager17_11.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-cainjector' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_11.yaml:20-27
────────────────────────────────────────
  20 ┌ - apiGroups:
  21 │   - ''
  22 │   resources:
  23 │   - secrets
  24 │   verbs:
  25 │   - get
  26 │   - list
  27 └   - watch
────────────────────────────────────────


AVD-KSV-0114 (CRITICAL): ClusterRole 'cert-manager-cainjector' should not have access to resources ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Webhooks can silently intercept or actively mutate/block resources as they are being created or updated. This includes secrets and pod specs.

See https://avd.aquasec.com/misconfig/ksv114
────────────────────────────────────────
 certmanager17_11.yaml:37-46
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - admissionregistration.k8s.io
  39 │   resources:
  40 │   - validatingwebhookconfigurations
  41 │   - mutatingwebhookconfigurations
  42 │   verbs:
  43 │   - get
  44 │   - list
  45 │   - watch
  46 └   - update
────────────────────────────────────────



certmanager17_12.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-issuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_12.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



certmanager17_13.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-clusterissuers' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_13.yaml:28-38
────────────────────────────────────────
  28 ┌ - apiGroups:
  29 │   - ''
  30 │   resources:
  31 │   - secrets
  32 │   verbs:
  33 │   - get
  34 │   - list
  35 │   - watch
  36 └   - create
  ..   
────────────────────────────────────────



certmanager17_14.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-certificates' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_14.yaml:50-61
────────────────────────────────────────
  50 ┌ - apiGroups:
  51 │   - ''
  52 │   resources:
  53 │   - secrets
  54 │   verbs:
  55 │   - get
  56 │   - list
  57 │   - watch
  58 └   - create
  ..   
────────────────────────────────────────



certmanager17_15.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-orders' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_15.yaml:51-58
────────────────────────────────────────
  51 ┌ - apiGroups:
  52 │   - ''
  53 │   resources:
  54 │   - secrets
  55 │   verbs:
  56 │   - get
  57 │   - list
  58 └   - watch
────────────────────────────────────────



certmanager17_16.yaml (kubernetes)
==================================
Tests: 117 (SUCCESSES: 112, FAILURES: 5)
Failures: 5 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 2, CRITICAL: 2)

AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_16.yaml:37-44
────────────────────────────────────────
  37 ┌ - apiGroups:
  38 │   - ''
  39 │   resources:
  40 │   - secrets
  41 │   verbs:
  42 │   - get
  43 │   - list
  44 └   - watch
────────────────────────────────────────


AVD-KSV-0041 (CRITICAL): ClusterRole 'cert-manager-controller-challenges' shouldn't have access to manage resource 'secrets'
════════════════════════════════════════
Viewing secrets at the cluster-scope is akin to cluster-admin in most clusters as there are typically at least one service accounts (their token stored in a secret) bound to cluster-admin directly or a role/clusterrole that gives similar permissions.

See https://avd.aquasec.com/misconfig/ksv041
────────────────────────────────────────
 certmanager17_16.yaml:97-104
────────────────────────────────────────
  97 ┌ - apiGroups:
  98 │   - ''
  99 │   resources:
 100 │   - secrets
 101 │   verbs:
 102 │   - get
 103 │   - list
 104 └   - watch
────────────────────────────────────────


AVD-KSV-0048 (MEDIUM): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 certmanager17_16.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 certmanager17_16.yaml:52-62
────────────────────────────────────────
  52 ┌ - apiGroups:
  53 │   - ''
  54 │   resources:
  55 │   - pods
  56 │   - services
  57 │   verbs:
  58 │   - get
  59 │   - list
  60 └   - watch
  ..   
────────────────────────────────────────


AVD-KSV-0056 (HIGH): ClusterRole 'cert-manager-controller-challenges' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 certmanager17_16.yaml:63-73
────────────────────────────────────────
  63 ┌ - apiGroups:
  64 │   - networking.k8s.io
  65 │   resources:
  66 │   - ingresses
  67 │   verbs:
  68 │   - get
  69 │   - list
  70 │   - watch
  71 └   - create
  ..   
────────────────────────────────────────



certmanager17_35.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 114, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 2, HIGH: 0, CRITICAL: 0)

AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 certmanager17_35.yaml:13-23
────────────────────────────────────────
  13 ┌ - apiGroups:
  14 │   - ''
  15 │   resources:
  16 │   - secrets
  17 │   resourceNames:
  18 │   - cert-manager-webhook-ca
  19 │   verbs:
  20 │   - get
  21 └   - list
  ..   
────────────────────────────────────────


AVD-KSV-0113 (MEDIUM): Role 'cert-manager-webhook:dynamic-serving' shouldn't have access to manage secrets in namespace 'cert-manager'
════════════════════════════════════════
Viewing secrets at the namespace scope can lead to escalation if another service account in that namespace has a higher privileged rolebinding or clusterrolebinding bound.

See https://avd.aquasec.com/misconfig/ksv113
────────────────────────────────────────
 certmanager17_35.yaml:24-29
────────────────────────────────────────
  24 ┌ - apiGroups:
  25 │   - ''
  26 │   resources:
  27 │   - secrets
  28 │   verbs:
  29 └   - create
────────────────────────────────────────



certmanager17_39.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certmanager17_39.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - protocol: TCP
  16 │     port: 9402
  17 │     name: tcp-prometheus-servicemonitor
  18 │     targetPort: 9402
  19 │   selector:
  20 │     app.kubernetes.io/name: cert-manager
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: controller
────────────────────────────────────────



certmanager17_40.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certmanager17_40.yaml:13-22
────────────────────────────────────────
  13 ┌   type: ClusterIP
  14 │   ports:
  15 │   - name: https
  16 │     port: 443
  17 │     protocol: TCP
  18 │     targetPort: https
  19 │   selector:
  20 │     app.kubernetes.io/name: webhook
  21 │     app.kubernetes.io/instance: cert-manager
  22 └     app.kubernetes.io/component: webhook
────────────────────────────────────────



certmanager17_41.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager-cainjector' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager-cainjector' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certmanager17_41.yaml:13-46
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cainjector
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: cainjector
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager-cainjector" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager-cainjector (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certmanager17_41.yaml:32-44
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-cainjector:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --leader-election-namespace=kube-system
  38 │         env:
  39 │         - name: POD_NAMESPACE
  40 └           valueFrom:
  ..   
────────────────────────────────────────



certmanager17_42.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certmanager17_42.yaml:13-55
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: cert-manager
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: controller
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certmanager17_42.yaml:36-53
────────────────────────────────────────
  36 ┌       - name: cert-manager
  37 │         image: quay.io/jetstack/cert-manager-controller:v1.9.1
  38 │         imagePullPolicy: IfNotPresent
  39 │         args:
  40 │         - --v=2
  41 │         - --cluster-resource-namespace=$(POD_NAMESPACE)
  42 │         - --leader-election-namespace=kube-system
  43 │         ports:
  44 └         - containerPort: 9402
  ..   
────────────────────────────────────────



certmanager17_43.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 101, FAILURES: 14)
Failures: 14 (UNKNOWN: 0, LOW: 11, MEDIUM: 2, HIGH: 1, CRITICAL: 0)

AVD-KSV-0003 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cert-manager' of 'deployment' 'cert-manager-webhook' in 'cert-manager' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cert-manager' of Deployment 'cert-manager-webhook' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certmanager17_43.yaml:13-73
────────────────────────────────────────
  13 ┌   replicas: 1
  14 │   selector:
  15 │     matchLabels:
  16 │       app.kubernetes.io/name: webhook
  17 │       app.kubernetes.io/instance: cert-manager
  18 │       app.kubernetes.io/component: webhook
  19 │   template:
  20 │     metadata:
  21 └       labels:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cert-manager" of deployment "cert-manager-webhook" in "cert-manager" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container cert-manager in deployment cert-manager-webhook (namespace: cert-manager) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 certmanager17_43.yaml:32-71
────────────────────────────────────────
  32 ┌       - name: cert-manager
  33 │         image: quay.io/jetstack/cert-manager-webhook:v1.9.1
  34 │         imagePullPolicy: IfNotPresent
  35 │         args:
  36 │         - --v=2
  37 │         - --secure-port=10250
  38 │         - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
  39 │         - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
  40 └         - --dynamic-serving-dns-names=cert-manager-webhook,cert-manager-webhook.cert-manager,cert-manager-webhook.cert-manager.svc
  ..   
────────────────────────────────────────



certs-persistentvolumeclaim.yaml (kubernetes)
=============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-persistentvolumeclaim.yaml:8-12
────────────────────────────────────────
   8 ┌   accessModes:
   9 │     - ReadWriteOnce
  10 │   resources:
  11 │     requests:
  12 └       storage: 100Mi
────────────────────────────────────────



certs-pv-claim.yaml (kubernetes)
================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv-claim.yaml:7-12
────────────────────────────────────────
   7 ┌   storageClassName: manual
   8 │   accessModes:
   9 │     - ReadWriteOnce
  10 │   resources:
  11 │     requests:
  12 └       storage: 1Gi
────────────────────────────────────────



certs-pv-claim1.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv-claim1.yaml:7-12
────────────────────────────────────────
   7 ┌   storageClassName: manual
   8 │   accessModes:
   9 │     - ReadWriteOnce
  10 │   resources:
  11 │     requests:
  12 └       storage: 1Gi
────────────────────────────────────────



certs-pv-volume.yaml (kubernetes)
=================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv-volume.yaml:9-15
────────────────────────────────────────
   9 ┌   storageClassName: manual
  10 │   capacity:
  11 │     storage: 1Gi
  12 │   accessModes:
  13 │     - ReadWriteOnce
  14 │   hostPath:
  15 └     path: "/certs"
────────────────────────────────────────



certs-pv-volume1.yaml (kubernetes)
==================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv-volume1.yaml:9-15
────────────────────────────────────────
   9 ┌   storageClassName: manual
  10 │   capacity:
  11 │     storage: 1Gi
  12 │   accessModes:
  13 │     - ReadWriteOnce
  14 │   hostPath:
  15 └     path: "/certs"
────────────────────────────────────────



certs-pv.yaml (kubernetes)
==========================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv.yaml:6-11
────────────────────────────────────────
   6 ┌   accessModes:
   7 │   - ReadWriteOnce
   8 │   capacity:
   9 │     storage: 5Gi
  10 │   hostPath:
  11 └     path: /tmp/certspv
────────────────────────────────────────



certs-pv_1.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv_1.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │   - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────



certs-pv_2.yaml (kubernetes)
============================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 certs-pv_2.yaml:6-10
────────────────────────────────────────
   6 ┌   accessModes:
   7 │   - ReadWriteOnce
   8 │   resources:
   9 │     requests:
  10 └       storage: 1Gi
────────────────────────────────────────



cf.yaml (kubernetes)
====================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-01010 (MEDIUM): ConfigMap 'nginx-conf' in 'default' namespace stores sensitive contents in key(s) or value(s) '{"key1.conf"}'
════════════════════════════════════════
Storing sensitive content such as usernames and email addresses in configMaps is unsafe

See https://avd.aquasec.com/misconfig/avd-ksv-01010
────────────────────────────────────────



cflor-clusterrolebinding.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cflor' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cflor-clusterrolebinding.yaml:4
────────────────────────────────────────
   4 [   name: cflor
────────────────────────────────────────



cflor-ps-clusterrolebinding.yaml (kubernetes)
=============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0111 (MEDIUM): ClusterRoleBinding 'cflor-ps-cluster-admin' should not bind to roles ["cluster-admin", "admin", "edit"]
════════════════════════════════════════
Either cluster-admin or those granted powerful permissions.

See https://avd.aquasec.com/misconfig/ksv111
────────────────────────────────────────
 cflor-ps-clusterrolebinding.yaml:4
────────────────────────────────────────
   4 [   name: cflor-ps-cluster-admin
────────────────────────────────────────



cgroup-test.yaml (kubernetes)
=============================
Tests: 115 (SUCCESSES: 93, FAILURES: 22)
Failures: 22 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 6, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'cgroup' of Pod 'cgroup-test' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'cgroup' of Pod 'cgroup-test' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'cgroup' of 'pod' 'cgroup-test' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0008 (HIGH): Pod 'cgroup-test' should not set 'spec.template.spec.hostIPC' to true
════════════════════════════════════════
Sharing the host’s IPC namespace allows container processes to communicate with processes on the host.

See https://avd.aquasec.com/misconfig/ksv008
────────────────────────────────────────
 cgroup-test.yaml:6-14
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 │       privileged: true
  12 │   hostPID: true
  13 │   hostIPC: true
  14 └   hostNetwork: true
────────────────────────────────────────


AVD-KSV-0009 (HIGH): Pod 'cgroup-test' should not set 'spec.template.spec.hostNetwork' to true
════════════════════════════════════════
Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.

See https://avd.aquasec.com/misconfig/ksv009
────────────────────────────────────────
 cgroup-test.yaml:6-14
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 │       privileged: true
  12 │   hostPID: true
  13 │   hostIPC: true
  14 └   hostNetwork: true
────────────────────────────────────────


AVD-KSV-0010 (HIGH): Pod 'cgroup-test' should not set 'spec.template.spec.hostPID' to true
════════════════════════════════════════
Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.

See https://avd.aquasec.com/misconfig/ksv010
────────────────────────────────────────
 cgroup-test.yaml:6-14
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 │       privileged: true
  12 │   hostPID: true
  13 │   hostIPC: true
  14 └   hostNetwork: true
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'cgroup' of Pod 'cgroup-test' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'cgroup' of Pod 'cgroup-test' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'cgroup' of Pod 'cgroup-test' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'cgroup' of Pod 'cgroup-test' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'cgroup' of Pod 'cgroup-test' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'cgroup' of Pod 'cgroup-test' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0017 (HIGH): Container 'cgroup' of Pod 'cgroup-test' should set 'securityContext.privileged' to false
════════════════════════════════════════
Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.

See https://avd.aquasec.com/misconfig/ksv017
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'cgroup' of Pod 'cgroup-test' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'cgroup' of Pod 'cgroup-test' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'cgroup' of Pod 'cgroup-test' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 cgroup-test.yaml:6-14
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 │       privileged: true
  12 │   hostPID: true
  13 │   hostIPC: true
  14 └   hostNetwork: true
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "cgroup" of pod "cgroup-test" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 cgroup-test.yaml:7-11
────────────────────────────────────────
   7 ┌   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 └       privileged: true
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod cgroup-test in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 cgroup-test.yaml:4
────────────────────────────────────────
   4 [   name: cgroup-test
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod cgroup-test in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 cgroup-test.yaml:6-14
────────────────────────────────────────
   6 ┌   containers:
   7 │   - name: cgroup
   8 │     image: busybox
   9 │     command: ["sh", "-c", "sleep 1d"]
  10 │     securityContext:
  11 │       privileged: true
  12 │   hostPID: true
  13 │   hostIPC: true
  14 └   hostNetwork: true
────────────────────────────────────────



chainoptim-config-server.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-config-server' of 'deployment' 'chainoptim-config-server' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-config-server' of Deployment 'chainoptim-config-server' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-config-server.yaml:8-36
────────────────────────────────────────
   8 ┌   replicas: 0
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-config-server
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-config-server
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-config-server" of deployment "chainoptim-config-server" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-config-server in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-config-server.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-config-server
   5 │   labels:
   6 └     app: chainoptim-config-server
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-config-server in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-config-server in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-config-server.yaml:17-36
────────────────────────────────────────
  17 ┌       containers:
  18 │       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 └         - name: SERVER_PORT
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container chainoptim-config-server in deployment chainoptim-config-server (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-config-server.yaml:18-36
────────────────────────────────────────
  18 ┌       - name: chainoptim-config-server
  19 │         image: tudoraorban/chainoptim-config-server:latest
  20 │         ports:
  21 │         - containerPort: 8888
  22 │         env:
  23 │         - name: SPRING_APPLICATION_NAME
  24 │           value: chainoptim-config-server
  25 │         - name: SERVER_PORT
  26 └           value: '8888'
  ..   
────────────────────────────────────────



chainoptim-config-server_1.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-config-server_1.yaml:6-10
────────────────────────────────────────
   6 ┌   ports:
   7 │   - port: 8888
   8 │     targetPort: 8888
   9 │   selector:
  10 └     app: chainoptim-config-server
────────────────────────────────────────



chainoptim-core.yaml (kubernetes)
=================================
Tests: 116 (SUCCESSES: 100, FAILURES: 16)
Failures: 16 (UNKNOWN: 0, LOW: 8, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-core' of Deployment 'chainoptim-core' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-core' of Deployment 'chainoptim-core' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-core' of 'deployment' 'chainoptim-core' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-core' of Deployment 'chainoptim-core' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-core' of Deployment 'chainoptim-core' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-core' of Deployment 'chainoptim-core' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-core' of Deployment 'chainoptim-core' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-core' of Deployment 'chainoptim-core' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-core.yaml:8-36
────────────────────────────────────────
   8 ┌   replicas: 1
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-core
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-core
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-core" of deployment "chainoptim-core" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-core in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-core.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-core
   5 │   labels:
   6 └     app: chainoptim-core
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-core in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-core in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-core.yaml:21-36
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 └           value: docker-dev
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container chainoptim-core in deployment chainoptim-core (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-core.yaml:22-36
────────────────────────────────────────
  22 ┌       - name: chainoptim-core
  23 │         image: tudoraorban/chainoptim-core:latest
  24 │         ports:
  25 │         - containerPort: 8080
  26 │         - containerPort: 50051
  27 │         env:
  28 │         - name: SPRING_PROFILES_ACTIVE
  29 │           value: docker-dev
  30 └         resources:
  ..   
────────────────────────────────────────



chainoptim-core_1.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-core_1.yaml:8-15
────────────────────────────────────────
   8 ┌   type: NodePort
   9 │   ports:
  10 │   - port: 8080
  11 │     targetPort: 8080
  12 │     protocol: TCP
  13 │     nodePort: 31234
  14 │   selector:
  15 └     app: chainoptim-core
────────────────────────────────────────



chainoptim-core_2.yaml (kubernetes)
===================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-core_2.yaml:6-12
────────────────────────────────────────
   6 ┌   type: ClusterIP
   7 │   ports:
   8 │   - port: 50051
   9 │     targetPort: 50051
  10 │     protocol: TCP
  11 │   selector:
  12 └     app: chainoptim-core
────────────────────────────────────────



chainoptim-demand.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-demand' of 'deployment' 'chainoptim-demand' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-demand' of Deployment 'chainoptim-demand' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-demand.yaml:8-33
────────────────────────────────────────
   8 ┌   replicas: 0
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-demand
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-demand
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-demand" of deployment "chainoptim-demand" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-demand in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-demand.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-demand
   5 │   labels:
   6 └     app: chainoptim-demand
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-demand in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-demand in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-demand.yaml:21-33
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 └         - name: GRPC_SERVICE_HOST
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container chainoptim-demand in deployment chainoptim-demand (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-demand.yaml:22-32
────────────────────────────────────────
  22 ┌       - name: chainoptim-demand
  23 │         image: tudoraorban/chainoptim-demand:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 │         - name: GRPC_SERVICE_HOST
  30 └           value: chainoptim-core-grpc
  ..   
────────────────────────────────────────



chainoptim-demand_1.yaml (kubernetes)
=====================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-demand_1.yaml:8-15
────────────────────────────────────────
   8 ┌   type: NodePort
   9 │   ports:
  10 │   - port: 8085
  11 │     targetPort: 8085
  12 │     protocol: TCP
  13 │     nodePort: 31281
  14 │   selector:
  15 └     app: chainoptim-demand
────────────────────────────────────────



chainoptim-notifications.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-notifications' of 'deployment' 'chainoptim-notifications' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-notifications' of Deployment 'chainoptim-notifications' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-notifications.yaml:8-29
────────────────────────────────────────
   8 ┌   replicas: 0
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-notifications
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-notifications
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-notifications" of deployment "chainoptim-notifications" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-notifications in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-notifications.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-notifications
   5 │   labels:
   6 └     app: chainoptim-notifications
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-notifications in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-notifications in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-notifications.yaml:21-29
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 └       automountServiceAccountToken: false
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container chainoptim-notifications in deployment chainoptim-notifications (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-notifications.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-notifications
  23 │         image: tudoraorban/chainoptim-notifications:latest
  24 │         ports:
  25 │         - containerPort: 8081
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────



chainoptim-notifications_1.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-notifications_1.yaml:8-15
────────────────────────────────────────
   8 ┌   type: NodePort
   9 │   ports:
  10 │   - port: 8081
  11 │     targetPort: 8081
  12 │     protocol: TCP
  13 │     nodePort: 31256
  14 │   selector:
  15 └     app: chainoptim-notifications
────────────────────────────────────────



chainoptim-production-ml.yaml (kubernetes)
==========================================
Tests: 116 (SUCCESSES: 97, FAILURES: 19)
Failures: 19 (UNKNOWN: 0, LOW: 12, MEDIUM: 4, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-production-ml' of 'deployment' 'chainoptim-production-ml' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-production-ml' of Deployment 'chainoptim-production-ml' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-production-ml.yaml:8-23
────────────────────────────────────────
   8 ┌   replicas: 0
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-production-ml
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-production-ml
  16 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-production-ml" of deployment "chainoptim-production-ml" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-production-ml in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-production-ml.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-production-ml
   5 │   labels:
   6 └     app: chainoptim-production-ml
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-production-ml in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-production-ml.yaml:18-23
────────────────────────────────────────
  18 ┌       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-production-ml in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-production-ml.yaml:17-23
────────────────────────────────────────
  17 ┌       containers:
  18 │       - image: chainoptim-production-ml:latest
  19 │         imagePullPolicy: IfNotPresent
  20 │         name: chainoptim-production-ml
  21 │         ports:
  22 │         - containerPort: 8000
  23 └           name: chainoptim
────────────────────────────────────────



chainoptim-production-ml_1.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-production-ml_1.yaml:8-15
────────────────────────────────────────
   8 ┌   type: NodePort
   9 │   ports:
  10 │   - port: 8000
  11 │     targetPort: 8000
  12 │     protocol: TCP
  13 │     nodePort: 30000
  14 │   selector:
  15 └     app: chainoptim-production-ml
────────────────────────────────────────



chainoptim-storage.yaml (kubernetes)
====================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-storage' of 'deployment' 'chainoptim-storage' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-storage' of Deployment 'chainoptim-storage' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-storage.yaml:8-29
────────────────────────────────────────
   8 ┌   replicas: 0
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-storage
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-storage
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-storage" of deployment "chainoptim-storage" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-storage in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-storage.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-storage
   5 │   labels:
   6 └     app: chainoptim-storage
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-storage in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-storage in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-storage.yaml:21-29
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 └       automountServiceAccountToken: false
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container chainoptim-storage in deployment chainoptim-storage (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-storage.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-storage
  23 │         image: tudoraorban/chainoptim-storage:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────



chainoptim-storage_1.yaml (kubernetes)
======================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-storage_1.yaml:8-15
────────────────────────────────────────
   8 ┌   type: NodePort
   9 │   ports:
  10 │   - port: 8084
  11 │     targetPort: 8084
  12 │     protocol: TCP
  13 │     nodePort: 31280
  14 │   selector:
  15 └     app: chainoptim-storage
────────────────────────────────────────



chainoptim-supply.yaml (kubernetes)
===================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'chainoptim-supply' of 'deployment' 'chainoptim-supply' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'chainoptim-supply' of Deployment 'chainoptim-supply' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-supply.yaml:8-29
────────────────────────────────────────
   8 ┌   replicas: 0
   9 │   selector:
  10 │     matchLabels:
  11 │       app: chainoptim-supply
  12 │   template:
  13 │     metadata:
  14 │       labels:
  15 │         app: chainoptim-supply
  16 └       annotations:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "chainoptim-supply" of deployment "chainoptim-supply" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-supply in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-supply.yaml:4-6
────────────────────────────────────────
   4 ┌   name: chainoptim-supply
   5 │   labels:
   6 └     app: chainoptim-supply
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-supply in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-supply in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-supply.yaml:21-29
────────────────────────────────────────
  21 ┌       containers:
  22 │       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 │           value: docker-dev
  29 └       automountServiceAccountToken: false
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container chainoptim-supply in deployment chainoptim-supply (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-supply.yaml:22-28
────────────────────────────────────────
  22 ┌       - name: chainoptim-supply
  23 │         image: tudoraorban/chainoptim-supply:latest
  24 │         ports:
  25 │         - containerPort: 8083
  26 │         env:
  27 │         - name: SPRING_PROFILES_ACTIVE
  28 └           value: docker-dev
────────────────────────────────────────



chainoptim-supply_1.yaml (kubernetes)
=====================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-supply_1.yaml:8-15
────────────────────────────────────────
   8 ┌   type: NodePort
   9 │   ports:
  10 │   - port: 8083
  11 │     targetPort: 8083
  12 │     protocol: TCP
  13 │     nodePort: 31280
  14 │   selector:
  15 └     app: chainoptim-supply
────────────────────────────────────────



chainoptim-vault.yaml (kubernetes)
==================================
Tests: 116 (SUCCESSES: 96, FAILURES: 20)
Failures: 20 (UNKNOWN: 0, LOW: 12, MEDIUM: 5, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'vault' of Deployment 'chainoptim-vault' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'vault' of 'deployment' 'chainoptim-vault' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'vault' of Deployment 'chainoptim-vault' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0013 (MEDIUM): Container 'vault' of Deployment 'chainoptim-vault' should specify an image tag
════════════════════════════════════════
It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.

See https://avd.aquasec.com/misconfig/ksv013
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'vault' of Deployment 'chainoptim-vault' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'vault' of Deployment 'chainoptim-vault' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-vault.yaml:6-29
────────────────────────────────────────
   6 ┌   replicas: 0
   7 │   selector:
   8 │     matchLabels:
   9 │       app: chainoptim-vault
  10 │   template:
  11 │     metadata:
  12 │       labels:
  13 │         app: chainoptim-vault
  14 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "vault" of deployment "chainoptim-vault" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0110 (LOW): deployment chainoptim-vault in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainoptim-vault.yaml:4
────────────────────────────────────────
   4 [   name: chainoptim-vault
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container chainoptim-vault in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment chainoptim-vault in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainoptim-vault.yaml:15-29
────────────────────────────────────────
  15 ┌       containers:
  16 │       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 └         env:
  ..   
────────────────────────────────────────


AVD-KSV-0125 (MEDIUM): Container vault in deployment chainoptim-vault (namespace: default) uses an image from an untrusted registry.
════════════════════════════════════════
Ensure that all containers use images only from trusted registry domains.

See https://avd.aquasec.com/misconfig/ksv0125
────────────────────────────────────────
 chainoptim-vault.yaml:16-29
────────────────────────────────────────
  16 ┌       - name: vault
  17 │         image: hashicorp/vault:latest
  18 │         ports:
  19 │         - containerPort: 8200
  20 │         args:
  21 │         - server
  22 │         - -dev
  23 │         env:
  24 └         - name: VAULT_DEV_ROOT_TOKEN_ID
  ..   
────────────────────────────────────────



chainoptim-vault_1.yaml (kubernetes)
====================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainoptim-vault_1.yaml:6-10
────────────────────────────────────────
   6 ┌   ports:
   7 │   - port: 8200
   8 │     targetPort: 8200
   9 │   selector:
  10 └     app: chainoptim-vault
────────────────────────────────────────



chainsaw-step-00-apply-1.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 chainsaw-step-00-apply-1.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:addrolebinding' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 chainsaw-step-00-apply-1.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



chainsaw-step-00-apply-114.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-env-var-in-pod' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-114.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-115.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-vols-pods' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-115.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - pods
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-116.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:scale-deployment-zero' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-116.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-118.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 chainsaw-step-00-apply-118.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:addrolebinding' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 chainsaw-step-00-apply-118.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



chainsaw-step-00-apply-123.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-env-var-in-pod' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-123.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-124.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-vols-pods' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-124.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - pods
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-125.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:scale-deployment-zero' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-125.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-127.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 chainsaw-step-00-apply-127.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:addrolebinding' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 chainsaw-step-00-apply-127.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



chainsaw-step-00-apply-132.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-env-var-in-pod' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-132.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-133.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-vols-pods' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-133.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - pods
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-134.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:scale-deployment-zero' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-134.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-136.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 chainsaw-step-00-apply-136.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:addrolebinding' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 chainsaw-step-00-apply-136.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



chainsaw-step-00-apply-141.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-env-var-in-pod' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-141.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-142.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-vols-pods' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-142.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - pods
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-143.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:scale-deployment-zero' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-143.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-15.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-env-var-in-pod' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-15.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-16.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:refresh-vols-pods' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-16.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - pods
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-17.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)

AVD-KSV-0048 (MEDIUM): ClusterRole 'kyverno:background-controller:scale-deployment-zero' should not have access to resources ["pods", "deployments", "jobs", "cronjobs", "statefulsets", "daemonsets", "replicasets", "replicationcontrollers"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
Depending on the policies enforced by the admission controller, this permission ranges from the ability to steal compute (crypto) by running workloads or allowing for creating workloads that escape to the node as root and escalation to cluster-admin.

See https://avd.aquasec.com/misconfig/ksv048
────────────────────────────────────────
 chainsaw-step-00-apply-17.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - apps
  12 │   resources:
  13 │   - deployments
  14 │   verbs:
  15 └   - update
────────────────────────────────────────



chainsaw-step-00-apply-19.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 113, FAILURES: 2)
Failures: 2 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 2)

AVD-KSV-0044 (CRITICAL): Role permits wildcard verb on wildcard resource
════════════════════════════════════════
Check whether role permits wildcard verb on wildcard resource

See https://avd.aquasec.com/misconfig/ksv044
────────────────────────────────────────
 chainsaw-step-00-apply-19.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────


AVD-KSV-0046 (CRITICAL): ClusterRole 'kyverno:background-controller:addrolebinding' shouldn't manage all resources
════════════════════════════════════════
Full control of the cluster resources, and therefore also root on all nodes where workloads can run and has access to all pods, secrets, and data.

See https://avd.aquasec.com/misconfig/ksv046
────────────────────────────────────────
 chainsaw-step-00-apply-19.yaml:10-15
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - '*'
  12 │   resources:
  13 │   - '*'
  14 │   verbs:
  15 └   - '*'
────────────────────────────────────────



chainsaw-step-01-apply-114.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'kyverno:background-controller:kubevirt-services' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 chainsaw-step-01-apply-114.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - services
  14 │   verbs:
  15 │   - create
  16 │   - update
  17 └   - delete
────────────────────────────────────────



chainsaw-step-01-apply-116.yaml (kubernetes)
============================================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 21, MEDIUM: 6, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'vault-agent' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'vault-agent' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:12-25
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app: busybox
  16 │   template:
  17 │     metadata:
  18 │       labels:
  19 │         app: busybox
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "vault-agent" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment00 in update-image-tag-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-116.yaml:21-25
────────────────────────────────────────
  21 ┌       containers:
  22 │       - image: busybox:1.35
  23 │         name: busybox
  24 │       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────



chainsaw-step-01-apply-123.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'kyverno:background-controller:kubevirt-services' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 chainsaw-step-01-apply-123.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - services
  14 │   verbs:
  15 │   - create
  16 │   - update
  17 └   - delete
────────────────────────────────────────



chainsaw-step-01-apply-125.yaml (kubernetes)
============================================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 21, MEDIUM: 6, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'vault-agent' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'vault-agent' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:12-25
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app: busybox
  16 │   template:
  17 │     metadata:
  18 │       labels:
  19 │         app: busybox
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "vault-agent" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment00 in update-image-tag-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-125.yaml:21-25
────────────────────────────────────────
  21 ┌       containers:
  22 │       - image: busybox:1.35
  23 │         name: busybox
  24 │       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────



chainsaw-step-01-apply-132.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'kyverno:background-controller:kubevirt-services' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 chainsaw-step-01-apply-132.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - services
  14 │   verbs:
  15 │   - create
  16 │   - update
  17 └   - delete
────────────────────────────────────────



chainsaw-step-01-apply-134.yaml (kubernetes)
============================================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 21, MEDIUM: 6, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'vault-agent' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'vault-agent' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:12-25
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app: busybox
  16 │   template:
  17 │     metadata:
  18 │       labels:
  19 │         app: busybox
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "vault-agent" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment00 in update-image-tag-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-134.yaml:21-25
────────────────────────────────────────
  21 ┌       containers:
  22 │       - image: busybox:1.35
  23 │         name: busybox
  24 │       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────



chainsaw-step-01-apply-141.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'kyverno:background-controller:kubevirt-services' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 chainsaw-step-01-apply-141.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - services
  14 │   verbs:
  15 │   - create
  16 │   - update
  17 └   - delete
────────────────────────────────────────



chainsaw-step-01-apply-143.yaml (kubernetes)
============================================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 21, MEDIUM: 6, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'vault-agent' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'vault-agent' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:12-25
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app: busybox
  16 │   template:
  17 │     metadata:
  18 │       labels:
  19 │         app: busybox
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "vault-agent" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment00 in update-image-tag-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-143.yaml:21-25
────────────────────────────────────────
  21 ┌       containers:
  22 │       - image: busybox:1.35
  23 │         name: busybox
  24 │       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────



chainsaw-step-01-apply-15.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 1, CRITICAL: 0)

AVD-KSV-0056 (HIGH): ClusterRole 'kyverno:background-controller:kubevirt-services' should not have access to resources ["services", "endpoints", "endpointslices", "networkpolicies", "ingresses"] for verbs ["create", "update", "patch", "delete", "deletecollection", "impersonate", "*"]
════════════════════════════════════════
The ability to control which pods get service traffic directed to them allows for interception attacks. Controlling network policy allows for bypassing lateral movement restrictions.

See https://avd.aquasec.com/misconfig/ksv056
────────────────────────────────────────
 chainsaw-step-01-apply-15.yaml:10-17
────────────────────────────────────────
  10 ┌ - apiGroups:
  11 │   - ""
  12 │   resources:
  13 │   - services
  14 │   verbs:
  15 │   - create
  16 │   - update
  17 └   - delete
────────────────────────────────────────



chainsaw-step-01-apply-17.yaml (kubernetes)
===========================================
Tests: 131 (SUCCESSES: 99, FAILURES: 32)
Failures: 32 (UNKNOWN: 0, LOW: 21, MEDIUM: 6, HIGH: 5, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0001 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'vault-agent' of Deployment 'deployment00' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'vault-agent' of 'deployment' 'deployment00' in 'update-image-tag-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'vault-agent' of Deployment 'deployment00' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:12-25
────────────────────────────────────────
  12 ┌   replicas: 1
  13 │   selector:
  14 │     matchLabels:
  15 │       app: busybox
  16 │   template:
  17 │     metadata:
  18 │       labels:
  19 │         app: busybox
  20 └     spec:
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "vault-agent" of deployment "deployment00" in "update-image-tag-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:22-23
────────────────────────────────────────
  22 ┌       - image: busybox:1.35
  23 └         name: busybox
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container deployment00 in update-image-tag-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:24-25
────────────────────────────────────────
  24 ┌       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────


AVD-KSV-0118 (HIGH): deployment deployment00 in update-image-tag-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-01-apply-17.yaml:21-25
────────────────────────────────────────
  21 ┌       containers:
  22 │       - image: busybox:1.35
  23 │         name: busybox
  24 │       - image: vault:1.2.3
  25 └         name: vault-agent
────────────────────────────────────────



chainsaw-step-01-apply-411.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-411.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-414.yaml (kubernetes)
============================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-414.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-42.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-42.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-45.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-45.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-48.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-48.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-6.yaml (kubernetes)
==========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-6.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-61.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-61.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-62.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-62.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-63.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-63.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-01-apply-64.yaml (kubernetes)
===========================================
Tests: 115 (SUCCESSES: 114, FAILURES: 1)
Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0)

AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-01-apply-64.yaml:7-34
────────────────────────────────────────
   7 ┌   egress:
   8 │   - ports:
   9 │     - port: 5978
  10 │       protocol: TCP
  11 │     to:
  12 │     - ipBlock:
  13 │         cidr: 10.0.0.0/24
  14 │   ingress:
  15 └   - from:
  ..   
────────────────────────────────────────



chainsaw-step-02-apply-1101.yaml (kubernetes)
=============================================
Tests: 116 (SUCCESSES: 98, FAILURES: 18)
Failures: 18 (UNKNOWN: 0, LOW: 12, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'pod01' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'pod01' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'pod01' in 'default' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'pod01' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:9-11
────────────────────────────────────────
   9 ┌   containers:
  10 │   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "pod01" in "default" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0110 (LOW): pod pod01 in default namespace should set metadata.namespace to a non-default namespace
════════════════════════════════════════
Checks whether a workload is running in the default namespace.

See https://avd.aquasec.com/misconfig/ksv110
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:4-7
────────────────────────────────────────
   4 ┌   labels:
   5 │     breakglass: dont
   6 │     foo: bar
   7 └   name: pod01
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pod01 in default namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:10-11
────────────────────────────────────────
  10 ┌   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod pod01 in default namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-02-apply-1101.yaml:9-11
────────────────────────────────────────
   9 ┌   containers:
  10 │   - image: busybox:1.35
  11 └     name: busybox
────────────────────────────────────────



chainsaw-step-02-apply-1108.yaml (kubernetes)
=============================================
Tests: 116 (SUCCESSES: 99, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'pod01' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'pod01' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'pod01' in 'refresh-vols-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'pod01' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:7-19
────────────────────────────────────────
   7 ┌   containers:
   8 │   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "pod01" in "refresh-vols-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pod01 in refresh-vols-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod pod01 in refresh-vols-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-02-apply-1108.yaml:7-19
────────────────────────────────────────
   7 ┌   containers:
   8 │   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
  ..   
────────────────────────────────────────



chainsaw-step-02-apply-116.yaml (kubernetes)
============================================
Tests: 116 (SUCCESSES: 99, FAILURES: 17)
Failures: 17 (UNKNOWN: 0, LOW: 11, MEDIUM: 3, HIGH: 3, CRITICAL: 0)

AVD-KSV-0001 (MEDIUM): Container 'busybox' of Pod 'pod01' should set 'securityContext.allowPrivilegeEscalation' to false
════════════════════════════════════════
A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.

See https://avd.aquasec.com/misconfig/ksv001
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0003 (LOW): Container 'busybox' of Pod 'pod01' should add 'ALL' to 'securityContext.capabilities.drop'
════════════════════════════════════════
The container should drop all default capabilities and add only those that are needed for its execution.

See https://avd.aquasec.com/misconfig/ksv003
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0004 (LOW): Container 'busybox' of 'pod' 'pod01' in 'refresh-vols-ns' namespace should set securityContext.capabilities.drop
════════════════════════════════════════
Security best practices require containers to run with minimal required capabilities.

See https://avd.aquasec.com/misconfig/ksv004
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0011 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.limits.cpu'
════════════════════════════════════════
Enforcing CPU limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv011
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0012 (MEDIUM): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsNonRoot' to true
════════════════════════════════════════
Force the running image to run as a non-root user to ensure least privileges.

See https://avd.aquasec.com/misconfig/ksv012
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0014 (HIGH): Container 'busybox' of Pod 'pod01' should set 'securityContext.readOnlyRootFilesystem' to true
════════════════════════════════════════
An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.

See https://avd.aquasec.com/misconfig/ksv014
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0015 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.requests.cpu'
════════════════════════════════════════
When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv015
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0016 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.requests.memory'
════════════════════════════════════════
When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.

See https://avd.aquasec.com/misconfig/ksv016
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0018 (LOW): Container 'busybox' of Pod 'pod01' should set 'resources.limits.memory'
════════════════════════════════════════
Enforcing memory limits prevents DoS via resource exhaustion.

See https://avd.aquasec.com/misconfig/ksv018
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0020 (LOW): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsUser' > 10000
════════════════════════════════════════
Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv020
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0021 (LOW): Container 'busybox' of Pod 'pod01' should set 'securityContext.runAsGroup' > 10000
════════════════════════════════════════
Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.

See https://avd.aquasec.com/misconfig/ksv021
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0030 (LOW): Either Pod or Container should set 'securityContext.seccompProfile.type' to 'RuntimeDefault'
════════════════════════════════════════
According to pod security standard 'Seccomp', the RuntimeDefault seccomp profile must be required, or allow specific additional profiles.

See https://avd.aquasec.com/misconfig/ksv030
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0040 (LOW): A resource quota policy with hard memory and CPU limits should be configured per namespace
════════════════════════════════════════
Ensure that a ResourceQuota policy is configured to limit aggregate resource usage within a namespace

See https://avd.aquasec.com/misconfig/ksv040
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:7-19
────────────────────────────────────────
   7 ┌   containers:
   8 │   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
  ..   
────────────────────────────────────────


AVD-KSV-0104 (MEDIUM): container "busybox" of pod "pod01" in "refresh-vols-ns" namespace should specify a seccomp profile
════════════════════════════════════════
A program inside the container can bypass Seccomp protection policies.

See https://avd.aquasec.com/misconfig/ksv104
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0106 (LOW): container should drop all
════════════════════════════════════════
Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability.

See https://avd.aquasec.com/misconfig/ksv106
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0118 (HIGH): container pod01 in refresh-vols-ns namespace is using the default security context
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:8-15
────────────────────────────────────────
   8 ┌   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
────────────────────────────────────────


AVD-KSV-0118 (HIGH): pod pod01 in refresh-vols-ns namespace is using the default security context, which allows root privileges
════════════════════════════════════════
Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access.

See https://avd.aquasec.com/misconfig/ksv118
────────────────────────────────────────
 chainsaw-step-02-apply-116.yaml:7-19
────────────────────────────────────────
   7 ┌   containers:
   8 │   - command:
   9 │     - sleep
  10 │     - "3600"
  11 │     image: busybox:1.35
  12 │     name: busybox
  13 │     volumeMounts:
  14 │     - mountPath: /mnt/foo
  15 └       name: refresh-vol
  ..   
────────────────────────────────────────


